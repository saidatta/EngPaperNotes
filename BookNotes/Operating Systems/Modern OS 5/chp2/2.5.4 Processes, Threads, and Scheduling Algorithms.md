#### **Guaranteed Scheduling**

Guaranteed scheduling is a form of **fair-share scheduling** where the system guarantees that each process (or user) will receive a fair amount of CPU time, proportional to the number of active processes.

##### **Key Concepts**
1. **Fairness**: The CPU time is divided evenly among active processes, guaranteeing that each process receives its fair share of the CPU.
2. **CPU Time Tracking**: The system keeps track of:
   - The **time** each process has been running since its creation.
   - The **time** each process should have received according to the fair share calculation.
   - The ratio of **actual CPU time received** to **expected CPU time**.
   
3. **Scheduling Logic**:
   - The process with the **lowest ratio** of actual-to-expected CPU time is selected to run next.
   - This ensures that processes that have been starved of CPU time are prioritized, preventing CPU monopolization.

##### **CFS (Completely Fair Scheduler)** in Linux
The **Completely Fair Scheduler (CFS)** uses a **red-black tree** to manage process scheduling:
- Each process is stored in the tree based on its **spent execution time**.
- The process with the **least execution time** (leftmost node) is selected to run.
- Once the process finishes or is interrupted, its new **execution time** is updated and the process is reinserted into the tree.

##### **Rust Example: Guaranteed Scheduling with Fair Share**

```rust
use std::sync::{Arc, Mutex};
use std::thread;
use std::time::{Duration, Instant};

struct Process {
    id: u32,
    run_time: Duration,
    entitled_time: Duration,
}

fn main() {
    let processes = vec![
        Arc::new(Mutex::new(Process {
            id: 1,
            run_time: Duration::new(0, 0),
            entitled_time: Duration::new(2, 0),
        })),
        Arc::new(Mutex::new(Process {
            id: 2,
            run_time: Duration::new(0, 0),
            entitled_time: Duration::new(2, 0),
        })),
    ];

    for p in processes.iter() {
        let p_clone = Arc::clone(p);
        thread::spawn(move || {
            let mut p = p_clone.lock().unwrap();
            let start = Instant::now();
            while start.elapsed() < p.entitled_time {
                // Simulate process execution
                println!("Process {} is running", p.id);
                thread::sleep(Duration::from_millis(500));
                p.run_time += Duration::from_millis(500);
            }
        });
    }
}
```
This example simulates a **fair-share scheduler** where each process is assigned an **entitled time**. The CPU ensures that each process receives its allocated time without monopolizing resources.

#### **Lottery Scheduling**

**Lottery Scheduling** is a probabilistic scheduling method where **processes are given tickets**. The process to run next is chosen randomly based on the lottery ticket it holds.

##### **Key Features**:
1. **Random Selection**: Each process holds a number of tickets proportional to its importance or priority.
2. **Fair Allocation**: In the long run, a process will receive CPU time proportional to the number of tickets it holds.
3. **Cooperation Between Processes**: Processes can exchange tickets, for example, a client may give tickets to a server while it waits for a response.
   
##### **Benefits of Lottery Scheduling**:
- **Flexibility**: It’s easy to implement without requiring complex priority systems.
- **Fairness**: By assigning tickets proportionally, it ensures fairness over the long run.
  
##### **Rust Example: Lottery Scheduling**

```rust
use rand::Rng;
use std::collections::HashMap;

struct Process {
    id: u32,
    tickets: u32,
}

fn main() {
    let mut processes: Vec<Process> = vec![
        Process { id: 1, tickets: 10 },
        Process { id: 2, tickets: 20 },
        Process { id: 3, tickets: 25 },
    ];

    let total_tickets: u32 = processes.iter().map(|p| p.tickets).sum();
    let mut rng = rand::thread_rng();

    for _ in 0..10 {
        let winning_ticket = rng.gen_range(0..total_tickets);
        let mut current_ticket_count = 0;

        for process in &processes {
            current_ticket_count += process.tickets;
            if winning_ticket < current_ticket_count {
                println!("Process {} won the lottery!", process.id);
                break;
            }
        }
    }
}
```
In this code, we simulate a **lottery scheduling** system. Each process has a number of **tickets**, and a **random winner** is chosen based on the number of tickets each process holds. Over time, processes with more tickets win more frequently.

#### **Fair-Share Scheduling**

Fair-share scheduling differs from traditional scheduling methods by ensuring that **each user** receives a fair share of the CPU. This prevents a single user with many processes from hogging CPU time.

##### **Key Concepts**:
1. **User-Based Quotas**: Each user is assigned a **quota** of the CPU, which is split among their processes.
2. **Process Scheduling Within Users**: Once a user is selected, their processes are scheduled in a round-robin fashion, ensuring that **each process gets a share** of the user’s quota.
   
##### **Example Scheduling**:
If user1 has 4 processes and user2 has 1, with a 50-50 CPU split, the scheduling might look like:

```
User1 ProcessA → User2 ProcessE → User1 ProcessB → User2 ProcessE → ...
```

This ensures **fairness across users**.

#### **Real-Time Scheduling**

Real-time scheduling is used in systems that need to respond to events within a strict time frame. It’s typically divided into **hard real-time** and **soft real-time** systems.

##### **Hard Real-Time Systems**:
- **Deadlines are critical**: If a deadline is missed, the system could fail.
- Examples include **aircraft control systems** or **medical monitoring**.

##### **Soft Real-Time Systems**:
- **Deadlines are preferred** but not critical: Missing a deadline might degrade the system’s performance, but it won’t result in a failure.
- Examples include **multimedia systems**, such as video or audio streaming, where occasional missed deadlines result in degraded quality.

##### **Schedulability Condition**:

\[
\sum_{i=1}^{n} \frac{C_i}{P_i} \leq 1
\]

Where:
- \(C_i\) = Computation time of the i-th event.
- \(P_i\) = Period of the i-th event.
- This equation ensures that the **total CPU demand** is less than or equal to the available CPU time.

##### **Example**:

Consider a system with three periodic events:
- Event 1: Period \(P_1 = 100 \text{ms}\), CPU time \(C_1 = 50 \text{ms}\)
- Event 2: Period \(P_2 = 200 \text{ms}\), CPU time \(C_2 = 30 \text{ms}\)
- Event 3: Period \(P_3 = 500 \text{ms}\), CPU time \(C_3 = 100 \text{ms}\)

The system is **schedulable** if:
\[
\frac{50}{100} + \frac{30}{200} + \frac{100}{500} = 0.5 + 0.15 + 0.2 = 0.85 \leq 1
\]

This shows that the system can handle the periodic events without missing deadlines.

#### **Thread Scheduling**

Thread scheduling becomes important in systems with **user-level** and **kernel-level threads**. 

##### **User-Level Threads**:
- **Efficient**: Fast thread context switches, but the kernel is unaware of threads, which can complicate I/O operations (e.g., blocking on I/O may block the whole process).

##### **Kernel-Level Threads**:
- **Preemptive**: The kernel schedules threads individually, but thread switches are slower due to **full context switches**.

##### **Example of Kernel-Level Thread Scheduling**:

```rust
use std::thread;

fn main() {
    let handles: Vec<_> = (0..5).map(|i| {
        thread::spawn(move || {
            println!("Thread {} is running", i);
        })
    }).collect();

    for handle in handles {
        handle.join().unwrap();
    }
}
```
In this simple **kernel-level thread** example, each thread is preemptively scheduled by the kernel, and all threads run concurrently.

#### **Conclusion**

- **Processes and threads** are fundamental to modern operating systems, providing concurrency and parallelism.
- **Scheduling algorithms** like **round-robin**, **lottery scheduling**, and **fair-share scheduling** help balance CPU time across processes and users.
- **Real-time scheduling** ensures that systems meet deadlines for critical tasks, while **thread scheduling** offers flexibility and performance optimization for multi-threaded applications.
  
The future of research in this area includes scaling systems to handle more cores and threads, ensuring **concurrency correctness**, and **optimizing scheduling** for real-time and distributed systems.