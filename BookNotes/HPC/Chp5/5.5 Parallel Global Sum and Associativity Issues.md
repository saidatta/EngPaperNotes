### **Global Sum Challenges in Parallel Computing**
The **global sum** is one of the most common patterns in parallel computing, but it faces significant challenges due to the non-associativity of finite-precision arithmetic. In parallel computing, the order in which numbers are added can change the result, making it non-deterministic and affecting the accuracy of calculations.

### **Associativity Problem with Floating-Point Arithmetic**
- **Finite-Precision Arithmetic**: In floating-point calculations, operations like addition are not associative. That means:
  \[
  (a + b) + c \neq a + (b + c)
  \]
- **Catastrophic Cancellation**: This occurs when two nearly equal numbers are subtracted, leading to a loss of significant digits and precision.

#### **Example: Catastrophic Cancellation**
```python
# Example demonstrating catastrophic cancellation in Python
x = 12.15692174374373 - 12.15692174374372
print(x)  # Output: 1.06581410364e-14
```
- This output shows a significant loss of precision, as only a few significant digits remain, with the rest filled with noise.

### **Reduction Pattern for Global Sum**
A **reduction** operation reduces an array to a scalar value, like computing the sum of all elements. In parallel computing, reduction patterns like global sum are prone to accuracy issues due to the changing order of additions.

### **Approaches to Address Associativity in Global Sum**
Several strategies have been developed to improve the accuracy of global sums in parallel computing. Each technique is designed to handle the non-associative nature of floating-point arithmetic.

#### **1. Long-Double Data Type**
- **Concept**: Uses 80-bit floating-point precision to store intermediate results, which reduces precision loss.
- **Limitation**: This approach is not portable since the precision of `long double` varies across architectures.

**Code Implementation: Long-Double Data Type**
```c
double do_ldsum(double *var, long ncells) {
    long double ldsum = 0.0;
    for (long i = 0; i < ncells; i++) {
        ldsum += (long double)var[i];  // Use long double for precision
    }
    return (double)ldsum;
}
```

#### **2. Pairwise Summation**
- **Concept**: This technique pairs elements and sums them up to reduce the overall error by combining values of similar magnitudes.
- **Complexity**: Pairwise summation has \( O(\log n) \) complexity for reductions but requires additional memory.

**Code Implementation: Pairwise Summation**
```c
double do_pair_sum(double *var, long ncells) {
    double *pwsum = (double *)malloc(ncells / 2 * sizeof(double));
    long nmax = ncells / 2;
    for (long i = 0; i < nmax; i++) {
        pwsum[i] = var[i * 2] + var[i * 2 + 1];
    }
    while (nmax > 1) {
        nmax /= 2;
        for (long i = 0; i < nmax; i++) {
            pwsum[i] = pwsum[i * 2] + pwsum[i * 2 + 1];
        }
    }
    double result = pwsum[0];
    free(pwsum);
    return result;
}
```

#### **3. Kahan Summation Algorithm**
- **Concept**: Adds a small correction factor to account for the rounding error in each step.
- **Benefit**: This method effectively doubles precision without requiring significantly more computation.

**Code Implementation: Kahan Summation**
```c
double do_kahan_sum(double *var, long ncells) {
    double sum = 0.0;
    double correction = 0.0;  // Correction term to compensate for lost low-order bits
    for (long i = 0; i < ncells; i++) {
        double y = var[i] - correction;
        double t = sum + y;
        correction = (t - sum) - y;
        sum = t;
    }
    return sum;
}
```

#### **4. Knuth Summation Algorithm**
- **Concept**: Extends Kahan's idea by using a double-double precision to track errors more precisely.
- **Complexity**: It requires a few more floating-point operations than Kahan's algorithm but offers better precision.

**Code Implementation: Knuth Summation**
```c
double do_knuth_sum(double *var, long ncells) {
    double sum = 0.0;
    double correction = 0.0;
    for (long i = 0; i < ncells; i++) {
        double u = sum;
        double v = var[i] + correction;
        double upt = u + v;
        double up = upt - v;
        double vpp = upt - up;
        sum = upt;
        correction = (u - up) + (v - vpp);
    }
    return sum;
}
```

#### **5. Quad-Precision Summation**
- **Concept**: Uses 128-bit precision to ensure higher accuracy in summations.
- **Limitation**: Not widely supported and tends to be slow due to software implementation.

**Code Implementation: Quad-Precision Summation**
```c
double do_qdsum(double *var, long ncells) {
    __float128 qdsum = 0.0;
    for (long i = 0; i < ncells; i++) {
        qdsum += (__float128)var[i];  // Cast to quad precision
    }
    return (double)qdsum;
}
```

### **Comparison of Global Sum Techniques**
| **Method**             | **Error**      | **Run Time** |
|------------------------|----------------|--------------|
| **Double**             | -1.99e-09      | 0.116 sec    |
| **Long Double**        | -1.31e-13      | 0.118 sec    |
| **Pairwise Summation** | 0.0            | 0.402 sec    |
| **Kahan Summation**    | 0.0            | 0.406 sec    |
| **Knuth Summation**    | 0.0            | 0.704 sec    |
| **Quad Double**        | 5.55e-17       | 3.010 sec    |

- **Double** precision summation has the fastest runtime but suffers from precision errors.
- **Kahan** and **Knuth** summations provide error-free results with a modest increase in runtime.
- **Quad precision** offers the highest accuracy but is not practical due to its performance cost.

## **Key Concepts for Future Parallel Algorithm Research**

### **Characteristics of Highly Parallel Algorithms**
1. **Locality**:
   - **Cache Locality**: Ensuring data accessed together is stored close in memory for efficient cache utilization.
   - **Operational Locality**: Limiting operations to relevant data points to avoid unnecessary computations.

2. **Asynchronous Execution**:
   - Minimizing synchronization points to improve scalability and reduce thread contention.

3. **Fewer Conditionals**:
   - Reducing branch instructions to avoid thread divergence, especially on SIMD and GPU architectures.

4. **Reproducibility**:
   - Enhancing precision in arithmetic to maintain consistency and ensure deterministic results.

5. **Higher Arithmetic Intensity**:
   - Increasing the ratio of arithmetic operations to memory operations to fully utilize computational power.

### **Recommendations for Further Reading**
For a deeper understanding of parallel algorithms and patterns, the following books are recommended:
- **"Introduction to Algorithms"** by Thomas Cormen et al.
- **"Structured Parallel Programming: Patterns for Efficient Computation"** by Michael McCool et al.
- **"Patterns for Parallel Programming"** by Timothy G. Mattson et al.

### **Final Thoughts**
The **prefix sum** (scan) and global sum techniques are integral patterns in parallel computing that highlight the importance of both algorithmic design and numerical accuracy. Techniques like Kahan and Knuth summation offer practical solutions to ensure that parallel algorithms produce results that are both accurate and consistent with their serial counterparts. By leveraging locality, reducing synchronization, and employing advanced precision techniques, we can achieve scalable, reproducible, and efficient parallel computations.