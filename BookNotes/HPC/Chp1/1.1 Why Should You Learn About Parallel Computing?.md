### The Future is Parallel
The evolution of CPU architecture has shifted focus from increasing clock speeds to leveraging multiple cores due to the limits of miniaturization, heat dissipation, and power consumption. Understanding parallel computing is essential because this trend indicates that improvements in computing performance will largely depend on parallelism rather than solely on faster clock speeds.![[Screenshot 2024-10-09 at 3.56.12 PM.png]]
#### Trends in Processor Design
**Figure 1.2: Processor Trends (1970-2018)**  
The following trends have been observed in commodity processors:
- **Single-thread performance:** Has plateaued since the mid-2000s.
- **CPU Clock Frequency:** Stagnant due to heat and power constraints.
- **Power Consumption:** Flattened as power efficiency became a priority.
- **Number of CPU Cores:** Increasing since 2005, signaling the shift to parallel computing.
The era of parallel computing began around 2005 when the number of cores started to rise, while clock frequency and power consumption remained constant. This suggests that increasing core count is the primary way to achieve ideal CPU performance, emphasizing the need for parallel computing strategies.
### Example of Theoretical Parallelism

#### Problem Statement
Consider a 16-core CPU with hyperthreading and a 256-bit vector unit.

**Calculation of Parallelism:**
- **Cores:** 16
- **Hyperthreads per Core:** 2
- **Vector Unit Width:** 256 bits
- **Data Type:** 64-bit double

The degree of parallelism is calculated as:
```
16 cores × 2 hyperthreads × (256-bit vector unit / 64-bit double) = 128-way parallelism
```
This implies that a serial program uses only:
```
1 serial path / 128 parallel paths = 0.8% of processing capability
```
Thus, a serial application utilizes only a fraction (0.8%) of the processor's potential.

### Hardware Features Boosting Parallelism
- **Hyperthreading:** Allows interleaving of two instruction queues per physical core.
- **Vector Processors:** Execute multiple instructions at once (e.g., a 256-bit unit can handle four 64-bit or eight 32-bit instructions).
These features highlight the importance of understanding both software-level optimizations and hardware capabilities to maximize parallelism.
## 1.1.1 Benefits of Parallel Computing
### 1. Faster Run Times
Parallel computing can significantly reduce application run times by distributing computations across multiple cores or processors. This speedup is especially critical in:
- **Real-time applications** like multimedia processing.
- **Big data operations** where timely insights are essential.
### 2. Larger Problem Sizes
Parallelism enables scaling to larger problems that would be impractical with serial computation due to limitations in memory or computational resources.
### 3. Energy Efficiency
By executing tasks faster and more efficiently, parallelism reduces energy consumption, which is crucial for:
- **Battery-operated devices:** Quick computations extend battery life.
- **Edge computing:** Processes data at the source, reducing the need to transmit large datasets over networks.
**Equation for Energy Consumption:**
```
P = (N Processors) × (R Watts/Processor) × (T hours)
```
Where:
- **P:** Energy consumption in kilowatt-hours (kWh)
- **N:** Number of processors
- **R:** Thermal design power (W)
- **T:** Application run time (hrs)

#### Example Calculation
**Intel Xeon E5-4660 CPU (16 cores) with 120 W TDP running for 24 hours:**
```
P = (20 processors) × (120 W/processor) × (24 hours) = 57.60 kWh
```

**NVIDIA Tesla V100 GPU (4 GPUs) with 300 W TDP running for 24 hours:**
```
P = (4 GPUs) × (300 W/GPU) × (24 hrs) = 28.80 kWh
```
This demonstrates that using GPUs can halve the energy consumption compared to CPU-only computations, provided the application has enough parallelism.
## 1.1.2 Parallel Computing Cautions
- **Application Suitability:** Not all applications benefit from parallelism. It's essential to identify whether the application has inherent parallelism.
- **Effort vs. Reward:** The transition to parallel architectures can be labor-intensive and requires a plan for accelerating the application with reasonable estimates of potential benefits.
## 1.2 Fundamental Laws of Parallel Computing
### 1.2.1 Amdahl’s Law (Strong Scaling)
Amdahl's Law provides a formula to calculate the theoretical speedup of a fixed-size problem when parallelized.
**Equation:**
```
SpeedUp(N) = 1 / [S + (P / N)]
```
Where:
- **P:** Parallel fraction of the code
- **S:** Serial fraction (S + P = 1)
- **N:** Number of processors
#### Implication
The speedup is limited by the serial portion of the code, emphasizing that even with a significant number of processors, performance gains are constrained if the serial portion dominates.
![[Screenshot 2024-10-09 at 3.59.33 PM.png]]
### 1.2.2 Gustafson-Barsis’s Law (Weak scaling)
Gustafson-Barsis’s Law addresses the limitations of Amdahl's Law by considering that as the number of processors increases, so should the problem size.
**Equation:**
```
SpeedUp(N) = N - S * (N - 1)
```
This means that a larger problem can be solved in the same time using more processors, which is known as **weak scaling**.
![[Screenshot 2024-10-09 at 4.00.08 PM.png]]
### Strong Scaling vs. Weak Scaling
- **Strong Scaling:** The total problem size is fixed; adding more processors reduces computation time.
- **Weak Scaling:** The problem size per processor remains constant; total problem size increases with more processors.
- ![[Screenshot 2024-10-09 at 4.02.46 PM.png]]
### Scalability Considerations
- **Memory Scalability:** Distributed arrays scale well with processor count, while replicated arrays require more memory on each processor, limiting scalability.
- **Run-time Scalability:** Memory constraints often dictate the limits of run-time performance improvements.
- ![[Screenshot 2024-10-09 at 4.03.06 PM.png]]
## Rust Code Example for Parallel Computing
To demonstrate parallel computation, let's implement a basic parallel summation using Rust's `rayon` crate, which provides a high-level API for data parallelism.

```rust
use rayon::prelude::*;  // Import rayon parallel iterator

fn parallel_sum(numbers: &[i32]) -> i32 {
    numbers.par_iter().sum()  // Parallel summation using rayon's parallel iterator
}

fn main() {
    let numbers: Vec<i32> = (1..=1_000_000).collect();  // Vector of numbers
    let result = parallel_sum(&numbers);  // Compute sum in parallel
    println!("The sum is: {}", result);
}
```
### Explanation
- **Parallel Iterator (`par_iter`)**: Enables parallel traversal and summation of the data.
- **Scalability**: This approach efficiently utilizes multiple cores, reducing the computation time for large datasets.
## Equations and Theoretical Insights

1. **Energy Efficiency Calculation:**
   ```
   P = (N Processors) × (R Watts/Processors) × (T hours)
   ```
2. **Amdahl's Law Formula:**
   ```
   SpeedUp(N) = 1 / [S + (P / N)]
   ```
3. **Gustafson-Barsis's Law Formula:**
   ```
   SpeedUp(N) = N - S * (N - 1)
   ```

These equations illustrate the limits and possibilities of parallel performance, helping guide decisions in parallel algorithm design.

## Conclusion

Understanding high-performance and parallel computing concepts is critical for leveraging modern processors efficiently. Concepts like Amdahl’s Law and Gustafson-Barsis’s Law highlight the potential and limitations of parallelism. With practical examples and Rust code, these notes aim to provide a deep understanding of why and how to harness parallel computing effectively in real-world applications.