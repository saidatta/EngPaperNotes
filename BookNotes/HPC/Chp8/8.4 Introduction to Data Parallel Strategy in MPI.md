- The **data parallel strategy** involves distributing subsets of data across different processes and performing operations on these subsets concurrently.
- MPI enables scalable parallelism by allowing each process to work on its part of the data independently, significantly improving performance in high-performance computing tasks.

### **Typical Data Parallel Patterns**
1. **Stream Triad**: Used to measure memory bandwidth by performing simple operations on large arrays without inter-process communication.
2. **Ghost Cell Exchange**: Essential for maintaining data consistency across the borders of different processes' data domains, especially in multi-dimensional grid computations.

---

## **2. STREAM Triad: Measuring Bandwidth**

### **Concept**
- The **STREAM Triad** benchmark is used to measure the memory bandwidth of a system when multiple processes operate simultaneously.
- This test involves simple arithmetic operations on arrays, helping to determine if the system's memory bandwidth scales with the number of processes.

### **Implementation Example**

```c
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>
#include "timer.h"

#define NTIMES 16
#define STREAM_ARRAY_SIZE 80000000

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    int nprocs, rank;
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int ibegin = STREAM_ARRAY_SIZE * (rank) / nprocs;
    int iend = STREAM_ARRAY_SIZE * (rank + 1) / nprocs;
    int nsize = iend - ibegin;

    double *a = malloc(nsize * sizeof(double));
    double *b = malloc(nsize * sizeof(double));
    double *c = malloc(nsize * sizeof(double));

    struct timespec tstart;
    double scalar = 3.0, time_sum = 0.0;

    for (int i = 0; i < nsize; i++) {
        a[i] = 1.0;
        b[i] = 2.0;
    }

    for (int k = 0; k < NTIMES; k++) {
        cpu_timer_start(&tstart);
        for (int i = 0; i < nsize; i++) {
            c[i] = a[i] + scalar * b[i];
        }
        time_sum += cpu_timer_stop(tstart);
        c[1] = c[2]; // Prevents loop optimization by the compiler
    }

    free(a);
    free(b);
    free(c);

    if (rank == 0)
        printf("Average runtime is %lf msecs\n", time_sum / NTIMES);

    MPI_Finalize();
    return 0;
}
```

### **Key Observations**
- **Scalability**: The performance should ideally scale linearly with the number of processors.
- **Memory Bandwidth**: The system's memory bandwidth could be a limiting factor in achieving perfect speedup.

---

## **3. Ghost Cell Exchange in 2D Mesh Computations**

### **Concept of Ghost Cells**
- **Ghost cells** are used to store neighboring data values from adjacent processes, reducing communication overhead.
- The **ghost cell technique** is fundamental to maintaining the data consistency required for operations on computational grids, such as finite difference methods.

### **Setup for Ghost Cell Exchange in a 2D Mesh**

```c
int imax = 2000, jmax = 2000;
int nprocx = 0, nprocy = 0;
int nhalo = 2; // Number of halo (ghost) cells

int xcoord = rank % nprocx;
int ycoord = rank / nprocx;

int nleft = (xcoord > 0) ? rank - 1 : MPI_PROC_NULL;
int nrght = (xcoord < nprocx - 1) ? rank + 1 : MPI_PROC_NULL;
int nbot  = (ycoord > 0) ? rank - nprocx : MPI_PROC_NULL;
int ntop  = (ycoord < nprocy - 1) ? rank + nprocx : MPI_PROC_NULL;

int ibegin = imax * xcoord / nprocx;
int iend = imax * (xcoord + 1) / nprocx;
int jbegin = jmax * ycoord / nprocy;
int jend = jmax * (ycoord + 1) / nprocy;
```

- **Grid partitioning**: Each process handles a subdomain of the computational grid.
- **Neighbor identification**: Determines which processes hold neighboring data.

### **Ghost Cell Exchange Logic**
- **Horizontal (Left/Right) Communication**: Exchanging ghost cells between neighboring processes in the x-direction.
- **Vertical (Top/Bottom) Communication**: Exchanging ghost cells in the y-direction after the horizontal exchange.

### **Ghost Cell Update Implementation with MPI_Pack**

```c
MPI_Request request[4 * nhalo];
MPI_Status status[4 * nhalo];
int bufcount = jsize * nhalo;
int bufsize = bufcount * sizeof(double);

double xbuf_left_send[bufcount];
double xbuf_rght_send[bufcount];
double xbuf_rght_recv[bufcount];
double xbuf_left_recv[bufcount];

// Packing data for the left and right communications
if (nleft != MPI_PROC_NULL) {
    int position_left = 0;
    for (int j = 0; j < jsize; j++) {
        MPI_Pack(&x[j][0], nhalo, MPI_DOUBLE, xbuf_left_send, bufsize, &position_left, MPI_COMM_WORLD);
    }
}

if (nrght != MPI_PROC_NULL) {
    int position_right = 0;
    for (int j = 0; j < jsize; j++) {
        MPI_Pack(&x[j][isize - nhalo], nhalo, MPI_DOUBLE, xbuf_rght_send, bufsize, &position_right, MPI_COMM_WORLD);
    }
}

// Sending and receiving packed data
MPI_Irecv(&xbuf_rght_recv, bufsize, MPI_PACKED, nrght, 1001, MPI_COMM_WORLD, &request[0]);
MPI_Isend(&xbuf_left_send, bufsize, MPI_PACKED, nleft, 1001, MPI_COMM_WORLD, &request[1]);
MPI_Irecv(&xbuf_left_recv, bufsize, MPI_PACKED, nleft, 1002, MPI_COMM_WORLD, &request[2]);
MPI_Isend(&xbuf_rght_send, bufsize, MPI_PACKED, nrght, 1002, MPI_COMM_WORLD, &request[3]);

MPI_Waitall(4, request, status);

// Unpacking received data into ghost cells
if (nrght != MPI_PROC_NULL) {
    int position_right = 0;
    for (int j = 0; j < jsize; j++) {
        MPI_Unpack(xbuf_rght_recv, bufsize, &position_right, &x[j][isize], nhalo, MPI_DOUBLE, MPI_COMM_WORLD);
    }
}
```

### **Key Considerations**
- **MPI_Pack and MPI_Unpack**: Used to handle non-contiguous data efficiently.
- **Synchronization**: The use of non-blocking communication (`MPI_Isend` and `MPI_Irecv`) avoids bottlenecks.

---

## **4. 3D Mesh Computations with Ghost Cell Exchanges**

### **3D Process Layout**
- 3D grid computations involve more complex communication patterns due to the additional dimension.
- Each process needs to communicate with its six direct neighbors (left, right, top, bottom, front, and back).

```c
int xcoord = rank % nprocx;
int ycoord = (rank / nprocx) % nprocy;
int zcoord = rank / (nprocx * nprocy);

int nleft = (xcoord > 0) ? rank - 1 : MPI_PROC_NULL;
int nrght = (xcoord < nprocx - 1) ? rank + 1 : MPI_PROC_NULL;
int nbot  = (ycoord > 0) ? rank - nprocx : MPI_PROC_NULL;
int ntop  = (ycoord < nprocy - 1) ? rank + nprocx : MPI_PROC_NULL;
int nfrnt = (zcoord > 0) ? rank - nprocx * nprocy : MPI_PROC_NULL;
int nback = (zcoord < nprocz - 1) ? rank + nprocx * nprocy : MPI_PROC_NULL;
```

### **Handling Ghost Cell Updates in 3D**
- Communication for ghost cell updates in 3D involves **layer-by-layer exchanges**.
- Updates are more complex and often require synchronization between directional exchanges.

### **Considerations for Ghost Cell Exchanges in 3D**
- **Data Contiguity**: Using data types that reflect the natural memory layout (e.g., rows in a grid) improves performance.
- **MPI Data Types**: Leveraging MPI's built-in data types and derived types can streamline communication.

### **Performance Optimization Tips**
1. **Non-blocking Operations**: Use `MPI_Isend` and `MPI_Irecv` to overlap computation and communication.
2. **Minimal Synchronization**: Reduce unnecessary barriers to avoid stalling parallel processes.
3. **Optimal Buffer Packing**: Use techniques like **MPI_Pack** to handle non-contiguous data transfers efficiently.

---

## **5. Summary of Data Parallel Techniques**

- **Stream Triad**: A simple yet powerful benchmark to understand memory bandwidth

 limitations.
- **Ghost Cell Techniques**: Critical for enabling efficient inter-process communication in multi-dimensional grid computations.
- **MPI Best Practices**:
  - Use collective operations judiciously.
  - Leverage data locality and non-blocking communication to maximize throughput.
  - Optimize memory layout and access patterns to reduce latency.

### **Further Reading**
- **MPI Official Documentation**: [MPI Forum](https://www.mpi-forum.org)
- **High-Performance Computing Tutorials**: Guides from institutions like [Lawrence Livermore National Laboratory](https://computing.llnl.gov/tutorials/).

These notes provide a comprehensive guide to data parallel techniques in MPI, emphasizing the practical implementation and optimization of communication patterns in multi-dimensional computational grids.