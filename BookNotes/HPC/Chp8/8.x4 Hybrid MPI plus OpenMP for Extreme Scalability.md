
Hybrid parallelization involves combining two or more parallel techniques to achieve better scalability and performance in high-performance computing (HPC). In this section, we'll explore **Hybrid MPI (Message Passing Interface) and OpenMP (Open Multi-Processing)**, which combines distributed memory parallelism with shared memory parallelism to optimize the utilization of computational resources.

#### **Key Concepts:**
- **Hybrid Parallelization:** Combining distributed memory (MPI) and shared memory (OpenMP) techniques.
- **Pure MPI (MPI Everywhere):** An approach using only MPI for parallelization, which may be inefficient at extreme scales.
- **MPI + OpenMP:** Uses MPI between nodes and OpenMP threads within nodes to improve memory efficiency and reduce communication overhead.

## **1. Benefits of Hybrid MPI plus OpenMP**
Using OpenMP with MPI offers several key benefits, especially for extreme-scale applications that run on thousands of cores:

### **1.1 Reduced Ghost Cell Communication**
- **Ghost cells** (also known as halo cells) are used to store boundary data exchanged between adjacent subdomains in spatial decompositions.
- By introducing OpenMP threads within MPI processes, the number of ghost cells required between nodes is reduced, which decreases communication overhead.

### **1.2 Memory Efficiency**
- Fewer MPI processes mean fewer communication buffers, leading to lower memory consumption.
- Shared memory in OpenMP allows data to be accessed directly by all threads in a node, reducing the need for data duplication.

### **1.3 Network Interface Card (NIC) Contention**
- Hybrid models reduce contention for the NIC by minimizing unnecessary data transfers within the node.
- Communication between MPI ranks is less frequent when they are replaced by OpenMP threads.

### **1.4 Improved Load Balancing and Affinity**
- Threads can balance workloads more effectively within a node's Non-Uniform Memory Access (NUMA) regions.
- Hardware-specific features are more accessible with threads, leading to better performance on architectures like Intelâ€™s Knights Landing (KNL).

### **1.5 Optimized Tree-Based Communications**
- OpenMP threads reduce the depth of tree-based communication structures, which improves scaling efficiency.
- This reduction leads to a lower overall communication latency in operations like reductions or broadcasts.

## **2. MPI plus OpenMP Example**

### **2.1 Initializing Hybrid MPI and OpenMP**

The transition from pure MPI to Hybrid MPI plus OpenMP starts with the initialization of MPI using `MPI_Init_thread`, which specifies the desired level of thread safety.

```c
#include <mpi.h>
#include <omp.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int provided;
    MPI_Init_thread(&argc, &argv, MPI_THREAD_FUNNELED, &provided);

    int rank, nprocs;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);

    #pragma omp parallel
    {
        #pragma omp master
        {
            if (rank == 0) {
                printf("Running with %d MPI ranks and %d OpenMP threads per rank\n",
                       nprocs, omp_get_num_threads());
            }
        }
    }

    MPI_Finalize();
    return 0;
}
```

### **2.2 Thread Safety Levels in MPI**

There are four levels of thread safety in MPI, listed from least to most thread-safe:
- **`MPI_THREAD_SINGLE`**: No threading support.
- **`MPI_THREAD_FUNNELED`**: Only the main thread can make MPI calls.
- **`MPI_THREAD_SERIALIZED`**: Multiple threads can make MPI calls, but not concurrently.
- **`MPI_THREAD_MULTIPLE`**: Fully multithreaded, allowing concurrent MPI calls from multiple threads.

**Recommendation:** Use the lowest level of thread safety required by the application to minimize overhead.

### **2.3 Computational Loop with OpenMP Threading and SIMD Vectorization**

Adding OpenMP parallelism to the computational loop enhances performance through multithreading and vectorization.

```c
// OpenMP parallel loop with SIMD vectorization for hybrid MPI + OpenMP implementation
#pragma omp parallel for
for (int j = 0; j < jsize; j++) {
    #pragma omp simd
    for (int i = 0; i < isize; i++) {
        xnew[j][i] = (x[j][i] + x[j][i-1] + x[j][i+1] + x[j-1][i] + x[j+1][i]) / 5.0;
    }
}
```

- **`#pragma omp parallel for`**: Distributes loop iterations across threads.
- **`#pragma omp simd`**: Enables Single Instruction, Multiple Data (SIMD) vectorization for the inner loop to optimize data processing.

## **3. Setting Affinity for Hybrid MPI plus OpenMP**

### **3.1 Affinity and Thread Placement**

Setting thread affinity is crucial for performance in hybrid parallel applications to ensure efficient use of computational resources.

```bash
export OMP_NUM_THREADS=22
export OMP_PLACES=cores
export OMP_PROC_BIND=true

mpirun -n 4 --bind-to socket ./CartExchange -x 2 -y 2 -i 20000 -j 20000 -h 2 -t -c
```

- **`OMP_NUM_THREADS=22`**: Configures the number of OpenMP threads per MPI process.
- **`--bind-to socket`**: Ensures that each MPI process is bound to a specific socket, improving memory locality and reducing communication costs.

### **3.2 Understanding Affinity**
- **Affinity**: Controls how processes and threads are mapped to hardware resources, reducing overhead from task migration.
- **Pinning**: Ensures that threads run on specific cores to maximize cache efficiency and minimize latency.

## **4. Hybrid MPI plus OpenMP Performance Analysis**

### **4.1 Scalability Considerations**

The effectiveness of hybrid parallelism depends on the architecture and workload characteristics:
- **Fewer MPI Processes**: Improves scalability on many-core systems like Intel KNL by reducing memory footprint and communication costs.
- **Thread Efficiency**: OpenMP threads share memory, reducing data duplication but requiring careful consideration of data locality to prevent NUMA effects.

### **4.2 Comparison of Hybrid MPI and Pure MPI**

- **Memory Usage**: Hybrid MPI + OpenMP significantly reduces memory consumption compared to pure MPI due to fewer MPI ranks.
- **Communication Overhead**: OpenMP threads communicate through shared memory, which is faster than inter-process communication in pure MPI.

## **5. Future Directions and Advanced MPI Features**

### **5.1 Advanced MPI Capabilities**

Explore additional MPI features to further optimize hybrid applications:
- **Comm Groups**: Create specialized communicator groups for sub-tasks within the application.
- **Shared Memory in MPI**: Utilize shared memory windows for on-node communication to minimize data transfer costs.
- **One-Sided Communication**: Use `MPI_Put` and `MPI_Get` to reduce synchronization requirements in distributed memory systems.

### **5.2 Integrating GPUs with Hybrid MPI plus OpenMP**

In systems with accelerators like GPUs, hybrid MPI + OpenMP can be extended to include GPU offloading:
- **MPI + OpenMP + CUDA**: Combine MPI for distributed processing, OpenMP for threading, and CUDA for GPU acceleration to maximize computational throughput.

## **6. Practical Exercise: Enhancing the MPI plus OpenMP Model**

### **Exercise**

1. **Run Performance Tests**: Test the performance of hybrid MPI + OpenMP on different architectures (e.g., Skylake, Cascade Lake) with varying numbers of threads and MPI ranks.
2. **Experiment with Affinity Settings**: Compare the runtime performance when using different affinity settings (`--bind-to core` vs. `--bind-to socket`).

## **7. Key Takeaways for Hybrid MPI plus OpenMP**

- **Combining MPI and OpenMP**: Offers a scalable solution for exploiting both inter-node and intra-node parallelism.
- **Optimized Resource Utilization**: Makes better use of memory and hardware by leveraging thread-level parallelism within nodes.
- **Careful Tuning Required**: Achieving the best performance involves careful tuning of thread affinity, MPI rank placement, and workload distribution.

## **Conclusion**

Hybrid MPI plus OpenMP provides a pathway to extreme scalability in HPC applications by effectively utilizing both distributed and shared memory architectures. Through thoughtful implementation and optimization, hybrid models can overcome the limitations of pure MPI, offering improved memory efficiency, reduced communication costs, and better load balancing on modern multi-core and many-core systems.

This approach is essential for achieving peak performance in the most demanding computational applications, such as scientific simulations, data analysis, and machine learning workloads, making it a critical skill set for advanced HPC practitioners.