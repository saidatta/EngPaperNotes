# Staff+ Obsidian Notes: Tokenizing the Data and Fine-Tuning an LLM

## Tokenization Overview
### Definition
**Tokenization** is the process of converting a sequence of text into smaller units called **tokens** (e.g., words, subwords, or characters). The purpose of tokenization is to create numerical representations that LLMs can process. The tokens are then mapped to **input IDs**, which are numerical identifiers in the model's vocabulary.

### Why Tokenization Matters
Different LLMs use different tokenization algorithms:
- **Character-based Tokenization**: Splits text into individual characters. Useful for tasks involving languages with complex morphology or spelling correction.
- **Word-based Tokenization**: Splits text into words. Effective for tasks like **NER** and **sentiment analysis**.
- **Subword-based Tokenization**: Balances granularity and efficiency. Common in models like BERT (uses **WordPiece**) and GPT-2 (uses **Byte-Pair Encoding (BPE)**).

### Example Code for Tokenizer Initialization
```python
from transformers import AutoTokenizer

# Initializing a tokenizer for the BERT model
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

**Note**: The tokenizer should match the model architecture. A mismatch could result in poor performance or errors.

## Tokenizer Function and Preprocessing
We define a function to tokenize the text data with padding and truncation. This ensures uniform input length, which is often a requirement for transformer models like BERT.

### Padding and Truncation Explained
- **Padding**: Adding special tokens (e.g., zeros) to sequences to reach a uniform length.
  - **Post-padding**: Padding added at the end (e.g., `[1, 2, 3]` → `[1, 2, 3, 0, 0]`).
  - **Pre-padding**: Padding added at the beginning (e.g., `[1, 2, 3]` → `[0, 0, 1, 2, 3]`).

- **Truncation**: Shortening sequences that exceed a specified length.
  - **Post-truncation**: Removing tokens from the end (e.g., `[1, 2, 3, 4, 5]` → `[1, 2, 3]`).
  - **Pre-truncation**: Removing tokens from the beginning (e.g., `[1, 2, 3, 4, 5]` → `[3, 4, 5]`).

### Code for Tokenizing the Data
```python
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Apply tokenization function to the dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

### Inspecting the Tokenized Data
To visualize the output, inspect a tokenized sample:
```python
print(tokenized_datasets['train'][100]['input_ids'])
```

**Output (truncated)**:
```plaintext
[101, 12008, 27788, ..., 0, 0, 0, 0, 0]
```

The trailing zeros indicate padding, resulting from the `padding='max_length'` parameter.

## Reducing Dataset Size for Faster Training
If necessary, reduce the dataset size for quicker training:
```python
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(500))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(500))
```

## Fine-Tuning the Model
### Overview of the BERT Model
**BERT (Bidirectional Encoder Representations from Transformers)** is a transformer-based, encoder-only architecture introduced by Google in 2018. Despite its age, BERT remains widely used due to its strong capabilities in **natural language understanding (NLU)** tasks.

**Components of BERT**:
- **Encoder**: Composed of multiple layers of transformer blocks with self-attention and feed-forward layers.
- **Output Layer**: Task-specific; for classification, this is typically a linear layer that outputs logits corresponding to class labels.

### BERT Versions
- **BERTbase**: 12 transformer layers, 768 hidden units, 110M parameters.
- **BERTlarge**: 24 transformer layers, 1024 hidden units, 340M parameters.
- **Variants**: **BERT-tiny**, **BERT-mini**, **BERT-small**, **BERT-medium** (smaller, optimized versions).

### Model Pretraining Objectives
1. **Masked Language Modeling (MLM)**:
   - Randomly masks words in the input, requiring the model to predict the original word based on context.
   - Example:
     ```plaintext
     Input: "He bought a new [MASK] yesterday."
     Output: "He bought a new car yesterday."
     ```
2. **Next Sentence Prediction (NSP)**:
   - Predicts whether a sentence follows another in the original text.
   - Example:
     ```plaintext
     Input: "She loves reading books." → "Her favorite genre is fantasy."
     Prediction: Consecutive.
     ```

### Instantiating the BERT Model for Fine-Tuning
```python
import torch
from transformers import AutoModelForSequenceClassification

# Load BERT model for binary classification
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
```

**Note**: We use `AutoModelForSequenceClassification` to set up the BERT model for classification tasks. The `num_labels=2` parameter specifies binary classification.

## Training the Fine-Tuned Model
### Preparing Evaluation Metrics
Define a metric function to evaluate the model’s performance, such as **accuracy** or **F1-score**.

### Code for Model Training
```python
from transformers import TrainingArguments, Trainer

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    evaluation_strategy="epoch",
    save_total_limit=2,
    logging_dir="./logs"
)

# Instantiate the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset
)

# Train the model
trainer.train()
```

### Evaluating the Model
Evaluate the model using the `Trainer`'s built-in evaluation method:
```python
metrics = trainer.evaluate()
print(metrics)
```

**Example Metrics Output**:
```plaintext
{'eval_loss': 0.45, 'eval_accuracy': 0.85}
```

## Summary
### Key Points
- Tokenization is crucial for preprocessing text into a format that LLMs can process.
- Use **AutoTokenizer** from Hugging Face for efficient tokenization and seamless integration with various models.
- Preprocess the dataset by applying tokenization functions with padding and truncation.
- Fine-tune the BERT model by loading a pretrained instance and training it with labeled data.
- Evaluate the fine-tuned model with metrics to assess its performance on new data.

### Next Steps
- **Advanced Training**: Explore **parameter-efficient fine-tuning techniques** (e.g., **LoRA**, **prefix tuning**).
- **Fine-Tuning on Custom Data**: Apply the outlined steps to domain-specific datasets to achieve better task performance.

### References
- **Hugging Face Transformers Library**: [Transformers Documentation](https://huggingface.co/docs/transformers)
- **IMDB Dataset Card**: [IMDB on Hugging Face](https://huggingface.co/datasets/imdb)
- **BERT Architecture Paper**: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" by Devlin et al.

This guide equips Staff+ engineers with the theoretical understanding and practical code examples needed to tokenize data and fine-tune LLMs for domain-specific NLP tasks.