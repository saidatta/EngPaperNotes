# Staff+ Obsidian Notes: Evaluation Metrics for Fine-Tuned LLMs

## Evaluation Metrics Overview
Evaluating a fine-tuned LLM depends on the specific task it was trained for. While evaluating general-purpose LLMs requires measuring aspects such as **fluency**, **coherence**, and **groundedness**, task-specific evaluations—like binary classification—utilize more traditional metrics.

### Key Metrics for Binary Classification:
1. **Accuracy**: The proportion of correct predictions over the total number of predictions.
   - **Formula**: \( \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN} \)
   - Example: Given \( TP = 20, TN = 72, FP = 3, FN = 5 \), 
     \[
     \text{Accuracy} = \frac{20 + 72}{20 + 3 + 72 + 5} = 0.92
     \]

2. **Precision**: The proportion of correctly predicted positive cases out of all predicted positive cases.
   - **Formula**: \( \text{Precision} = \frac{TP}{TP + FP} \)
   - Example: 
     \[
     \text{Precision} = \frac{20}{20 + 3} = 0.87
     \]

3. **Recall** (Sensitivity): The proportion of true positive cases correctly predicted.
   - **Formula**: \( \text{Recall} = \frac{TP}{TP + FN} \)
   - Example:
     \[
     \text{Recall} = \frac{20}{20 + 5} = 0.8
     \]

4. **Specificity**: The proportion of true negative cases correctly predicted.
   - **Formula**: \( \text{Specificity} = \frac{TN}{TN + FP} \)
   - Example:
     \[
     \text{Specificity} = \frac{72}{72 + 3} = 0.96
     \]

5. **F1-Score**: The harmonic mean of precision and recall.
   - **Formula**: 
     \[
     \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
     \]
   - Example:
     \[
     \text{F1-Score} = 2 \times \frac{0.87 \times 0.8}{0.87 + 0.8} = 0.83
     \]

### Confusion Matrix Explained:
A confusion matrix is a 2x2 table that visualizes the performance of a binary classifier:
|                 | **Predicted Positive** | **Predicted Negative** |
|-----------------|-------------------------|-------------------------|
| **Actual Positive** | TP (20)                  | FN (5)                  |
| **Actual Negative** | FP (3)                   | TN (72)                 |

## Advanced Evaluation: ROC Curve and AUC
- **ROC Curve**: A plot of recall (true positive rate) vs. the false positive rate (\( \text{FP rate} = \frac{FP}{FP + TN} \)).
- **AUC (Area Under the Curve)**: Measures the ability of the model to rank positive cases higher than negative cases. A perfect model has an AUC of 1.0.

### Figure 11.3: Illustration of a ROC curve
- **Details**: The ROC curve visualizes model performance across various thresholds. A higher AUC indicates better discriminative ability.

## Implementing Accuracy Metric in Python
```python
import numpy as np
import evaluate

# Load the accuracy metric from the Hugging Face library
metric = evaluate.load("accuracy")

# Function to compute accuracy from evaluation predictions
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

### Evaluation Strategy Configuration
Set the strategy for how often evaluations occur during training.
```python
from transformers import TrainingArguments, Trainer

# Define training arguments
training_args = TrainingArguments(
    output_dir="test_trainer",
    num_train_epochs=2,
    evaluation_strategy="epoch"  # Evaluates at the end of each epoch
)
```

**Definition**:
- **Epoch**: One complete pass through the entire training dataset. It is a key hyperparameter that helps determine how well the model learns from the training data.

## Training and Saving the Model
### Trainer Class Initialization
The **`Trainer`** class in Hugging Face’s Transformers library simplifies training and evaluation:
```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
```

### Training the Model
Start the training process:
```python
trainer.train()
```

### Sample Output After Two Epochs:
```plaintext
{'eval_loss': 0.672, 'eval_accuracy': 0.58, 'epoch': 1.0}
{'eval_loss': 0.537, 'eval_accuracy': 0.82, 'epoch': 2.0}
```

**Interpretation**: After two epochs, the model's accuracy improved from 58% to 82%.

### Saving the Fine-Tuned Model
```python
trainer.save_model('models/sentiment-classifier')
```

### Loading the Model for Testing
```python
from transformers import AutoModelForSequenceClassification

# Load the saved model
model = AutoModelForSequenceClassification.from_pretrained('models/sentiment-classifier')
```

## Model Inference and Prediction
Pass a sample sentence for inference:
```python
inputs = tokenizer("I cannot stand it anymore!", return_tensors="pt")
outputs = model(**inputs)
```

### Analyzing the Logits
The model returns raw scores (logits):
```plaintext
SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6467, -0.0041]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
```

### Converting Logits to Probabilities
Use **softmax** to normalize logits:
```python
import tensorflow as tf

# Convert logits to probabilities
predictions = tf.math.softmax(outputs.logits.detach(), axis=-1)
print(predictions)
```

**Output Example**:
```plaintext
tf.Tensor([[0.6571879  0.34281212]], shape=(1, 2), dtype=float32)
```
**Interpretation**: The model predicts a negative sentiment with a 65.71% probability.

## Saving the Model to Hugging Face Hub
Enable notebook login and push the model:
```python
from huggingface_hub import notebook_login

# Login and push to the hub
notebook_login()
trainer.push_to_hub('vaalto/sentiment-classifier')
```

**Result**: The model will be published on the Hugging Face Hub for broader accessibility.

## Summary
- Evaluating LLMs requires task-specific metrics.
- For binary classification, use metrics like **accuracy**, **precision**, **recall**, and **F1-score**.
- Implement and configure evaluation during training to monitor model performance.
- Save and share models via the **Hugging Face Hub** to make them easily consumable.

### References:
- **Hugging Face Datasets**: [IMDB Dataset](https://huggingface.co/datasets/imdb)
- **Hugging Face AutoTrain**: [Documentation](https://huggingface.co/docs/autotrain/index)
- **BERT Architecture Paper**: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" by Devlin et al. [Arxiv](https://arxiv.org/abs/1810.04805)

This guide provides comprehensive information on using and configuring evaluation metrics, essential for training high-quality, task-specific LLMs.