
aliases: ["test-time-compute", "beam-search", "structured-outputs", "hallucination"]
tags: ["AI-engineering", "foundation-models", "notes", "LLM"]

This note discusses **test time compute**—the practice of generating multiple outputs at **inference** time—and how it can improve the **quality** of a model’s responses. We’ll also cover **structured outputs** (e.g., JSON, SQL), consistency issues, and hallucinations.

---
## 1. Generating Multiple Outputs
### 1.1 Best of N (Multiple Samples)
At **inference/test time**, a straightforward way to increase response quality is to **sample multiple outputs** (N outputs) and **pick the best**. This approach is sometimes called **“Best of N.”**
- **Random Variation**: You let the model generate multiple outputs via sampling, possibly with different random seeds or sampling parameters (temperature, top-p, etc.).
- **Cost Trade-off**: Generating N outputs is roughly **N times** more expensive (in compute/latency).
#### Example: Probabilistic Selection
1. Generate 5 outputs from the model using the same prompt.
2. Compare them, picking the one with:
   - **Highest average log probability** of the entire sequence, **or**  
   - **Highest reward model score**, if you have a reward model.
### 1.2 Beam Search
An alternative approach to generating multiple diverse outputs is **beam search**:
- **Maintains a “beam”** of the most promising partial sequences at each decoding step.
- Explores multiple potential paths in parallel.

```mermaid
flowchart LR
    Start[Start Token] --> Step1{{Beam of top partial sequences}}
    Step1 --> Step2{{Beam of next-step partial sequences}}
    Step2 --> Step3{{Beam of next-step partial sequences}}
    Step3 --> Final{{Final best sequences}}
```

- Typically used in **machine translation** and **dialogue** systems to find better or more likely outputs.
- Tends to focus on more **likely** (higher probability) sequences, possibly sacrificing **diversity**.
### 1.3 Increasing Diversity
Generating multiple outputs is more effective if they are **diverse**:
- Vary **temperature**, **top-p**, or **top-k** to induce variation.
- Use **different random seeds** for each sample.

---
## 2. Scoring & Selecting the Best Output

Once you generate multiple outputs, you need a **selection** mechanism:

1. **Highest Probability (Log Probability)**  
   - Compute the product (or sum of log probabilities) of all tokens in the sequence.  
   - To avoid bias toward short sequences, use **average log probability** or some length penalty.

2. **Reward Model**  
   - Score each output with a **reward model** (learned or rule-based).  
   - Pick the highest-scoring output.

3. **Application-Specific Heuristics**  
   - E.g., pick the **shortest** or **longest** candidate if length is a factor.  
   - For text-to-SQL, pick the **valid** SQL query. If multiple are valid, choose the best.  

4. **Verification & Voting**  
   - Solve a problem multiple times and pick the **most common** or “majority vote” answer (self-consistency).

**Cost**: Each additional output can significantly increase inference cost, so production systems must balance **quality vs. cost**.

---
## 3. Verifiers & Parallel Generation
### 3.1 Verifiers
A **verifier** is a specialized model or tool that checks the correctness of an output:
- OpenAI used verifiers on math problems to boost accuracy.
- The improvement from adding a verifier can be equivalent to a huge increase in model size (e.g., a 100M-parameter model + verifier ≈ 3B model without verifier).
### 3.2 Parallel Generation for Latency
Some teams (e.g., TIFIN) generate multiple outputs **in parallel** and **immediately** return the first that passes certain checks. This can reduce **latency** for tasks that might otherwise require a long chain-of-thought.

---
## 4. Structured Outputs
A major challenge for LLMs: generating **valid** structured formats like JSON, YAML, SQL, or regular expressions. Typical use cases:
1. **Semantic Parsing**  
   - E.g., **text-to-SQL**, **text-to-regex**, classification with specific schema.
2. **Downstream Pipelines**  
   - Model output is used by other apps; must be **machine-readable**.
### 4.1 Approaches for Structured Generation
1. **Prompting**  
   - Simply instruct the model: “Output valid JSON.”  
   - Not guaranteed to be 100% accurate—common approach but can fail.
2. **Post-processing**  
   - Fix predictable mistakes in a **defensive** manner (e.g., missing brackets).
   - E.g., LinkedIn’s “defensive YAML parser.”
3. **Constrained Sampling**  
   - At each token step, **filter** logits to **only** valid tokens as per a grammar.
   - Ensures syntactically correct output but can be **complex** to implement (must define grammar, handle partial strings, etc.).
4. **Finetuning**  
   - **Train** the model on examples with the desired format.
   - Possibly attach a **classifier head** or specialized architecture for guaranteeing certain outputs.  
   - Most robust approach but requires **data** and resources to retrain.
### 4.2 Test Time Compute for Structure
- Keep generating outputs until you get a **valid** structured format.  
- Potentially expensive if the model fails many times or you have strict correctness requirements.
---
## 5. The Probabilistic Nature of AI
Language models produce outputs by **sampling** from a probability distribution. This leads to:
1. **Inconsistency**  
   - Same prompt → different outputs.  
   - Slightly changed prompt → drastically different output.
2. **Hallucination**  
   - Model generates content not **grounded** in facts.  
   - Possibly perpetuated by self-delusion (the model treats its own generated text as “truth”) or mismatched internal knowledge vs. labelers.
### 5.1 Inconsistency
**Scenario**: You query ChatGPT with the same question, and it yields different results each time.  
- **Mitigations**:
  - Cache responses (for repeated queries).  
  - Set **temperature=0** (greedy decoding).  
  - Fix random **seed** (though in practice, API-level control can be limited).  
  - Prompt engineering & memory systems can help unify responses.
### 5.2 Hallucination
- **Definition**: The model “makes up” information it has no basis for.  
- **Potential Causes**:
  1. **Self-delusion Hypothesis**: The model can’t distinguish between user-provided text vs. its own generated text.  
  2. **Mismatched Knowledge**: The model tries to mimic labeler answers that rely on knowledge the model doesn’t actually have.  
- **Example**: A model referencing nonexistent sources, or stating wrong facts about a known topic.  
- **Mitigations**:
  - Encourage “I don’t know” responses.  
  - Provide relevant context or retrieval from a knowledge base.  
  - Use **reinforcement** or reward models penalizing hallucinations.  
  - Keep responses concise or do multi-step verification.

**No** single perfect fix: Hallucinations remain a key challenge.

---
## 6. Summary
1. **Test Time Compute** can significantly improve model quality by:
   - Generating multiple outputs (Best of N, beam search, etc.).  
   - Using verifiers or reward models to pick the best.  
   - Balancing the **compute/time** trade-off.
2. **Structured Outputs** are crucial for many production pipelines:
   - Prompting, post-processing, or finetuning can yield consistent **JSON, SQL, regex**, etc.
   - Constrained sampling is a robust but complex solution.
3. **Probabilistic Nature**:
   - Leads to **inconsistency** and **hallucinations**.  
   - Also enables **creativity**.  
   - Ongoing research attempts to reduce hallucinations (via RL, better training data, or retrieval augmentation).
4. **Practical Advice**:
   - For important tasks, consider generating multiple outputs in parallel and selecting the best.  
   - Invest in format-checking or grammar-based approaches for structured tasks.  
   - Evaluate costs and latency carefully—**test time compute** can be expensive.

---
## 7. Code Snippets & Visualizations

### 7.1 Beam Search (Pseudo-Python)

```python
import torch
import torch.nn.functional as F

def beam_search(model, prompt, beam_size=3, max_len=20):
    # model: function that takes partial sequence and returns logits
    # prompt: initial token sequence (list of token IDs)
    # beam_size: number of beams
    # max_len: max tokens to generate
    sequences = [(prompt, 0.0)]  # each entry is (token_ids, cumulative_log_prob)

    for _ in range(max_len):
        all_candidates = []
        for seq, score in sequences:
            logits = model(seq)  # shape [vocab_size]
            probs = F.softmax(logits[-1], dim=-1)  # last token's distribution

            top_scores, top_ids = probs.topk(beam_size)
            for idx, prob in zip(top_ids, top_scores):
                new_seq = seq + [idx.item()]
                new_score = score + torch.log(prob).item()
                all_candidates.append((new_seq, new_score))

        # Keep the best beam_size sequences
        all_candidates.sort(key=lambda x: x[1], reverse=True)
        sequences = all_candidates[:beam_size]

    # Return the sequence with the highest score
    return sequences[0][0]
```
- **Note**: This simplistic snippet only ranks sequences by **log probability**. Real implementations handle length normalization, end tokens, etc.
### 7.2 “Best of N” Pseudocode

```python
def best_of_n_generation(model, prompt, n=5, max_len=20):
    candidates = []
    for i in range(n):
        out_tokens = sample_sequence(model, prompt, max_len=max_len)
        # sample_sequence uses temperature or top-p
        log_prob = calculate_avg_logprob(model, prompt, out_tokens)
        candidates.append((out_tokens, log_prob))

    # Pick candidate with highest average log probability
    candidates.sort(key=lambda x: x[1], reverse=True)
    best_sequence = candidates[0][0]
    return best_sequence
```

### 7.3 Constrained Sampling (Schematic)

```python
def constrained_sample(model, valid_tokens_fn, context, max_len=20):
    out_tokens = []
    for _ in range(max_len):
        logits = model(context + out_tokens)
        # Filter out invalid tokens
        valid_indices = valid_tokens_fn(context + out_tokens)
        filtered_logits = torch.full_like(logits[-1], float('-inf'))
        filtered_logits[valid_indices] = logits[-1][valid_indices]
        
        probs = F.softmax(filtered_logits, dim=-1)
        next_token = torch.multinomial(probs, 1).item()
        out_tokens.append(next_token)

        # Possibly break if end token or structure is complete
        if next_token == END_TOKEN:
            break

    return out_tokens
```
- `valid_tokens_fn` is a function that returns which token IDs are allowed next given the partially built output.
---
## 8. Further Reading
- **Cobbe et al. (2021)** – _Training verifiers for math problems_  
- **Snell et al. (2024)** – _DeepMind on scaling test time compute_  
- **Goyal et al. (2016)** – _Early mentions of hallucination in NLG_  
- **LinkedIn’s Defensive YAML Parsing** – Bottaro & Ramgopal (2020)  
- **OpenAI Best-of Parameter (`best_of`)** – official docs  