### **: Guaranteed Scheduling, Lottery Scheduling, and Fair-Share Scheduling**

### **1. Guaranteed Scheduling**

**Guaranteed Scheduling** provides a system in which the CPU resources are fairly divided among users or processes. The fundamental idea is to ensure that each process gets a predictable share of the CPU time. 

#### **Concepts:**
- Each process (or user) gets **1/n** of the CPU time, where **n** is the number of active users or processes.
- The system keeps track of how much CPU time each process has consumed and compares it to what they are entitled to.
- The process with the **lowest ratio** of actual CPU time consumed to entitled CPU time is chosen next.

#### **CPU Time Ratio Calculation**:
- **Entitled CPU Time**: \( \text{Entitled CPU Time} = \frac{\text{Time since creation}}{n} \)
- **Actual CPU Time**: The actual CPU time a process has consumed.
- The scheduler selects the process with the lowest ratio of \( \frac{\text{Actual CPU Time}}{\text{Entitled CPU Time}} \).

#### **Example:**
If there are 3 processes, A, B, and C, running on the system, and each is entitled to **1/3rd** of the CPU:

- Suppose the CPU time since creation is 60 seconds.
- Entitled time for each process is \( \frac{60}{3} = 20 \) seconds.
- If process A has used 15 seconds, B has used 25 seconds, and C has used 10 seconds, their ratios would be:
  - A: \( \frac{15}{20} = 0.75 \)
  - B: \( \frac{25}{20} = 1.25 \)
  - C: \( \frac{10}{20} = 0.5 \)
  
Process C will be scheduled next since it has the lowest ratio.

#### **CFS in Linux**:
In **Completely Fair Scheduling (CFS)** used by Linux, the scheduler maintains the execution time of each process in an efficient **red-black tree**. The process with the **least spent execution time** is chosen to run next. After a process uses up its time slot, it is reinserted into the tree based on its new execution time.

---

### **2. Lottery Scheduling**

**Lottery Scheduling** is a probabilistic scheduling algorithm that gives processes a set of lottery tickets. The CPU time is allocated randomly by drawing a ticket, and the process holding the winning ticket gets the CPU for the next time slice.

#### **Key Features**:
- **Randomized scheduling**: Processes receive tickets proportional to their priority.
- **Fair share**: Each process has a probability of winning proportional to the number of tickets it holds.
- **Responsive to new processes**: A newly arriving process can quickly start running based on its ticket allocation.
  
#### **Rust Example**:

```rust
use rand::Rng;

struct Process {
    id: usize,
    tickets: usize,
}

fn lottery_scheduling(processes: Vec<Process>, rounds: usize) {
    let total_tickets: usize = processes.iter().map(|p| p.tickets).sum();
    
    for round in 0..rounds {
        let winning_ticket = rand::thread_rng().gen_range(0..total_tickets);
        let mut current_ticket = 0;
        
        for process in &processes {
            current_ticket += process.tickets;
            if winning_ticket < current_ticket {
                println!("Round {}: Process {} wins the lottery!", round, process.id);
                break;
            }
        }
    }
}

fn main() {
    let processes = vec![
        Process { id: 1, tickets: 10 },
        Process { id: 2, tickets: 20 },
        Process { id: 3, tickets: 30 },
    ];
    lottery_scheduling(processes, 5);
}
```

#### **Properties**:
- **Ticket Transfers**: Processes can transfer tickets to another. For example, a client process can transfer tickets to a server process to ensure faster responses.
- **Proportional Resource Allocation**: A process holding \( \frac{k}{n} \) tickets gets \( \frac{k}{n} \) of the CPU cycles.
- **Responsive to System Changes**: When a new process is added, it immediately participates in the next lottery draw.

---

### **3. Fair-Share Scheduling**

In **Fair-Share Scheduling**, each **user** gets a guaranteed share of CPU time, rather than individual processes. The CPU is divided among users, and each user’s processes share their user’s allocated CPU time.

#### **Characteristics**:
- If user 1 starts four processes and user 2 starts one, **user 1** will not consume 80% of the CPU while user 2 only gets 20%. Instead, user 1’s four processes share 50% of the CPU, while user 2’s single process gets 50%.
  
#### **Example**:
Consider the following scheduling scenario with two users:
- **User 1** has four processes: A, B, C, D.
- **User 2** has one process: E.
  
Fair-share scheduling could alternate CPU time like so:
```
A -> E -> B -> E -> C -> E -> D -> E
```
This ensures that **both users** get their entitled share of the CPU.

#### **Rust Example**:

```rust
struct Process {
    id: usize,
    user: usize,
    burst_time: usize,
}

fn fair_share_scheduling(processes: Vec<Process>, users: Vec<usize>, cycles: usize) {
    let mut user_cycle_map: std::collections::HashMap<usize, Vec<Process>> = std::collections::HashMap::new();
    
    // Group processes by user
    for process in processes {
        user_cycle_map.entry(process.user).or_insert(vec![]).push(process);
    }
    
    // Fair-share round-robin per user
    for _ in 0..cycles {
        for user in &users {
            if let Some(user_processes) = user_cycle_map.get_mut(user) {
                if let Some(process) = user_processes.pop() {
                    println!("User {} running process {}", user, process.id);
                    user_processes.insert(0, process); // Reinsert process in the back of the queue
                }
            }
        }
    }
}

fn main() {
    let processes = vec![
        Process { id: 1, user: 1, burst_time: 10 },
        Process { id: 2, user: 1, burst_time: 15 },
        Process { id: 3, user: 1, burst_time: 20 },
        Process { id: 4, user: 1, burst_time: 25 },
        Process { id: 5, user: 2, burst_time: 30 },
    ];
    
    let users = vec![1, 2];
    
    fair_share_scheduling(processes, users, 10);
}
```

---

### **Scheduling in Real-Time Systems**

**Real-Time Systems** require processes to be completed within strict time constraints. Real-time scheduling ensures that tasks are completed before their deadlines.

#### **Types of Real-Time Systems**:
- **Hard Real-Time**: Missing a deadline is catastrophic (e.g., autopilot systems).
- **Soft Real-Time**: Missing a deadline is undesirable but tolerable (e.g., multimedia streaming).

#### **Schedulability**:
A real-time system is schedulable if the CPU can handle all incoming tasks without missing deadlines.

- For **periodic tasks**, if a task **i** occurs every **Pi** seconds and requires **Ci** seconds of CPU time, the system is schedulable if:
  \[
  \sum_{i=1}^{m} \frac{C_i}{P_i} \leq 1
  \]
  
  If the sum of **CPU time requirements divided by periods** is less than or equal to 1, the system can meet all deadlines.

#### **Example**:
Consider three periodic tasks:
- Task 1: **Period** = 100 ms, **Execution time** = 50 ms
- Task 2: **Period** = 200 ms, **Execution time** = 30 ms
- Task 3: **Period** = 500 ms, **Execution time** = 100 ms

The system is schedulable because:
\[
\frac{50}{100} + \frac{30}{200} + \frac{100}{500} = 0.5 + 0.15 + 0.2 = 0.85 \leq 1
\]

#### **Static vs. Dynamic Scheduling**:
- **Static Scheduling**: Decisions are made before execution starts, typically for predictable workloads.
- **Dynamic Scheduling**: Decisions are made at runtime, suitable for less predictable workloads.

#### **Conclusion**:
These scheduling algorithms help balance system resources among processes, users, or meet deadlines in real-time systems. Depending on the system’s needs, different algorithms are selected to optimize for fairness, efficiency, or deadline adherence.

