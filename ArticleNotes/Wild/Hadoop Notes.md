
### Basic Concepts & Interfaces

1. **RecordReader**: This is an interface used in Hadoop MapReduce for reading input records from a data source. It's used in the Map phase of the MapReduce job. The `next(Writable key, Writable value)` method is used to read the next key-value pair from the input. The `getPos()` method returns the current position in the input, and `close()` is used to close the `RecordReader` when there are no more records to read.

2. **RecordWriter**: This is an interface used in Hadoop MapReduce for writing output records to an output file. It's used in the Reduce phase of the MapReduce job. The `write(WritableComparable key, Writable value)` method is used to write a key-value pair to the output. The `close(Reporter reporter)` method is used to close the `RecordWriter`.

3. **OutputCollector**: This is an interface used in Hadoop MapReduce for collecting output data during the Map or Reduce phase. The `collect(key, val)` method is used to add a key-value pair to the output.

4. **InputFormat**: This is an abstract base class for input formats. Input formats define how to read data from the data source and create a `RecordReader` for that data source. There are several built-in input formats in Hadoop, such as `TextInputFormat` for plain text files and `SequenceFileInputFormat` for Hadoop's SequenceFile format.

5. **OutputFormat**: This is an abstract base class for output formats. Output formats define how to write output data to the output destination and create a `RecordWriter` for that destination. There are several built-in output formats in Hadoop, such as `TextOutputFormat` for plain text files and `SequenceFileOutputFormat` for Hadoop's SequenceFile format.

These interfaces and classes are fundamental to the operation of a MapReduce job in Hadoop. They allow the MapReduce framework to handle the details of reading input data, writing output data, and collecting intermediate data, so that you can focus on writing the Map and Reduce functions that define the actual data processing logic.

### Basic Classes
Sure, let's dive a bit deeper into these classes and their roles in HDFS:

1. **Block**: As mentioned, a `Block` represents a chunk of data in HDFS. When you store a file in HDFS, it's split into several blocks, and each block is stored independently. This allows HDFS to distribute the data across multiple nodes in the cluster, improving performance and fault tolerance. Here's a simple example of how you might create a new `Block`:

   ```java
   long blockId = 123456L; // This would usually be generated by HDFS
   long length = 1024L; // The length of the block in bytes
   Block block = new Block(blockId, length);
   ```

2. **BlockCommand**: A `BlockCommand` is used by the NameNode (the master server in HDFS) to instruct DataNodes (the worker servers) to perform operations on one or more blocks. For example, the NameNode might send a `BlockCommand` to tell a DataNode to delete a block, or to replicate a block on another DataNode. Here's an example of creating a `BlockCommand` to invalidate (delete) a block:

   ```java
   Block[] blocks = {block}; // The blocks to invalidate
   BlockCommand command = new BlockCommand(DatanodeProtocol.DNA_INVALIDATE, null, blocks);
   ```

3. **LocatedBlock**: A `LocatedBlock` represents a block along with the locations of its replicas. This is used when reading data from HDFS. The NameNode uses the block's ID to find all the DataNodes holding replicas of the block, then returns a `LocatedBlock` to the client. The client can then read the data from the closest DataNode. Here's an example of creating a `LocatedBlock`:

   ```java
   DatanodeInfo[] locations = {datanodeInfo}; // The locations of the block's replicas
   LocatedBlock locatedBlock = new LocatedBlock(block, locations);
   ```

4. **DataNodeInfo**: A `DataNodeInfo` contains information about a DataNode, such as its name, IP address, and the amount of storage it has available. This information is used by the NameNode to manage the distribution of blocks across the cluster. Here's an example of creating a `DataNodeInfo`:

   ```java
   String name = "datanode1"; // The name of the DataNode
   String storageID = "storage1"; // The ID of the DataNode's storage
   String host = "192.168.1.1"; // The DataNode's IP address
   DataNodeInfo datanodeInfo = new DataNodeInfo(name, storageID, host);
   ```

5. **DataNodeReport**: A `DataNodeReport` is a report from a DataNode to the NameNode, containing information about the DataNode's status and the blocks it's storing. This is used by the NameNode to monitor the health of the cluster and to make decisions about where to store new blocks. Here's an example of creating a `DataNodeReport`:

   ```java
   DataNodeReport report = new DataNodeReport(datanodeInfo, DatanodeReportType.LIVE, "No problems");
   ```

These classes are all part of the internal workings of HDFS, and you wouldn't normally interact with them directly when using HDFS. However, understanding them can help you get a better idea of how HDFS works under the hood.

### FileSystem

The `FileSystem` class in Hadoop is an abstract base class that provides a common interface for interacting with a file system. It is extended by other classes like `LocalFileSystem` and `DistributedFileSystem` to provide implementations for specific types of file systems.

Here's a brief explanation of the points you mentioned:

1. **HashMap: name-> filesystem**: This is a map that keeps track of all the file systems that are currently in use. The key to this map is a string that identifies the file system. For a local file system, this key is "Local". For a distributed file system, the key is "Host:Port", where "Host" is the hostname of the NameNode (the master node in a Hadoop cluster that manages the file system metadata) and "Port" is the port number on which the NameNode is listening.

2. **Inherited from Configured class**: The `FileSystem` class extends the `Configured` class. This means that it can use the methods provided by `Configured` to load configuration parameters from a `Configuration` object. These parameters can be used to configure the behavior of the file system.

3. **Checksums**: To ensure the integrity of the data, Hadoop generates a checksum for each file. This checksum is stored in a separate hidden file with the extension ".crc". When data is read from the file system, the checksum of the data is calculated and compared with the stored checksum. If the two checksums do not match, it indicates that the data has been corrupted.

The `FileSystem` class and its subclasses are a crucial part of Hadoop's architecture. They provide a unified interface for reading and writing data, regardless of whether the underlying file system is a local file system or a distributed file system like HDFS.

-----
## block

---

### [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#blkid%E5%92%8Clen "blkid and len")blkid and len

**Block** is the basic unit of HDFS file storage. It has two key attributes `blkid`and `len`. The former is used to identify a file on an operating system, and the file name is `"blk_" + String.valueOf(blkid)`concatenated ; the latter is the length of the file in bytes.  
It abstracts away the two fundamental dimensions of **storage** , origin and size. The same goes for variables, arrays, files, etc.

### [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#%E6%B3%A8%E5%86%8C%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95 "register factory method")register factory method

Another interesting point is that all classes that implement the Writable interface have registered a factory method. What is the specific use, I will add it later.

1   
2   
3   
4   
5   
6   
7  

static {                                       // register a ctor  
 WritableFactories. setFactory   
   (Block. class, new WritableFactory () { public Writable newInstance () { return new Block (); }}); }  
       
          
  
  

### [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#%E5%BA%8F%E5%88%97%E5%8C%96 "Serialization")Serialization

Implementation `Writable`Use Java's serialization interface ( `DataOutput`) to realize the serialization and deserialization of the basic fields of Block.  
Each class to be serialized independently implements its own pair of serialization and deserialization functions, which is a commonly used basic design. When I practiced writing desktop programs, I wanted to store some control information as xml. The idea I used was similar to this Same, but did a poor job of not defining the Writable interface as an abstraction for this behavior.

**implements `Comparable`(presumably for being comparable when indexed) and `Writable`the interface**

## [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#BlockCommand "BlockCommand")BlockCommand

---

An instruction parameter encapsulation, the command acts on a series of Blocks `DataNode`under ; there are two operations, move this group of Blocks to another `DataNode`, or mark the reorganized Blocks as invalid.

### [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#%E5%AE%9E%E7%8E%B0 "accomplish")accomplish

1   
2   
3   
4  

boolean  transferBlocks  =  false ;   
boolean  invalidateBlocks  =  false ;   
Block blocks[];   
DatanodeInfo targets[][];  

Use two flag variables to indicate what kind of operation;  
use two arrays to store operation objects.

Then through constructor overloading, three constructors are given, no parameters, move command or invalidation command. And provides read permission for each field.

**Implements `Writable`the interface** .

### [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#%E6%80%BB%E7%BB%93 "Summarize")Summarize

To encapsulate the basic information of a simple command, use the constructor to accept parameters, determine the operation type and operation object; use flag variable + array object to implement.  
Bundling a set of data together according to certain semantics is also convenient when passing between functions, and the reusability is better.

## [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#LocatedBlock "Located Block")Located Block

---

A data pair, including a `Block`and `DataNode`the information of where several replicates are located.

1   
2  

Block b;   
DatanodeInfo locs[];  

It is equivalent to maintaining a pointer from a logical block to its storage location, which is used to locate the physical location of the block.

**Implements `Writable`the interface** .

## [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#DataNodeInfo "DataNodeInfo")DataNodeInfo

---

Contains the status information `DataNode`of (total size, remaining size, last update time), uses the name (customized `UTF8`storage `host:port`) as ID, and maintains `Block`all to find the tree ( `TreeSet`should be a red-black tree, sorted by the blkid `Block`of ).

### [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#%E5%85%B3%E9%94%AE%E5%87%BD%E6%95%B0 "key function")key function

Update status information ( **a heartbeat** . The name is good - it seems that DataNode is saying, "I am still alive, my basic signs are as follows, balabala", vivid and easy to remember.

1   
2   
3   
4   
5  

public  void  updateHeartbeat ( long capacity, long remaining) { this .capacityBytes = capacity; this .remainingBytes = remaining; this .lastUpdate = System.currentTimeMillis() ;  
     
     
     
  

**Implements `Comparable`and `Writable`(interestingly, blocks are not serialized) interfaces**

## [](https://www.qtmuniao.com/2017/07/02/hadoop-source-DFS/#DataNodeReport "DataNodeReport")DataNodeReport

---

a [POJO](https://martinfowler.com/bliki/POJO.html), Haha, I want to laugh when I think of the origin of this name, Uncle Ma is really talented and unique. You can see from its fields that this is a simple encapsulation of heartbeat source + heartbeat information, each field has package-level access rights, and several public read methods are also provided.

1   
2   
3   
4   
5  

String name;   
String host;   
long capacity;   
long remaining;   
long lastUpdate;  

`DataNodeInfo`ID plus heartbeat information.  
Finally, there is a toString function, which is for reporting after all.
