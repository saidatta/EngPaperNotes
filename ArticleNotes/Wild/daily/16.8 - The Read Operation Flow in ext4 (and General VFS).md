aliases: [Linux Read Operation, ext4, Page Cache, Readahead, I/O Stack Execution]
tags: [Linux, Kernel, VFS, ext4, Page Cache, bio, Block Layer]

This note digs into how **read** system calls flow through the Linux kernel I/O stack, focusing on the **ext4** file system as an example. It also illustrates **page cache** usage, the **readahead** mechanism, and how we eventually end up submitting a `bio` to the lower layers.

---
## 1. The VFS `read(2)` Entry Points

### 1.1 Syscall Implementation

```c
SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count)
{
    return ksys_read(fd, buf, count);
}
```
- The glibc `read()` call invokes the kernel’s `sys_read()` (macro-ed to `SYSCALL_DEFINE3(read, ...)`).
- Internally calls `ksys_read()`, which does minimal logic and calls:

```c
ssize_t ksys_read(unsigned int fd, char __user *buf, size_t count)
{
    ...
    ret = vfs_read(f.file, buf, count, ppos);
    ...
}
```

### 1.2 `vfs_read()`

```c
ssize_t vfs_read(struct file *file, char __user *buf, size_t count, loff_t *pos)
{
    if (file->f_op->read)
        ret = file->f_op->read(file, buf, count, pos);
    else if (file->f_op->read_iter)
        ret = new_sync_read(file, buf, count, pos);
    else
        ret = -EINVAL;
    ...
}
```
- If the file’s `f_op->read` exists, call it, else fallback to `f_op->read_iter` via `new_sync_read()`.
- In modern file systems, **`read_iter`** is typically implemented (the kernel is shifting to “iter-based” I/O).  

### 1.3 For ext4

In **ext4**, the main file operations structure is:

```c
const struct file_operations ext4_file_operations = {
    .read_iter    = ext4_file_read_iter,
    .write_iter   = ext4_file_write_iter,
    /* ... */
};
```

Hence, we call:

```c
static ssize_t ext4_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
{
    return generic_file_read_iter(iocb, to);
}
```

---

## 2. `generic_file_read_iter()`

```c
ssize_t generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
{
    /* If O_DIRECT set, do Direct I/O path, skipping page cache. */
    if (iocb->ki_flags & IOCB_DIRECT) {
        ...
    }

    /* Otherwise, do buffered I/O via page cache: */
    return filemap_read(iocb, iter, retval);
}
```
- If `IOCB_DIRECT` is set, the read bypasses the page cache (Direct I/O).
- Otherwise, it uses **`filemap_read()`** for buffered I/O (the common path).

---

## 3. `filemap_read()`

```c
ssize_t filemap_read(struct kiocb *iocb, struct iov_iter *iter, ssize_t already_read)
{
    do {
        error = filemap_get_pages(iocb, iter->count, &fbatch, false);
        if (error < 0)
            break;

        /* Now copy data from page cache pages into user buffer: */
        for (i = 0; i < folio_batch_count(&fbatch); i++) {
            struct folio *folio = fbatch.folios[i];
            /* figure out offset, length, then copy: */
            copied = copy_folio_to_iter(folio, offset, bytes, iter);

            already_read += copied;
            iocb->ki_pos += copied;
            if (copied < bytes) {
                error = -EFAULT;
                break;
            }
        }

        /* put_folios: reduce reference count on each folio: */
        for (i = 0; i < folio_batch_count(&fbatch); i++)
            folio_put(fbatch.folios[i]);

    } while (iov_iter_count(iter) && iocb->ki_pos < isize && !error);

    /* Update last-access time, do some housekeeping */
    file_accessed(filp);
    ...
    return already_read ? already_read : error;
}
```
**Key Steps**:
1. **`filemap_get_pages()`** tries to find or fetch the needed pages in the page cache (and triggers readahead).  
2. Then we do **`copy_folio_to_iter()`** to transfer data from the cache into user-space memory.  
3. If the data wasn’t in the cache, we do synchronous or asynchronous I/O to bring it in.

---

## 4. `filemap_get_pages()`

```c
static int filemap_get_pages(struct kiocb *iocb, size_t count,
                             struct folio_batch *fbatch, bool need_uptodate)
{
    // 1) Attempt page cache batch load
    filemap_get_read_batch(mapping, index, last_index - 1, fbatch);

    // 2) If not found (cache miss), do sync readahead:
    if (!folio_batch_count(fbatch)) {
        page_cache_sync_readahead(mapping, ra, filp, index, last_index - index);
        filemap_get_read_batch(...);
    }

    // 3) If still no pages found, we might allocate a new folio & block waiting for I/O
    if (!folio_batch_count(fbatch)) {
        filemap_create_folio();
        // ...
    }

    // 4) If we find a page flagged with PG_readahead, do asynchronous readahead
    if (folio_test_readahead(folio)) {
        filemap_readahead(...);
    }

    // 5) If page not uptodate, block waiting for I/O to complete
    if (!folio_test_uptodate(folio)) {
        filemap_update_page(...);
    }
    return 0;
}
```

Essentially:
1. **Try** to get pages from the cache via `filemap_get_read_batch()`.
2. If not present, sync readahead from disk → re-check cache.
3. If still missing, allocate new folio and do a blocking read.
4. If the last page in batch is flagged `PG_readahead`, do an async readahead for subsequent pages.
5. If a page is not up-to-date, we block until the I/O finishes.

---

## 5. Readahead Mechanism

### 5.1 `page_cache_sync_readahead()`

```c
void page_cache_sync_readahead(struct address_space *mapping,
       struct file_ra_state *ra, struct file *file, pgoff_t index,
       unsigned long req_count)
{
    // If readahead is disabled or queue congested, fallback
    ondemand_readahead(..., req_count); 
}
```
- “Synchronous” readahead: calls `ondemand_readahead()`, which sets up a read from disk for the range the process is about to read.  

### 5.2 `filemap_readahead()` (Asynchronous)

```c
static int filemap_readahead(struct kiocb *iocb, struct file *file,
       struct address_space *mapping, struct folio *folio,
       pgoff_t last_index)
{
    page_cache_async_ra(ractl, folio, last_index - folio->index);
    return 0;
}
```

If we see the final page had `PG_readahead`, the kernel triggers an **asynchronous** readahead beyond the current request. This helps performance for sequential reads, but if the queue is congested, the kernel might skip readahead.

---

## 6. Down to ext4’s `read_folio()` and `readahead()`

In **ext4**’s `address_space_operations`:

```c
static const struct address_space_operations ext4_aops = {
    .read_folio     = ext4_read_folio,
    .readahead      = ext4_readahead,
    .writepages     = ext4_writepages,
    .write_begin    = ext4_write_begin,
    .write_end      = ext4_write_end,
    /* ... */
};
```

**`read_folio()`** is invoked for a single page read:

```c
static int ext4_read_folio(struct file *file, struct folio *folio)
{
    /* Possibly read inline data first if ext4_has_inline_data() */
    if (ext4_has_inline_data(inode))
        ret = ext4_readpage_inline(inode, folio);

    /* Otherwise, do multi-page reading: */
    return ext4_mpage_readpages(inode, NULL, folio);
}
```

**`readahead()`** is invoked for batch readahead:

```c
static void ext4_readahead(struct readahead_control *rac)
{
    struct inode *inode = rac->mapping->host;

    if (ext4_has_inline_data(inode))
        return;

    ext4_mpage_readpages(inode, rac, NULL);
}
```

Ultimately both call **`ext4_mpage_readpages()`**, which constructs a **`bio`**:

```c
int ext4_mpage_readpages(struct inode *inode,
         struct readahead_control *rac, struct folio *folio)
{
    struct bio *bio = NULL;
    // merges requests, sets up bio segments
    ...
    if (bio)
        submit_bio(bio);  // send down to block layer
    return 0;
}
```

Then the block layer (generic block layer) takes over, calling `submit_bio()`. The I/O eventually is queued, scheduled, driver-level translated, and physically read from disk, populating the page with data.

---

## 7. Conclusion of Read Flow

**In summary**:

1. **sys_read** → `ksys_read` → `vfs_read` → `f_op->read_iter` → `generic_file_read_iter`  
2. → `filemap_read` → `filemap_get_pages` → possibly triggers `page_cache_sync_readahead()` / `page_cache_async_ra()`.  
3. If needed, calls **`ext4_read_folio()`** / `ext4_readahead()` → constructs a `bio`, calls `submit_bio()`.  
4. The block layer merges or schedules → the driver dispatches → disk read completes → data is in the page cache.  
5. The kernel copies data to user space (`copy_folio_to_iter()`).

Hence, the entire chain from user space to the actual device read is quite layered. The advantage is modularity: each layer can optimize. For synchronous read, the user process blocks until the needed pages are loaded into memory. For readahead, the kernel tries to get subsequent data proactively, boosting performance for sequential access.

```markdown