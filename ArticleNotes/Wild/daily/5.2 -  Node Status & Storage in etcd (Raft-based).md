## Overview
- **Core Topics**:
  - Raft Node States (Follower, Candidate, Leader, PreCandidate)  
  - State transitions in etcd’s Raft (\`becomeFollower\`, \`becomeCandidate\`, \`becomeLeader\`, etc.)  
  - Message processing (\`stepFollower\`, \`stepCandidate\`, \`stepLeader\`)  
  - Timers (\`tickElection\`, \`tickHeartbeat\`)  
  - etcd v3 Storage Architecture (Backend, BoltDB)  
  - Read transactions vs Write transactions  
  - Index / Revision management

Below, we explore how a Raft node in **etcd** transitions through various states and how etcd manages its **persistent key-value storage** using **BoltDB** (bbolt).

---

## 1. Raft Node States in etcd

A Raft node can be in one of four states:

1. **Follower**  
2. **Candidate**  
3. **Leader**  
4. **PreCandidate** (used in pre-vote scenarios to avoid repeated term increments)

### 1.1 Switching States with `becomeXxx()` Methods
Each state has a dedicated function to set up that state:

- \`becomeFollower\`
- \`becomeCandidate\`
- \`becomeLeader\`
- \`becomePreCandidate\` (not shown in detail here)

These functions do three main things:
1. **Reset** the node’s internal term or related variables (e.g., `r.reset(term)`).
2. **Assign** a new `step` function (e.g., `r.step = stepFollower`).
3. **Choose** the correct timer function (e.g., `r.tick = r.tickElection` vs `r.tick = r.tickHeartbeat`).

---

### 1.2 Follower State

```go
func (r *raft) becomeFollower(term uint64, lead uint64) {
    r.step = stepFollower
    r.reset(term)
    r.tick = r.tickElection
    r.lead = lead
    r.state = StateFollower
}
```

- **`stepFollower`**: processes incoming messages specific to the Follower role.
- **`tickElection`**: if a follower sees no leader heartbeats for too long, it triggers an election (`MsgHup`).

#### Timer (`tickElection`)
```go
func (r *raft) tickElection() {
    r.electionElapsed++
    if r.promotable() && r.pastElectionTimeout() {
        r.electionElapsed = 0
        r.Step(pb.Message{From: r.id, Type: pb.MsgHup})
    }
}
```
- On timeout, the node transitions from Follower to Candidate.

---

### 1.3 Candidate State

```go
func (r *raft) becomeCandidate() {
    r.step = stepCandidate
    r.reset(r.Term + 1)
    r.tick = r.tickElection
    r.Vote = r.id
    r.state = StateCandidate
}
```
- Uses the **same** `tickElection` logic as Follower, but now uses `stepCandidate` for message handling.
- Immediately increments its own `Term`.
- Votes for itself (`r.Vote = r.id`).

---

### 1.4 Leader State

```go
func (r *raft) becomeLeader() {
    r.step = stepLeader
    r.reset(r.Term)
    r.tick = r.tickHeartbeat
    r.lead = r.id
    r.state = StateLeader

    r.pendingConfIndex = r.raftLog.lastIndex()
    r.appendEntry(pb.Entry{Data: nil})
}
```
- Assigns `stepLeader` to handle messages.
- **Timer** function becomes `tickHeartbeat`, which sends periodic heartbeats.
- Immediately appends a blank entry to the log (common Raft pattern upon leadership).

#### Timer (`tickHeartbeat`)
```go
func (r *raft) tickHeartbeat() {
    r.heartbeatElapsed++
    r.electionElapsed++

    if r.electionElapsed >= r.electionTimeout {
        r.electionElapsed = 0
        if r.checkQuorum {
            r.Step(pb.Message{From: r.id, Type: pb.MsgCheckQuorum})
        }
    }

    if r.heartbeatElapsed >= r.heartbeatTimeout {
        r.heartbeatElapsed = 0
        r.Step(pb.Message{From: r.id, Type: pb.MsgBeat})
    }
}
```
- **`MsgCheckQuorum`**: ensures the leader still has a majority.
- **`MsgBeat`** triggers a heartbeat → `MsgHeartbeat` to followers.

---

## 2. etcd v3 Storage Overview

etcd supports **two major versions**:  
1. **v2** (legacy): in-memory store, not persisted to disk.  
2. **v3** (current): leverages an embedded KV store, typically **BoltDB** (or a fork, **bbolt**).

### 2.1 The `Backend` Abstraction

**Backend** is an interface that abstracts the underlying storage engine:

```go
type Backend interface {
    ReadTx() ReadTx
    BatchTx() BatchTx
    Snapshot() Snapshot
    Hash(...) (uint32, error)
    Size() int64
    SizeInUse() int64
    Defrag() error
    ForceCommit()
    Close() error
}
```
- etcd **defaults** to using **BoltDB** under the hood.
- Each **Backend** struct manages:
  - A `bolt.DB` connection
  - Transaction objects for **reads** (`readTx`) and **writes** (`batchTx`)
  - Periodic commit schedules

#### The `backend` struct
```go
type backend struct {
    size       int64
    sizeInUse  int64
    commits    int64

    mu         sync.RWMutex
    db         *bolt.DB

    batchInterval time.Duration
    batchLimit    int
    batchTx       *batchTxBuffered
    readTx        *readTx

    stopc chan struct{}
    donec chan struct{}

    lg *zap.Logger
}
```
- `db` → the **BoltDB** object.
- `readTx` / `batchTx` → for read or batch read-write ops.
- `batchInterval` → how often background commits happen.
- `stopc` / `donec` → goroutines signals.

---

### 2.2 Starting the Backend

```go
func newBackend(bcfg BackendConfig) *backend {
    bopts := &bolt.Options{}
    bopts.InitialMmapSize = bcfg.mmapSize()
    db, _ := bolt.Open(bcfg.Path, 0600, bopts)

    b := &backend{
        db: db,
        batchInterval: bcfg.BatchInterval,
        batchLimit:    bcfg.BatchLimit,
        readTx: &readTx{ /* ... init read buffers ... */ },
        stopc: make(chan struct{}),
        donec: make(chan struct{}),
    }
    b.batchTx = newBatchTxBuffered(b)
    go b.run()
    return b
}
```
- Opens **BoltDB** on disk, sets up `readTx` and a **buffered** `batchTx`.
- Spawns `b.run()` to periodically commit pending write transactions.

#### Background Commit Loop

```go
func (b *backend) run() {
    defer close(b.donec)
    t := time.NewTimer(b.batchInterval)
    defer t.Stop()
    for {
        select {
        case <-t.C:
        case <-b.stopc:
            b.batchTx.CommitAndStop()
            return
        }
        if b.batchTx.safePending() != 0 {
            b.batchTx.Commit()
        }
        t.Reset(b.batchInterval)
    }
}
```
- **Every** `batchInterval`, etcd commits any pending writes from the buffered write transaction.
- Or stops if `stopc` is signaled.

---

## 3. Read Transactions in etcd

### 3.1 `ReadTx` Interface
```go
type ReadTx interface {
    Lock()
    Unlock()
    UnsafeRange(bucketName []byte, key, endKey []byte, limit int64) (keys [][]byte, vals [][]byte)
    UnsafeForEach(bucketName []byte, visitor func(k, v []byte) error) error
}
```
- Provides read-only operations, e.g., `UnsafeRange()` for retrieving key-value pairs.

### 3.2 Implementation: `readTx`

```go
type readTx struct {
    mu  sync.RWMutex
    buf txReadBuffer

    txmu    sync.RWMutex
    tx      *bolt.Tx
    buckets map[string]*bolt.Bucket
}
```
- A **read** transaction can read from a memory buffer or from the actual BoltDB buckets.
- `UnsafeRange()` first checks an in-memory buffer, then falls back to the underlying BoltDB.

```go
func (rt *readTx) UnsafeRange(bucketName, key, endKey []byte, limit int64) ([][]byte, [][]byte) {
    // 1. Look up in rt.buf
    keys, vals := rt.buf.Range(bucketName, key, endKey, limit)
    if int64(len(keys)) == limit {
        return keys, vals
    }

    // 2. If more needed, read from Bolt bucket
    bn := string(bucketName)
    bucket, ok := rt.buckets[bn]
    if !ok {
        bucket = rt.tx.Bucket(bucketName)
        rt.buckets[bn] = bucket
    }

    if bucket == nil {
        return keys, vals
    }
    c := bucket.Cursor()
    k2, v2 := unsafeRange(c, key, endKey, limit-int64(len(keys)))

    // Append results
    return append(k2, keys...), append(v2, vals...)
}
```
- If the buffer doesn’t have enough data, it uses a **Bolt cursor** to fetch from disk.

---

## 4. Write Transactions in etcd

### 4.1 `BatchTx` Interface
```go
type BatchTx interface {
    ReadTx
    UnsafeCreateBucket(name []byte)
    UnsafePut(bucketName []byte, key []byte, value []byte)
    UnsafeSeqPut(bucketName []byte, key []byte, value []byte)
    UnsafeDelete(bucketName []byte, key []byte)
    Commit()
    CommitAndStop()
}
```
- Inherits from `ReadTx` but adds **write** operations (`Put`, `Delete`).
- Batching is used to group writes before committing to BoltDB.

### 4.2 `batchTx`

```go
type batchTx struct {
    sync.Mutex
    tx      *bolt.Tx
    backend *backend
    pending int
}
```
- A `batchTxBuffered` extension also exists, which caches writes in a `txWriteBuffer`.

#### `UnsafePut`

```go
func (t *batchTx) UnsafePut(bucketName []byte, key []byte, value []byte) {
    t.unsafePut(bucketName, key, value, false)
}

func (t *batchTx) unsafePut(bucketName, key, value []byte, seq bool) {
    bucket := t.tx.Bucket(bucketName)
    if err := bucket.Put(key, value); err != nil {
        plog.Fatalf("cannot put key into bucket (%v)", err)
    }
    t.pending++
}
```
- Directly calls BoltDB’s `bucket.Put(key, value)`.
- `t.pending` tracks how many writes are uncommitted.

#### Commit

```go
func (t *batchTx) Commit() {
    t.Lock()
    t.commit(false)
    t.Unlock()
}

func (t *batchTx) commit(stop bool) {
    if t.tx != nil {
        if t.pending == 0 && !stop {
            return
        }
        // Do BoltDB commit
        start := time.Now()
        err := t.tx.Commit()
        // ...
        t.pending = 0
    }
    if !stop {
        t.tx = t.backend.begin(true)
    }
}
```
- Actually writes pending data to disk.
- If `pending == 0`, no commit needed.
- After commit, a new Bolt transaction is started if not stopping.

---

## 5. Index / Revision Management

etcd maintains a **revision** concept for each key:

- Each write increments a global revision counter.
- Each key can have multiple historical revisions, accessible if you do a range query with an older revision.

Internally, etcd manages a **B-tree** (`treeIndex`) mapping from **key** → **keyIndex**, which stores revision history.

```go
func (ti *treeIndex) Get(key []byte, atRev int64) (modified, created revision, ver int64, err error) {
    keyi := &keyIndex{key: key}
    if keyi = ti.keyIndex(keyi); keyi == nil {
        return revision{}, revision{}, 0, ErrRevisionNotFound
    }
    return keyi.get(ti.lg, atRev)
}
```
- `keyIndex` then finds which generation or version is valid at `atRev`.

```go
func (ki *keyIndex) get(lg *zap.Logger, atRev int64) (modified, created revision, ver int64, err error) {
    g := ki.findGeneration(atRev)
    if g.isEmpty() {
        return revision{}, revision{}, 0, ErrRevisionNotFound
    }
    // ...
    // returns the appropriate revision record
}
```
Thus, etcd can serve historical queries or watch events by referencing older revision entries.

---

## 6. Summary

1. **Raft Node States**:
   - Each node can be **Follower**, **Candidate**, **Leader**, or **PreCandidate**.
   - \`becomeXxx\` sets timers and message handlers:
     - Follower/Candidate → \`tickElection\`
     - Leader → \`tickHeartbeat\`
2. **Message Handlers**:  
   - \`stepFollower\`, \`stepCandidate\`, \`stepLeader\`
   - Control how each node type processes Raft messages (e.g., `MsgHeartbeat`, `MsgVote`, etc.).
3. **etcd v3 Storage**:
   - **BoltDB** (bbolt) as the embedded KV store.
   - A `backend` abstraction with `readTx` (read-only) + `batchTx` (read-write).
   - Periodic commit loop merges pending writes into BoltDB.
4. **Index / Revision**:
   - Each key has versions tracked by a global revision.
   - Historical lookups are possible by specifying an older revision.
5. **Overall**:
   - etcd orchestrates high-availability data consistency via **Raft**.
   - The internal **Backend** design (with BoltDB) ensures durability and efficient reads/writes.

---

## References

- [etcd GitHub Repository](https://github.com/etcd-io/etcd)  
- [Raft Implementation in etcd’s raft package](https://github.com/etcd-io/etcd/tree/main/raft)  
- [BoltDB / bbolt Documentation](https://github.com/etcd-io/bbolt)  
- [etcd Storage Internals](https://etcd.io/docs)  

```