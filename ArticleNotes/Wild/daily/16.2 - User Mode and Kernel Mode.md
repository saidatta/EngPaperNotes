aliases: [Linux, User Mode, Kernel Mode, Virtual File System, Memory Zoning, Linux I/O Stack]
tags: [Operating Systems, Linux, Kernel, User Space, Memory, VFS]
## 1. Introduction
In **Linux**, the operating system is conceptually divided into **user mode** (user space) and **kernel mode** (kernel space). This separation enforces security and stability:  
- **User mode**: Restricted privileges, typical running context for user processes.  
- **Kernel mode**: Full privileges, runs the Linux kernel and device drivers.  

Modern Linux systems heavily rely on **virtual memory** so that processes see a large contiguous address space, while the OS protects kernel space from direct user process access.

---
## 2. Kernel Space vs. User Space

### 2.1 Basic Concepts

- **Kernel space**:
  - Top 1 GB of the 32-bit virtual address space ([0xC0000000, 0xFFFFFFFF]).
  - Can access **all** physical memory.
  - Unrestricted privileges (privilege level 0 on x86).
- **User space**:
  - Lower 3 GB of the address space ([0x00000000, 0xBFFFFFFF]).
  - Normal processes run here, restricted (privilege level 3 on x86).
  - Must request kernel services via **syscalls**, **exceptions**, or **interrupts**.

### 2.2 Privilege Levels

Intel x86 defines **4 privilege rings** (0 = highest, 3 = lowest). Linux mainly uses ring 0 for kernel mode, ring 3 for user mode. That means:

- Programs in user mode can’t directly manipulate hardware or kernel memory.  
- OS code in kernel mode can do *anything* (I/O, memory management, device config).

---

## 3. Switching Between User Mode and Kernel Mode

A user-mode process typically runs in ring 3. To get help from the kernel, it transitions briefly to ring 0:

1. **System call (trap)**:  
   - The process *explicitly* calls a syscall (like `read(2)`, `write(2)`, `malloc(3)` which calls `sbrk(2)`, etc.).
   - CPU switches to kernel mode to handle the request, then returns to user mode.

2. **Exception**:  
   - Some CPU instruction triggers a fault (e.g., divide-by-zero, page fault).
   - CPU switches to kernel mode to run the exception handler.

3. **Interrupt**:  
   - Hardware device signals the CPU (e.g. NIC, disk).  
   - CPU temporarily suspends the running process and executes interrupt service routine in kernel mode.

After kernel tasks are done, CPU returns to ring 3 to continue user processes.

---

## 4. Memory Zoning

### 4.1 Why Memory Zones?

Linux sometimes has hardware constraints like:
- Certain devices can only do DMA (Direct Memory Access) in *low* physical memory addresses.
- On 32-bit systems, not all physical memory can be permanently mapped.

Hence the kernel divides **physical memory** into distinct **zones**:

1. **ZONE_DMA**  
   - Typically for old 16MB DMA-limited devices (ISA).  
   - Can be disabled in some kernels (`CONFIG_ZONE_DMA`).

2. **ZONE_DMA32**  
   - For 32-bit devices that can only address up to 4GB.  
   - Also optional (`CONFIG_ZONE_DMA32`).

3. **ZONE_NORMAL**  
   - The "normal" range that can be **permanently** mapped into the kernel space.

4. **ZONE_HIGHMEM**  
   - On some 32-bit platforms (e.g., i386) memory beyond 896MB is "highmem."  
   - Highmem can only be **temporarily** mapped.  
   - Not used on 64-bit systems, since they have enough address space to map all physical memory.

5. **ZONE_MOVABLE**  
   - Movable pages that can be relocated (for memory hotplug or compaction).

6. **ZONE_DEVICE**  
   - Memory belonging to certain hardware devices, like GPU memory, mapped differently.

### 4.2 Example: i386 Layout

```plaintext
 0         16M          896M               2G
 +----------+------------+------------------+
 | ZONE_DMA | ZONE_NORMAL|   ZONE_HIGHMEM   |
 +----------+------------+------------------+
```

- `ZONE_DMA` occupies first 16MB for older DMA-limited hardware.
- `ZONE_NORMAL` is from 16MB to 896MB.
- Above 896MB is `ZONE_HIGHMEM`.

On 64-bit systems, typically only `ZONE_DMA`, `ZONE_NORMAL`, and maybe `ZONE_MOVABLE` exist, because the entire physical memory can be mapped.

---

## 5. System Layout

Thus, we can define the **whole** system layout in three broad layers:

1. **User space**: Processes, apps.  
2. **Kernel space**: The OS kernel, device drivers, memory management code.  
3. **Hardware**: Physical CPU, memory (with zones), devices.

---

## 6. The Linux I/O Stack Overview

### 6.1 The Famous Storage Stack Diagram

**v4.10** version (by Werner Fischer):

```plaintext
[  Storage Devices ] -> [ SCSI Layers ] -> [ Device Mapper ] -> [ FS / VFS ] ...
```

But typically shown in a large flowchart with multiple layers (block layer, I/O schedulers, etc.).

### 6.2 Simplified View

```plaintext
User Space
  | (syscall)
  v
Kernel VFS
  |-> FS driver(s) (ext4, xfs, nfs, etc.)
  |-> Pseudo-fs (proc, sysfs) or stacking (overlayfs)
  v
Block layer / I/O schedulers
  v
Device Driver
  v
Hardware
```

**In summary**:
- The **VFS** (virtual filesystem) layer abstracts each filesystem.  
- After passing through VFS and possibly block layer scheduling, requests go to the **device driver**.  
- Then the driver communicates with actual hardware (disk, SSD, or other).

---

## 7. Virtual File System (VFS)

**VFS** is a kernel subsystem that provides a unified file API interface. Multiple actual filesystems can plug into VFS by implementing standard ops and data structures (like `super_block`, `inode`, `dentry`, and `file`). From user space perspective, all files look uniform, regardless of their actual FS.

### 7.1 Filesystem Types

1. **Block-based FS**: ext2/3/4, xfs, btrfs, etc.  
2. **Network FS**: nfs, cifs, ceph, etc. (distributed filesystem).  
3. **Pseudo-fs**: proc, sysfs, pipefs, futexfs, tmpfs (virtual or memory-backed).  
4. **Stackable FS**: overlayfs, unionfs, ecryptfs.  
5. **FUSE**: Userspace FS modules (like sshfs).  

They all interoperate with the kernel via the VFS layer.

---

## 8. Summary

In **Linux**:

- The OS is split into **user mode** and **kernel mode** to ensure security and resource management.  
- **Virtual memory** manages memory addresses, dividing them between user space and kernel space. The kernel can directly access all physical memory, but it organizes it into **zones** (like `ZONE_NORMAL`, `ZONE_DMA`, etc.).  
- The **Linux I/O stack** is layered: user space syscalls → VFS → actual FS drivers → block I/O schedulers → device drivers → hardware.  
- **VFS** provides a universal interface for filesystems, letting multiple FS types (ext4, xfs, nfs, pseudo-fs) coexist.

**Next**:  
- The second part (not shown here) would address how Linux extends from the traditional I/O approach to **Zero-Copy** I/O with minimal data copying between user space and kernel space.

---

## Code & Illustrations

### 8.1 Simple Syscall Example

```c
#include <unistd.h>
#include <fcntl.h>

int main() {
    int fd = open("testfile.txt", O_RDONLY);
    char buf[1024];
    ssize_t n = read(fd, buf, sizeof(buf));
    // This read syscalls from user space -> kernel space -> VFS -> FS -> device
    close(fd);
    return 0;
}
```

### 8.2 Memory Address Diagram

```plaintext
For 32-bit Linux:
   0x00000000  +-------------------+  (start user space)
               |   user process   |
   0xBFFFFFFF  +-------------------+  (end user space)
   0xC0000000  +-------------------+  (start kernel space)
               | kernel code/data |
   0xFFFFFFFF  +-------------------+
```

---

## References & Further Reading

1. Linux kernel docs: [User Mode and Kernel Mode](https://www.kernel.org/doc/)  
2. “Understanding the Linux Kernel” by Bovet & Cesati  
3. Werner Fischer’s [Linux Storage Stack Diagram v4.10 & v6.9](https://blog.cloudical.io/)  
4. “Linux I/O stack simplified” references in kernelnewbies.org  
5. [The VFS and the Linux Kernel](https://www.kernel.org/doc/Documentation/filesystems/vfs.txt)  