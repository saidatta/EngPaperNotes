https://strikefreedom.top/archives/linux-io-stack-and-zero-copy
aliases: [Linux I/O Stack, Zero-Copy, Virtual Memory, TLB]
tags: [Operating Systems, Linux, I/O, Memory Management]
## Introduction
Modern **I/O** handling is at the core of operating systems—particularly **Linux**, which dominates the server side. As user bases explode (internet-scale traffic), **I/O-intensive** workloads (network servers, databases) often find **I/O** to be the performance bottleneck. Over 30+ years, Linux has evolved its I/O stack to become more flexible and robust.  
This article (part of a two-part series) focuses on:
1. **The entire Linux I/O stack** (from user space to kernel space to physical hardware). 
2. **Zero-Copy** I/O in Linux, which harnesses kernel innovations to minimize data copies and drastically improve throughput.
---
## 1. Computer Storage Devices
### 1.1 Hierarchical Design
We ideally want a storage device that is:
- **Fast** (CPU-speed),
- **Large** capacity,
- **Cheap** (affordable at scale).
In reality, there's an **impossible triangle**: devices that are large and cheap are slow; those that are fast are small and expensive. Modern computers adopt a **layered** or **hierarchical** design:

```mermaid
flowchart LR
   R[Registers (~KB)] --> C[CPU Cache L1/L2/L3 (~MB)]
   C --> M[Main Memory (DRAM, ~GB)]
   M --> D[Disk (SSD/HDD, ~GB-TB)]
```

1. **Registers**: ~few hundred bytes to <1 KB, same speed as CPU.  
2. **Cache**: L1, L2, L3 caches on-CPU, larger but slower than registers.  
3. **Main Memory (RAM)**: 1s-100s of GB.  
4. **Disk**: Up to TB scale, slowest (especially mechanical HDD).

### 1.2 Main Memory & Virtual Memory
For the OS, **main memory** (RAM) is the direct resource for running processes. The OS must manage it effectively. Modern systems use **virtual memory** to abstract physical memory and provide each process a continuous address space, leveraging page-based mapping and **TLB** to accelerate address translation.

---

## 2. Physical vs. Virtual Memory

### 2.1 Physical Memory
- Physical **RAM** modules on the motherboard.  
- Directly accessible by CPU via memory bus, but “raw” usage is complicated if multiple programs share.

### 2.2 Virtual Memory
> *"Any problem in CS can be solved by adding an indirect layer."* 

To handle the capacity mismatch (apps want more memory than physically present) and to isolate processes:

1. **Each process** gets a *virtual address space*.  
2. **Paging**: This space is split into pages (commonly 4 KB), mapped to **page frames** in RAM.  
3. If a page is not in RAM, a **page-fault** triggers the OS to swap it in from disk.  

**Address Translation** via an **MMU**:
- CPU issues a virtual address → MMU uses **page table** to find the corresponding physical address.  
- If page is not in memory, OS loads it from disk (swapping).  

**TLB** (Translation Lookaside Buffer):
- A small hardware cache storing recent mappings (VPN → PPN).  
- A TLB hit spares a slow page-table memory lookup.

This is crucial for performance. Without TLB, each memory reference might double the cost.

---

## 3. The Linux I/O Stack (Big Picture)

While this article references memory fundamentals, the **Linux I/O stack** extends from **user space** syscalls down to **kernel** drivers and eventually the **hardware devices**. Summarized:

1. **User space**:  
   - Program issues read/write calls. Possibly uses libraries like `fread`, `writev`, etc.
2. **System call interface**:  
   - The call transitions from user space to kernel space via a trap or interrupt.
3. **VFS layer**:  
   - Linux’s **virtual file system** that abstracts different filesystems, network sockets, etc.
4. **Filesystem / Socket / Device driver**:  
   - For files on disk, we pass through the filesystem driver.  
   - For sockets, we pass to the TCP/IP stack.  
   - For block devices, the block layer, etc.
5. **Buffer cache / Page cache** (in kernel memory):  
   - Data may be cached in kernel buffers to avoid repeated disk/swap accesses.
6. **I/O scheduler**:  
   - Orders block I/O requests for optimal performance (e.g. `cfq`, `deadline`).
7. **Device driver**:  
   - Converts generic I/O requests to device-specific commands, controlling DMA, etc.
8. **Physical hardware**:  
   - The actual disk, NIC, or other device does the final read/write.

---

## 4. Zero-Copy in Linux

### 4.1 Traditional I/O Path
Without zero-copy, data typically traverses multiple copies:
1. Device → kernel buffer → user buffer → maybe other transformations, etc.

**Example**: Reading a file from disk and sending it over a socket might involve:
- Disk read into kernel page cache,
- Copy from kernel page cache to user space buffer,
- Copy from user space buffer to kernel socket buffer,
- Then out NIC hardware buffer.

### 4.2 Zero-Copy Concept
**Zero-Copy** aims to minimize these copies. The OS tries to “map” the same memory pages directly into the final location, e.g., from kernel page cache to NIC buffers, skipping user-space copies.

**Key Mechanisms**:
1. **`sendfile()`**: Transfer data from a file descriptor to a socket FD in kernel space, bypassing user space buffer.  
2. **`splice()`**: Pipe-based zero-copy for network or file data.  
3. **`mmap()`**-based approaches: Memory maps a file into user space for direct read/writes.  
4. **`MSG_ZEROCOPY`** in newer Linux for advanced socket-based zero-copy.

### 4.3 Gains
- Less CPU usage (fewer copy instructions).  
- Fewer context switches.  
- Better throughput for large data transfers (e.g., streaming server, file server).

### 4.4 Implementation Details
1. **sendfile()**:  
   - OS reuses the page cache pages for socket output.  
   - Sums up lengths, no user-space buffer needed.  
2. **splice()** & **tee()**:  
   - Connect file descriptors with an in-kernel pipe buffer, passing data around without user copies.  
3. **MSG_ZEROCOPY** (for TCP in recent kernels):  
   - Data is pinned in user space, NIC uses scatter-gather to read from those pinned pages.

---

## 5. Summary & Forward

In modern Linux, **I/O** is heavily optimized:
1. **Virtual memory** ensures each process sees a large address space, and TLB speeds up translation.
2. The **I/O stack** is layered but has paths like **zero-copy** that avoid unnecessary copies.
3. **Zero-copy** is essential for high-volume networking or file servers, drastically lowering CPU usage.

**Next**: The second part (not included here) might delve into more advanced zero-copy techniques, specialized NIC offloads, or how the kernel merges these technologies for maximum concurrency.

---

## Code & Examples

### Example: Using `sendfile()`

```c
#include <sys/sendfile.h>
#include <fcntl.h>
#include <unistd.h>

int main() {
    int infd = open("input.data", O_RDONLY);
    int outfd = ... // e.g., a socket FD
    off_t offset = 0;
    ssize_t bytes = sendfile(outfd, infd, &offset, 4096);
    // Zero copies to user space, data is moved kernel->kernel
    close(infd);
    // ...
    return 0;
}
```

### Example: Using `splice()`

```c
#include <fcntl.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>

int main() {
    int pipefd[2];
    pipe(pipefd);

    int fd_in = open("input.data", O_RDONLY);
    int fd_out = ... // e.g., a socket FD
    // Move up to 4096 bytes from fd_in -> pipe
    ssize_t n = splice(fd_in, NULL, pipefd[1], NULL, 4096, SPLICE_F_MOVE);
    // then pipe -> fd_out
    n = splice(pipefd[0], NULL, fd_out, NULL, n, SPLICE_F_MOVE);
    // done
    close(fd_in);
    close(pipefd[0]);
    close(pipefd[1]);
    return 0;
}
```

---

## References & Further Reading

- Linux kernel docs for [`sendfile(2)`](https://man7.org/linux/man-pages/man2/sendfile.2.html), [`splice(2)`](https://man7.org/linux/man-pages/man2/splice.2.html)
- [LWN: zero-copy networking in Linux](https://lwn.net/)
- Understanding the Linux Virtual Memory system:
  - [“Linux Kernel Development” by Robert Love](https://www.amazon.com/)
  - [“Understanding the Linux Kernel” by Bovet & Cesati](https://www.amazon.com/)
- [“Computer Architecture: A Quantitative Approach”] for memory hierarchy discussions.  

```