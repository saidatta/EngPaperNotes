This note covers two major topics:

1. **`MSG_ZEROCOPY`** sends in Linux (introduced in kernel v4.14).  
2. **Direct I/O** approaches that bypass the kernel’s traditional buffering or even bypass the kernel entirely.

Both aim to **reduce CPU copying** overhead and potentially boost performance—especially for large-data or high-throughput scenarios.

---

## 1. `MSG_ZEROCOPY` for Socket Sends

### 1.1 Overview

In **Linux kernel 4.14**, **Willem de Bruijn** (Google) introduced **`MSG_ZEROCOPY`** to allow sending data from **user space** to a **TCP socket** (later UDP in kernel ≥ 5.0) with minimal copying in the kernel.

> **Key improvement**: Previous zero-copy techniques like `sendfile()`, `splice()`, etc. usually require that the user process **not** modify or even touch the data, just forward it from one FD to another. **`MSG_ZEROCOPY`** instead allows the application to **own** the buffer and transmit it out via a socket with minimal copying.

**Performance Gains** (from Willem’s paper):
- Up to **39%** improvement in large-packet netperf tests.
- Around **5–8%** improvement in real production traffic.
- Works best with **packets ~10 KB** or larger.

#### Currently Supported
- **TCP** as of kernel **4.14**  
- **UDP** support added in kernel **5.0**  

### 1.2 How It Works

1. **Enable Zero-Copy on the Socket**  
   ```c
   int one = 1;
   if (setsockopt(socket_fd, SOL_SOCKET, SO_ZEROCOPY, &one, sizeof(one)) < 0) {
       perror("setsockopt(SO_ZEROCOPY)");
       // handle error
   }
   ```

2. **Use `MSG_ZEROCOPY` in `send()`**  
   ```c
   ssize_t ret = send(socket_fd, buffer, length, MSG_ZEROCOPY);
   if (ret < 0) {
       perror("send(MSG_ZEROCOPY)");
       // handle error
   }
   ```

**Note**: The reason for enabling zero-copy both via `setsockopt()` **and** using `MSG_ZEROCOPY` is partly for backward compatibility. The kernel will ignore unknown flags, so historically a spurious `MSG_ZEROCOPY` might have been set (and ignored). Now it’s a real flag, but to ensure correct usage, you must **explicitly** enable it at both levels.

### 1.3 Caveat: Asynchronous Buffer Release

Because the kernel is effectively using the user’s pages **directly** (with pinned memory pages), user space **cannot** immediately reuse or free that buffer after `send()`. You must wait until the kernel notifies you that it’s done transmitting. This notification arrives via the **socket error queue** (when using `MSG_ERRQUEUE` in `recvmsg()`).

Typical flow:

1. **Send** data with `MSG_ZEROCOPY`.
2. **Poll** the socket for `POLLERR` events.
3. **Receive** the error queue message with `recvmsg(fd, &msg, MSG_ERRQUEUE)`.
4. **Check** the extended error info for `SO_EE_ORIGIN_ZEROCOPY`.

```c
struct pollfd pfd;
pfd.fd = socket_fd;
pfd.events = POLLERR;

if (poll(&pfd, 1, timeout) == 1 && (pfd.revents & POLLERR)) {
    struct msghdr msg = {0};
    // Prepare iov, cmsg, etc. to read the error queue
    ssize_t ret = recvmsg(socket_fd, &msg, MSG_ERRQUEUE);
    if (ret >= 0) {
        // parse cmsg data
        read_notification(&msg);
    }
}

uint32_t read_notification(struct msghdr *msg)
{
    struct cmsghdr *cm = CMSG_FIRSTHDR(msg);
    if (!cm || cm->cmsg_level != SOL_IP || cm->cmsg_type != IP_RECVERR) {
        // error ...
    }
    struct sock_extended_err *serr = (struct sock_extended_err *) CMSG_DATA(cm);
    if (serr->ee_origin != SO_EE_ORIGIN_ZEROCOPY) {
        // error ...
    }
    // `serr->ee_data` might contain a reference to which transmission is completed
    return serr->ee_data;
}
```

### 1.4 Drawbacks of `MSG_ZEROCOPY`

1. **Works best with large data (>~10 KB)**:
   - The overhead of page pinning, notifications, etc. can negate benefits for small writes.
2. **Asynchronous**:
   - Requires an extra poll/recvmsg to manage buffer life cycle, causing **additional** system calls and context switches.
3. **Only send side** is supported currently, not receive side.
4. **Application Complexity**:
   - You must track which buffers are “still pinned” and which can be safely reused.

Despite these downsides, for large or frequent sends, `MSG_ZEROCOPY` can yield **significant** CPU savings.

---

## 2. Direct I/O Bypassing the Kernel

All previous zero-copy or “partial copy” techniques revolve around **kernel involvement**. Even if the kernel doesn’t do the data copy, it orchestrates the data flow (page caches, pinning, TLB updates, etc.).

An alternative approach is: **what if we bypass the kernel entirely?** This typically comes in two flavors:

1. **Direct User Access to Hardware**  
2. **Kernel-Controlled Direct Access** (the kernel is minimally involved, but doesn’t copy data)

### 2.1 Direct User Access to Hardware

- The user-space process communicates **directly** with device hardware.  
- Kernel’s role is only to do minimal setup (e.g., memory registration, device mapping).  
- This often requires:
  - **Special hardware** (e.g., custom NICs with simpler protocols, InfiniBand, or specialized HPC networks).  
  - Pages in user space must be **pinned** so that DMA can read/write them without page faults.  

**Pros**:
- Potentially **fastest** possible I/O.  
- No kernel copying overhead at all.

**Cons**:
- Breaks **OS abstraction** (the user process has near “raw” access to hardware).  
- Very **hardware-specific**, limited to specialized HPC or cluster environments.  
- Memory must be pinned (which can be expensive or inflexible).  
- Security and resource contention concerns (since the kernel is not mediating every access).

**Typical Use Cases**:
- **MPI** in HPC (High-Performance Computing) clusters.  
- Some **RDMA** solutions in data centers or specialized local networks.

### 2.2 Kernel-Controlled Direct Access

- The kernel remains **in control** of the device but does not do data copies.  
- The kernel configures the **DMA engine** to do device ↔ user-buffer transfers.  
- The user code can still call `read()`/`write()` (or similar) normally.

**Pros**:
- Still avoids major CPU copies.  
- The kernel can handle any fallback if the hardware is unavailable or if an error occurs.  
- More secure than direct user-hardware access (the kernel can enforce resource limits).

**Cons**:
- Still need pinned pages or memory constraints.  
- Still hardware-dependent for efficient DMA operations.  
- Cache-coherency overhead (e.g., flushing CPU caches around DMA transfers).

---

## 3. `O_DIRECT` in Linux (Direct I/O to Block Devices)

Linux provides the **`O_DIRECT`** flag (passed to `open()`), which allows user processes to bypass the **page cache** for block devices (disk files).

```c
int fd = open("datafile", O_RDWR | O_DIRECT | O_SYNC, 0644);
```

1. **Bypassing Page Cache**:  
   - Reads or writes go directly to disk buffers, skipping the kernel’s caching.  
   - Avoids extra copies but also forfeits automatic read-ahead and write-back optimizations.

2. **Common Use Case**: **Databases**.  
   - Databases often have their own sophisticated caching layers. They don’t want the kernel’s generic page cache interfering or duplicating memory usage.

3. **Key Caveats**:
   - **Block Alignment**: The I/O must be aligned to the device’s block size. If not, errors (`EINVAL`) or fallback to buffered I/O might happen, depending on FS and kernel version.  
   - **Not all filesystems** support `O_DIRECT`. Some may return errors.  
   - **Mixing** `mmap()` or normal buffered I/O with `O_DIRECT` can cause **data corruption** or heavy performance penalties.  
   - Often used with `O_SYNC` to ensure data is truly persisted.

### 3.1 Linus’ Critique of `O_DIRECT`

Linus Torvalds famously **dislikes** `O_DIRECT`, calling its interface “designed by a deranged monkey.” The gist is that it’s tricky, error-prone, and replicates caching logic that should be done in the kernel. Despite that, **databases** find `O_DIRECT` indispensable because they can manage caching themselves.

### 3.2 Alignment and Performance Issues

- When using `O_DIRECT`, the **user** is responsible for **block alignment** to avoid the dreaded **RMW (Read-Modify-Write)** penalty.
- An example of **misaligned** blocks can cause 2 I/O ops instead of 1, leading to up to **25x** slowdown.
- With the kernel’s page cache, this alignment detail is hidden. Without it, you must handle it manually.

### 3.3 Who Should Use `O_DIRECT`?

- Typically **not** regular applications.  
- **Databases**, or other software with its **own** specialized caching, benefit from it.  
- If you’re building high-performance or unusual storage solutions and want full control over caching, `O_DIRECT` might be your friend.

---

## 4. Summary & Best Practices

1. **`MSG_ZEROCOPY`**:
   - Great for **large** socket sends.  
   - Requires asynchronous poll for “done” signals.  
   - Gains can be substantial but only in specific workloads (≥ ~10 KB chunks).

2. **Direct I/O Bypassing the Kernel**:
   - Fully or mostly bypass kernel data copying.  
   - Involves **pinned memory**, specialized hardware, or a custom setup in the kernel.  
   - Potentially highest throughput, but limited or complex.

3. **`O_DIRECT`**:
   - Avoids page cache for file I/O on block devices.  
   - Requires careful alignment, can cause more complexity.  
   - Commonly used by **databases**, rarely by normal apps.

**Conclusion**: Each zero-copy technique addresses different needs:
- **`MSG_ZEROCOPY`** = Lower CPU overhead for large **network** sends.
- **`O_DIRECT`** = Control over file I/O caching on block devices.
- **Direct hardware access** (user or kernel controlled) = Very specialized HPC or device-specific use.

They all share a common limitation: **they add complexity** to code and often impose constraints (e.g., alignment, pinned pages, asynchronous completions). There is **no single silver bullet**. You must weigh whether the performance gains justify the added development and maintenance overhead.

```  
────────────────────────────────────────────────────────────────────
 "Zero-copy is a broad umbrella of techniques, each with its own
 trade-offs. The further you stray from standard buffered I/O,
 the more complexity arises—especially with alignment, pinned
 memory, or asynchronous notifications. Yet for high-performance
 or specialized scenarios, these advanced methods can pay off."
────────────────────────────────────────────────────────────────────
```
```