aliases: [Raft, Multi-Paxos, Distributed Consensus, Paxos Variant]
tags: [Consensus Algorithms, Raft, Distributed Systems]
## 1. Introduction
**Raft** is essentially a variant of **Multi-Paxos** with two additional constraints:
1. **Append Log Constraint**  
   In Raft, follower logs must always be a **contiguous prefix** of the leader's log. By contrast, **Multi-Paxos** can have logs appended in a more concurrent manner, with out-of-order appended entries that eventually reorder.
2. **Leader Election Restriction**  
   In Raft, only a node with the **latest and most complete log** can become the leader, ensuring that once elected, its log is effectively the cluster’s “authoritative” log. Multi-Paxos, on the other hand, does not enforce a “latest log” rule for leadership; any node can become leader, and the logs are reconciled *after* leader election.
These constraints **simplify** Raft’s implementation and logic compared to Multi-Paxos, although the concurrency potential of Multi-Paxos is theoretically higher. In real systems, the simpler approach of Raft is often favored because it’s easier to reason about and implement correctly.

---
## 2. Differences: Raft vs. Multi-Paxos
### 2.1 Log Appending
- **Raft**:  
  - Follower logs must be a *subset* of the leader’s log.  
  - The leader enforces continuity and never allows the follower to skip entries or reorder them.  
- **Multi-Paxos**:  
  - Allows more concurrency in appending.  
  - Follower logs might temporarily have out-of-order entries or missing chunks that get resolved eventually.  
  - Final consistency is still guaranteed once chosen proposals are applied, but the path can be more complicated.
### 2.2 Leader Election
- **Raft**:  
  - Only a node with the most up-to-date log can be elected leader.
  - This ensures the newly elected leader doesn’t miss log entries that others might have.  
- **Multi-Paxos**:
  - Any node can become leader. Once chosen, the new leader recovers the chosen but not yet learned entries.  
Hence, **Raft** enforces simpler invariants, which is part of why it is advertised as more understandable than Paxos (though it does the same *type* of work—consensus via a single leader).
Below is a rough depiction of logs under Multi-Paxos vs. logs under Raft:
```plaintext
Multi-Paxos Logs (possible states):
Node A: [1, 2,    4,  ?]
Node B: [1, 2, 3, 4,  ?]
Node C: [1,    3,    5]

Raft Logs (enforced subset relation):
Leader: [1, 2, 3, 4]
Follwr1: [1, 2, 3]
Follwr2: [1, 2, 3, 4]
```
---
## 3. Why Raft is Favored
**Raft** is simpler to present and implement. For each log index, a single leader’s log is the ground truth, and followers replicate the same entries in order.  

**Multi-Paxos** can yield better concurrency in theory, but implementing it in a large codebase can be tricky. Moreover, in practice, Raft’s “one leader at a time” approach is sufficiently performant and significantly more straightforward to reason about.  
**In engineering**:  
- Paxos has no universal standard of “Multi-Paxos” implementation—each project implements its own variant.  
- Raft is a single, well-documented algorithm with official reference implementations and a simpler conceptual approach.  
Hence many new distributed systems choose **Raft**.
---
## 4. Algorithm Type & BFT vs. CFT
### 4.1 Crash Fault Tolerance (CFT)
- Paxos, Raft, Zab, etc., assume non-Byzantine faults.  
- Tolerate crashed nodes or network partitions, but not maliciously crafted messages. 
### 4.2 Byzantine Fault Tolerance (BFT)
- PBFT, PoW-based approaches.  
- Tolerate nodes that act arbitrarily or maliciously.  
- Typically higher overhead, up to 1/3 node failures tolerance.  
**Raft** is a CFT consensus protocol.
---
## 5. Relationship to Distributed Transactions & Data Consistency
**Consensus** solves the problem of “which value has been chosen” among nodes. Meanwhile, **consistency** in distributed transactions might require:
- **Atomic commit** (2PC, 3PC, or Paxos-based approach).
- **Concurrency control** (locking or timestamp-based).  
**Raft** can underlie:
- Master election,
- Replicated log,
- Transaction manager election, or
- Global ordering of commits in a distributed DB.
But by itself, it’s “just” a consensus mechanism. Additional logic ensures transaction-level concurrency control, etc.
---
## 6. Summary
- **Raft** extends the ideas of Multi-Paxos but places constraints on log structure and leader election.  
- This simplifies the algorithm design while retaining fault tolerance.  
- In practice, systems like etcd and Consul use Raft for reliable log replication.  
**Key points**:
1. Raft is effectively a simpler, more specialized variant of Multi-Paxos.  
2. Strict log subset constraints → easier to reason about correctness.  
3. Leader must have the newest log → ensures minimal conflict at election time.  
4. Concurrency is more restricted than Multi-Paxos, but it’s rarely a bottleneck in real scenarios.
**Further Reading**:
- “In Search of an Understandable Consensus Algorithm” (the Raft Paper) by Diego Ongaro and John Ousterhout.  
- [Raft official site](https://raft.github.io/)  
- Paxos vs. Raft user study (YouTube talk by Ongaro).  

```