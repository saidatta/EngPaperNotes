aliases: [Block Layer, bio, I/O Scheduler, CFQ, Deadline, Single-Queue I/O]
tags: [Linux, Kernel, Block Devices, I/O Scheduling, Bio Data Structure]
## 1. Introduction
The **Generic Block Layer (GBL)** is the kernel subsystem that abstracts all **block device** I/O operations. It provides:
- A **unified interface** for block device drivers
- Mechanisms for advanced features like zero-copy I/O and logical volume management
- The central place to handle I/O scheduling policies, queue management, etc.

Key structures and components:
- **`bio`**: Describes an active I/O request (one or more segments).
- **I/O Scheduling**: Merging & reordering requests for optimal disk usage.
- **Block device drivers**: The low-level interface to actual hardware.

---
## 2. The `bio` Data Structure

### 2.1 Purpose

**`bio`** stands for *block I/O*. It encapsulates an I/O operation at the block layer. It's used by the kernel to unify read/write requests from various upper layers (like file systems) to the underlying block device. 

**Key**: A single `bio` can represent a multi-segment (vector) I/O operation, so the kernel can handle scattered buffers in one request.

### 2.2 Definition

In `<include/linux/bio.h>` (or `<linux/blk_types.h>` in newer kernels), we see something like:

```c
struct bio {
	struct bio       *bi_next;       // Link to next bio in a list
	struct block_device *bi_bdev;    // Block device this bio targets
	unsigned short   bi_vcnt;        // how many bio_vec segments
	unsigned short   bi_max_vecs;    // max segments possible
	atomic_t         __bi_cnt;       // reference count
	struct bio_vec   *bi_io_vec;     // array of i/o segments
	struct bio_set   *bi_pool;       // memory pool
	...
	struct bio_vec   bi_inline_vecs[]; // possibly inline segments
};
```

#### 2.2.1 `bio_vec`
Segments in a `bio` are stored in an array of **`bio_vec`**:

```c
struct bio_vec {
	struct page *bv_page;    // which page in memory
	unsigned int bv_len;     // length of segment
	unsigned int bv_offset;  // offset in the page
};
```

So a `bio`:
1. Has an array of these `bio_vec` to describe each contiguous memory chunk.
2. The kernel can combine multiple small adjacent chunks or reorder them if needed.
3. `__bi_cnt` is a reference count controlling the lifetime of the `bio`.

### 2.3 Lifecycle

- **Allocation**: Typically `bio_alloc()` from a slab or special bio pool.  
- **Filling**: The upper layer sets the device `bi_bdev`, the segments in `bi_io_vec`.  
- **Reference**: Use `bio_get(bio)` and `bio_put(bio)` for increment/decrement refcount. 
- **Scheduling**: `bio` is handed off to the I/O scheduler, eventually to the device driver.
- **Completion**: Freed once I/O finishes and references drop to 0.

---

## 3. I/O Scheduling

### 3.1 Single-Queue vs. Multi-Queue Architecture

Historically, older HDD-based systems had a **single** I/O queue per device (the “SQ” approach). Newer NVMe/SSD systems have multiple queues (the “MQ” approach). 

**In the SQ design**:
- There's one request queue for each block device.  
- The kernel’s I/O scheduler merges, sorts, or reorders requests in that queue.  
- Then dispatches them to the device driver.

We focus first on **single-queue** scheduling approaches, typically used for HDD.

### 3.2 Single-Queue I/O Scheduling

**Goal**: Optimize throughput for mechanical disks. The disk addressing overhead is large, so the OS merges adjacent requests and sorts them by sector address.

Implementation: The block layer holds a **request_queue** (a `struct request_queue` object). Each I/O request from `bio` can be merged with others or appended in a sorted manner.

#### 3.2.1 The `request_queue` Structure

In `<include/linux/blkdev.h>`:

```c
struct request_queue {
	void                *queuedata;      // pointer to device-specific data
	struct elevator_queue *elevator;     // scheduling policy
	unsigned long       queue_flags;
	struct list_head    // ...
	/* more fields for mq or sq scheduling */
};
```

The **I/O scheduler** (or elevator) is configured here.

### 3.3 SQ I/O Scheduling Algorithms

Linux provides multiple scheduling algorithms for single-queue devices:

1. **Linus Elevator (Anticipatory)**  
   - Original approach. Merges requests if they are for adjacent disk sectors. Also sorts them in ascending order of sector addresses. Could cause starvation in random scenarios.

2. **Deadline**  
   - Each request has an **expiration** time (e.g. 500 ms for reads, 5 s for writes). Two internal queues:
     - A sorted queue by sector
     - A FIFO queue by arrival time
   - If a request is about to expire in the FIFO, it forcibly runs it. Greatly reduces starvation. Emphasizes read requests (lower expiry) to keep latencies down.

3. **CFQ (Completely Fair Queuing)**  
   - Assigns each process two queues (sorted + FIFO). Round-robin time slices to each queue. Aimed at fairness so no single process hogs all disk bandwidth. Good for multi-tasking desktops or systems with many concurrent processes.

4. **noop**  
   - Minimal merging, no sorting. Good for random-access devices or advanced controllers that do their own reordering. Also suitable for SSD if single-queue approach is forced.

**Note**: For HDD-based systems, Deadline or CFQ used to be defaults. For SSDs or modern systems, typically **mq-deadline** or **none** (multi-queue) is used instead.

---

## 4. pdflush vs. flusher: Writeback Congestion

### 4.1 pdflush

Introduced in early 2.6, **pdflush** used multiple threads that globally handled dirty page writeback. Each thread tried to do balanced I/O across all disks. 
- **Issue**: If multiple pdflush threads all blocked on one device, other devices starved.
- A special “congestion control” algorithm tried to see if a device queue was busy, and skip it if so. Implementation was complex and not always effective.

### 4.2 flusher Threads

From kernel 2.6.32 onward, replaced pdflush with:
- **Per-disk (or per-bdi) flusher** threads.
- Each device has its own flusher, so no single device can starve all. No complicated global logic needed.

**Hence**: The flusher approach is simpler, effectively localizing writeback tasks to each device. This yields better throughput and less risk of a single queue dominating all threads.

---

## 5. Dirty Page Thresholds

- **`dirty_background_ratio`**: If the ratio of dirty pages in the system surpasses this, the kernel **wakes** flusher threads to start asynchronous writeback.
- **`dirty_ratio`**: If the dirty ratio reaches this higher threshold, the kernel starts **blocking** new writes or throttling them more aggressively. This ensures the system doesn’t go out of memory for dirty pages. 

**Tip**: Usually keep `dirty_background_ratio` about 1/4~1/2 of `dirty_ratio`. Setting these incorrectly can cause big I/O stalls or insufficient write caching.

---

## 6. Manual Writeback

Even with automatic flushing, user space can forcibly flush:

- **`sync()` / `syncfs()`**:  
  - Sync entire system or a single filesystem. Historically could return immediately, but on modern Linux it usually blocks until data hits disk.

- **`fsync(fd)` / `fdatasync(fd)`**:  
  - Sync a single open file.  
  - `fsync` includes metadata + data, `fdatasync` mostly just data (except crucial metadata like file size).

- **`sync_file_range()`**:  
  - Fine-grained flush of partial file ranges, plus asynchronous modes.  
  - Non-portable, for specialized apps.

- **`O_SYNC`, `O_DSYNC`, `O_RSYNC`**:  
  - Synchronous write modes set at `open()`.  
  - `O_SYNC` → every write blocks until data+metadata on disk; `O_DSYNC` → only data on disk (faster).

---

## 7. Summation of the Block Layer

**GBL** and **bio** unify how upper layers (VFS, page cache, filesystem) talk to block devices. The block layer organizes requests into `bio`s, merges them via I/O scheduling, and dispatches them to device drivers. The block layer also supports advanced features like:

- **LVM** & RAID (multiple partitions or devices combined).
- Possibly advanced DMA interactions (like zero-copy).
- Single-queue or multi-queue scheduling.

**Multi-queue** scheduling is widely used for modern SSD/NVMe to handle concurrency. The single-queue approach (with CFQ, Deadline, etc.) is more relevant for older or spinning HDD hardware.

---

## 8. Code & Visuals

### 8.1 `bio` Example

```c
#include <linux/bio.h>
#include <linux/blkdev.h>

void my_submit_bio(struct block_device *bdev, void *buffer, int nr_pages) {
    struct bio *my_bio = bio_alloc(GFP_KERNEL, nr_pages);
    my_bio->bi_bdev = bdev;
    my_bio->bi_iter.bi_sector = 1000; // hypothetical sector
    // fill in my_bio->bi_io_vec with pages
    // ...
    bio_get(my_bio);
    submit_bio(my_bio); 
    // The I/O scheduler merges it, eventually freeing
    bio_put(my_bio);
}
```

### 8.2 Single-Queue I/O Scheduling Layers

```mermaid
flowchart LR
    Subgraph BFS[Block FS]
    BFS-->()IO_Sched
    end
    IO_Sched[ I/O Scheduler <br>(SQ: CFQ/Deadline/noop/etc.) ]
    IO_Sched-->( )DeviceDriver
    DeviceDriver[Block Device Driver]-->HW[HDD]
```

---

## References & Further Reading

- Kernel doc: [`Documentation/block/biodoc.txt`](https://www.kernel.org/doc/Documentation/block/)  
- Source code in `block/blk-core.c`, `block/blk-mq.c`, `include/linux/blkdev.h`  
- For I/O scheduling specifics: `block/cfq-iosched.c`, `block/deadline-iosched.c`, `block/noop-iosched.c`  
- [LWN Articles on pdflush to flusher transition](https://lwn.net/)  
- Tuning parameters: `dirty_ratio`, `dirty_background_ratio`, `vm.dirty_*` sysctls.  

```