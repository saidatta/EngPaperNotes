aliases: [Writeback, pdflush, flusher, dirty_background_ratio, direct I/O, mapping layer]
tags: [Linux, I/O, File Systems, Kernel, Caching, Buffering]
## 1. Introduction

In Linux, **writeback** refers to the mechanism of flushing **dirty pages** from memory (Page Cache) to the disk in the background, ensuring that eventually, data modifications in memory are persisted. The design of this mechanism has evolved:

- **bdflush** (single-threaded, older kernels)
- **pdflush** (multi-threaded, kernel 2.6)
- **flusher threads** (local-per-disk approach, from ~2.6.32 onward)

This note covers how Linux overcame issues with single-threaded writeback, introduced multi-threading, then refined it further to avoid complicated congestion control logic and starvation.

---

## 2. Single-Threaded `bdflush`

Historically, the Linux kernel had **`bdflush`**, a single kernel thread that handled writing dirty pages to disk. However:

1. If it got stuck on a busy disk’s I/O queue, no other disk's writeback would proceed → potential *starvation*.
2. In the era of mechanical HDDs, a single I/O scheduling queue tried to merge adjacent requests for sequential optimization (reduced seek overhead). But single-threading could block the entire system if `bdflush` was busy on one device.

Hence, multi-threading was introduced to handle multiple disks more gracefully.

---

## 3. `pdflush` in Kernel 2.6

### 3.1 Multi-threaded Approach

The **pdflush** mechanism spun up multiple threads. They each attempted to do writeback on the “global” set of dirty pages, scanning for whichever disk was available. This resolved the single-thread bottleneck, but led to another extreme scenario:

- All pdflush threads might get blocked on the I/O queue of a single heavily loaded disk, causing other disks to starve.  
- Each thread had to detect whether an I/O queue was “congested,” then pick a less busy disk → complicated congestion logic.

### 3.2 Congestion Control Strategy

**pdflush** tried to solve this by requiring each thread to actively sense I/O queue busyness. If busy, it’d try another disk. But:

1. **Implementation complexity**: Balancing threads among multiple disks, plus detecting per-queue busyness, increased overhead.
2. **I/O stack mismatch**: The rest of the I/O system (e.g., block layer, device drivers) did not do such elaborate congestion control. The pdflush approach sometimes was suboptimal or overshadowed by other layers’ constraints.

Thus, the cost/benefit ratio of complicated multi-thread congestion control was not always good.

---

## 4. Flusher Threads in Kernel 2.6.32+

### 4.1 Local Per-Disk Threads

The kernel introduced **flusher threads**. Each disk (or block device) has its **own** flusher. That is:

- Instead of a global pool of threads, each device uses one dedicated thread to write out dirty pages *for that device only*.
- This local model is simpler and inherently load-balances: no single queue or disk starves others because each device has its own flusher.

### 4.2 Benefit

- Minimal or no complicated congestion detection logic needed.
- Each flusher sees only one device → straightforward. 
- The approach avoids global blocking if one device is heavily loaded. 
- With local flushers, the system achieves high I/O throughput while reducing risk of resource starvation.

**Hence**: The current Linux architecture (post-2.6.32) typically uses **per-bdi (block device) flusher** threads.

---

## 5. Dirty Page Thresholds

The kernel uses two primary parameters:

1. **`dirty_background_ratio`**  
   - The percentage of memory filled with dirty pages at which the kernel **starts** writeback in the background (flusher threads). 
   - The writeback is asynchronous, so user processes can continue writing.

2. **`dirty_ratio`**  
   - A higher threshold at which the kernel **blocks** new writes altogether if the flusher can’t keep up. 
   - In older kernels, user processes are fully blocked.  
   - In newer kernels, it’s more gradual (a “throttling” approach). Once `dirty_ratio` is hit, the kernel heavily slows or stops user writes to let the flusher reduce the dirty page ratio.

**Relationship**: 
- Typically, `dirty_background_ratio` < `dirty_ratio`. 
- If the dirty ratio climbs above `dirty_background_ratio` but is still below `dirty_ratio`, flushers try to catch up. 
- If it hits `dirty_ratio`, the system halts new writes, preventing a runaway scenario.

**Tuning**: 
- Setting `dirty_ratio` too low can cause frequent forced flushes → big performance hits for write-heavy apps. 
- Recommended best practice is often `dirty_background_ratio` ~ 1/4 to 1/2 of `dirty_ratio`.

---

## 6. Manual Writeback

Even though the kernel does automatic writeback, user applications can manually flush data to disk for reliability:

### 6.1 `sync()` & `syncfs()`

```c
#include <unistd.h>

void sync(void);
int syncfs(int fd);
```

- **`sync()`**: Globally flushes *all* dirty pages from *all* file systems to disk.  
- **`syncfs(int fd)`**: Flushing only the file system containing the file descriptor `fd`.  
- By POSIX, `sync()` can return before the flush is complete, but Linux modifies it so it typically blocks until data is on disk.

### 6.2 `fsync()` & `fdatasync()`

```c
#include <unistd.h>

int fsync(int fd);
int fdatasync(int fd);
```

- **`fsync()`**: 
  - Ensures both file data **and** metadata (like size, timestamps, etc.) are persisted before returning. 
  - The call is fully blocking.

- **`fdatasync()`**: 
  - Only forces file data (and metadata changes *affecting subsequent reads*, e.g. file size) to disk.  
  - More efficient, ignoring updates to e.g. `st_atime`.

### 6.3 `sync_file_range()`

```c
#define _GNU_SOURCE
#include <fcntl.h>
int sync_file_range(int fd, off_t offset, off_t nbytes, unsigned int flags);
```

- Introduced in kernel 2.6.17.  
- Allows fine-grained partial-file sync, and asynchronous or blocking modes.  
- *Caveat*: Doesn’t guarantee file metadata is synced. Risky if a crash occurs. 
- Non-portable, Linux-only, used rarely in specialized scenarios.

### 6.4 `O_SYNC`, `O_DSYNC`, `O_RSYNC`

Instead of calling flush syscalls manually after every write, you can open the file with these flags (via `open(2)` or `fcntl(2)`) to request synchronous I/O:

- **`O_SYNC`**: All data & metadata are forced to disk at each `write(2)`.
- **`O_DSYNC`**: Similar, but only ensures file data (and essential metadata). 
- **`O_RSYNC`**: Affects reads, ensuring data is up-to-date on disk *before read*. On Linux, `O_RSYNC` is generally treated like `O_SYNC`.

They are part of the POSIX standard, but the actual behavior may vary depending on the filesystem’s implementation.

---

## 7. Read Cache & Pre-reading

On reads, Linux uses **Page Cache** to speed up future accesses. If a user reads a file offset, the kernel checks if the page is in the cache:
- **Hit** → return data from memory.
- **Miss** → load from disk into cache, then return data.

#### Pre-reading (Readahead)

Because of **spatial locality**, if we read page `N`, we might soon read page `N+1`. So the kernel *preemptively* loads subsequent pages into the cache. This helps large sequential reads be mostly cached. The function behind it is typically `readahead()` in the address_space_ops.

---

## 8. Address Space Object

**`struct address_space`** is the key structure in Page Cache: it groups pages for a given inode’s data, linking them from disk blocks to memory pages. Found in `<linux/fs.h>`:

```c
struct address_space {
	struct inode      *host;
	struct xarray     i_pages;      // pages in the cache
	struct rw_semaphore invalidate_lock;
	gfp_t             gfp_mask;
	/* more fields */
	const struct address_space_operations *a_ops; // ops for read/write
	...
};
```

**`a_ops`** (address_space_operations) includes methods like:

- `read_folio()`, `writepage()`, `readahead()`, `writepages()`, etc.  
- Each filesystem implements these to define how pages are read/written from/to disk.

---

## 9. Buffered vs. Direct I/O

- **Buffered I/O**: The default, uses Page Cache for reads/writes. Writes are asynchronous (dirty pages) and reads are synchronous but can benefit from caching.  
- **Direct I/O**: Bypasses Page Cache, reading/writing directly from disk buffers. This is typically used for database workloads that manage their own caching. It’s *not* the default in normal FS usage.

---

## 10. Mapping Layer

Below VFS and Page Cache is the **mapping layer**, including actual FS drivers (ext4, xfs, btrfs, etc.) and block device drivers. The mapping layer handles:

1. Converting logical block offsets into physical addresses on disk.  
2. Possibly abstracting block device files.  

Thus, when a file is read/written, the kernel obtains its logical blocks from the inode, and the FS driver transforms them to physical sectors on disk.

---

## 11. Summary

- Historically, **bdflush** was a single-thread flusher, replaced by **pdflush** multi-threading in 2.6, then refined to **per-device flusher threads** from 2.6.32 onward.  
- The kernel uses `dirty_background_ratio` and `dirty_ratio` to manage how many dirty pages are tolerated before auto-writeback or blocking.  
- Various syscalls (`sync`, `fsync`, `fdatasync`, `sync_file_range`) let user processes manually flush writes.  
- For reading, the Page Cache uses **readahead** strategies.  
- The **address_space** object organizes pages for each inode, hooking into `a_ops` for reading/writing.  
- Finally, VFS and these operation tables (`super_operations`, `inode_operations`, `dentry_operations`, `address_space_operations`) implement a flexible, object-based approach to handle different file systems and I/O strategies (buffered, direct, etc.).

```