aliases: [MQ Scheduler, blk-mq, NVMe, I/O Scheduling]
tags: [Linux, Kernel, Block Layer, MQ, Scheduling]
## 1. Overview
As **SSD** adoption rose, the IOPS performance soared, making the old single-queue (SQ) I/O scheduling design in the Linux kernel a bottleneck. The **multi-queue (MQ)** I/O scheduler was introduced to reduce lock contention, leverage NUMA multicore architectures, and exploit the advanced hardware (like **NVMe**) that supports many hardware queues.
In the **MQ** approach:
1. **Software level**: Multiple software I/O queues (one per CPU or socket) in the kernel
2. **Hardware level**: Possibly multiple hardware dispatch queues (HDQ) in the device (NVMe can have up to 64k queues)
3. The kernel merges requests if beneficial (especially for small write merges) but typically defers deeper scheduling to the device hardware.

## 2. Why Multi-Queue?

### 2.1 Performance Bottlenecks in SQ

In the older single-queue approach:

1. **Queue lock**: A single request queue is heavily contended by all cores.  
2. **Hardware interrupts**: Typically only one CPU handles interrupts, needing inter-processor interrupts (IPI) to notify other cores. This causes excessive context switching.  
3. **NUMA cross-memory access**: With many cores, lock states bounce among CPU caches, further hurting performance.

The **MQ** design addresses these by distributing or localizing queues to avoid cross-core locking and bus traffic.

### 2.2 Key Goals

- **I/O fairness**: Even with multiple queues, ensure that each process or cgroup can share device bandwidth fairly (like CFQ in the past).  
- **Enhanced stats/accounting**: Provide better device usage metrics, easier debugging.  
- **Staging area**: A queue that can absorb large bursts of I/O and do merges. 

---

## 3. Multi-Queue (MQ) Architecture

### 3.1 Two-Level Queue Model

**1) Software Staging Queues (SSQ)**  
- The kernel’s “request queue(s)” are no longer single but multiple. Possibly one per NUMA node or per CPU.  
- Minimizes lock contention because each queue is only used by one CPU or socket.

**2) Hardware Dispatch Queues (HDQ)**  
- Provided by the block device driver.  
- E.g., an NVMe drive might have 1–64k hardware queues.  
- Each SSQ eventually dispatches to a specific HDQ or splits requests among them.

### 3.2 HDQ Count & NVMe
- SATA-based SSDs might only support 1 hardware queue (like older HDD).  
- **NVMe** can provide up to 64k queues, each up to 64k in length. This concurrency is how NVMe achieves extremely high IOPS.  
- Kernel MQ framework (`blk-mq`) leverages these multiple device queues for parallel I/O, drastically reducing CPU overhead for locking/sorting.

### 3.3 Freed from Sorting for SSD

Because mechanical seeking is not a big factor in SSD performance, advanced reordering at the software layer is less essential. Merging small requests is still valuable, but sorting them by sector is usually less beneficial. SSD firmware can handle concurrency well, so the kernel can adopt simpler scheduling logic (like FIFO or minimal reordering).

---

## 4. `blk-mq` Implementation

**`blk-mq`** is the kernel’s multi-queue framework in the block layer. It organizes I/O flows as follows:
1. Each CPU (or socket) has a local software queue (SSQ).
2. `bio` structures arrive from upper layers, get inserted into the local queue. 
3. `blk-mq` manages merging, assigning a hardware dispatch queue (HDQ).
4. Device driver (NVMe, etc.) picks up requests from the HDQ.  

### 4.1 Labeling (Tagging) I/O

- Each request is given a **tag** so the completion path can quickly identify it. 
- Instead of scanning the entire queue to find which I/O finished, the kernel uses that tag to quickly look up the request.

### 4.2 Reduced Lock Contention

With per-CPU queues, each queue can be protected by a local lock or use atomic operations, minimizing global spinlock usage.

---

## 5. MQ I/O Scheduling Algorithms

### 5.1 `mq-deadline`

**mq-deadline** is the multi-queue version of the old “deadline” approach:

- Merges requests for adjacency if possible.
- Maintains a FIFO to ensure requests do not starve (with time deadlines).
- Very straightforward, good for moderate concurrency.

### 5.2 BFQ (Budget Fair Queueing)

- Introduced ~4.12 kernel, focuses on **fairness** and low latency.
- Complex code, higher CPU overhead. 
- Good for use cases where each process or cgroup’s I/O share must be regulated precisely, e.g. desktop/multimedia workloads.

### 5.3 Kyber

- Introduced ~4.12 too, aimed at **high-performance** multi-queue devices (e.g. SSD).
- Has two queues internally (read vs. write). 
- Focuses on limit-based scheduling: short queue lengths to reduce latency while still grouping I/Os.
- A simpler design than BFQ, decent for I/O-intensive server apps like databases.

### 5.4 None

- Minimalist scheduler (similar to `noop` in single-queue). 
- Merges requests but no reordering. 
- Perfect for NVMe or advanced SSD with strong hardware logic. 
- Leaves concurrency management to the device and yields maximum throughput with minimal CPU overhead.

---

## 6. Example: NVMe Multi-Queue

**NVMe** typically creates a hardware submission queue + completion queue per CPU core. `blk-mq` sets up a matching set of SSQs so each CPU issues commands to “its” queue. This synergy drastically reduces cross-CPU interference. The device can handle 64K queues, each up to 64K commands, matching HPC or large server usage.

**Flow**:

1. Application issues write → user space → kernel (VFS) → block layer.  
2. `blk-mq` picks the local SSQ on CPU X.  
3. Request is tagged and merged if needed, put in HDQ.  
4. NVMe driver sees request in the relevant hardware queue, completes it.  
5. Minimal context switching or cross-core locking.

---

## 7. Conclusion

Multi-queue block I/O architecture addresses the fundamental limitations of the old single-queue approach:

1. **Eliminates** single lock contention.  
2. **Leverages** multiple hardware queues for SSD/NVMe devices.  
3. Freed from elaborate sorting for mechanical seeks, focuses on merges or simple bounding of queue length.  

**Algorithm choices** under MQ:
- **mq-deadline**: Balanced approach for normal servers.  
- **bfq**: Complex fairness, good for interactive or desktop workloads.  
- **kyber**: High concurrency, moderate CPU overhead, suitable for data centers.  
- **none**: Minimal overhead, best for advanced SSD with hardware scheduling.

As NVMe and HPC become mainstream, the **multi-queue** design is effectively the new default for modern Linux I/O.

---

## References & Further Reading

- Jens Axboe’s discussions on `blk-mq`: [LWN articles](https://lwn.net/) and kernel mailing lists  
- Kernel docs: `Documentation/block/queue-sysfs.rst`  
- For NVMe details: `Documentation/admin-guide/nvme.rst`  
- BFQ/kyber details: in `block/bfq*` and `block/kyber*`  
- Deep explanation of multi-queue: [kernel source code `block/blk-mq.c`](https://git.kernel.org/).  