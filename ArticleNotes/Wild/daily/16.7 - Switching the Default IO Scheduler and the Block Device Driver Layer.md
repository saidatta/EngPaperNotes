aliases: [I/O Scheduler Switching, MQ vs SQ, NVMe, Ubuntu, BFS, BFS Implementation, Block Device Driver]
tags: [Linux, I/O Schedulers, Multi-Queue, NVMe, Kernel Configuration]

## 1. Overview
**Before Linux 5.0**: The default I/O schedulers typically used **single-queue (SQ)** approaches (e.g., **deadline**, **cfq**, **noop**).  
**Linux 5.0+**: By default, the kernel uses **multi-queue (MQ)** schedulers (e.g., **mq-deadline**, **bfq**, **kyber**, **none**).

By **5.3**, the SQ I/O scheduling code was removed. On older kernels (<5.3), you can disable multi-queue to revert to single-queue. This note describes how to switch schedulers on modern systems (e.g., Ubuntu with kernel 5.15) and how the kernel chooses or loads BFS drivers for block devices.

---

## 2. Checking If `blk-mq` is Enabled

On Ubuntu (or other distros), you can see if multi-queue scheduling is active for SCSI or DM (device-mapper) devices:

```bash
cat /sys/module/scsi_mod/parameters/use_blk_mq
Y
cat /sys/module/dm_mod/parameters/use_blk_mq
Y
```

- **`Y`** means the block layer is using the MQ approach for that driver.
- **`N`** means single-queue approach.

To **disable** `blk-mq`, edit `/etc/default/grub` and add:
```plaintext
GRUB_CMDLINE_LINUX_DEFAULT="scsi_mod.use_blk_mq=N dm_mod.use_blk_mq=N ..."
```
Then run `sudo update-grub` and reboot to apply. *(Caution: only older kernels allow reverting to SQ. 5.3+ removed that code entirely.)*

---

## 3. Switching the I/O Scheduler per Device

### 3.1 Checking Current Scheduler

On a system with block device **`vda`** (like a KVM/QEMU disk):
```bash
cat /sys/block/vda/queue/scheduler
[mq-deadline] none
```
It indicates the default is **`mq-deadline`**, with **`none`** as an alternative. Brackets `[mq-deadline]` show the active choice.

If BFQ or Kyber aren’t shown, they might be absent as modules. Load them with:

```bash
sudo modprobe kyber-iosched
sudo modprobe bfq
cat /sys/block/vda/queue/scheduler
[mq-deadline] kyber bfq none
```

### 3.2 Changing Scheduler

To set the scheduler:

```bash
echo "bfq" | sudo tee /sys/block/vda/queue/scheduler
# or
echo "kyber" | sudo tee /sys/block/vda/queue/scheduler
# or
echo "none" | sudo tee /sys/block/vda/queue/scheduler
```

**Note**:
- For **NVMe** or advanced SSD, `none` or `mq-deadline` is often recommended. 
- `kyber` can yield high performance if carefully tuned. 
- `bfq` is more complex, focusing on fairness, might hamper throughput on HPC servers.

---

## 4. The Block Device Driver Layer

### 4.1 SCSI Subsystem

For many devices (SATA, SAS, USB) the kernel uses the **SCSI** stack, presenting them as `/dev/sdX`. The block device driver translates kernel-level requests into SCSI commands. For **multipath** setups, `dm_mod` is also involved.

**NVMe** devices use a separate subsystem, hooking into the block layer via the **`nvme`** driver. The driver implements multi-queue logic, typically giving each CPU a hardware dispatch queue.

### 4.2 Interaction with the I/O Scheduler

When an I/O request is dispatched from the I/O scheduling layer (SQ or MQ) to the block device, the driver:

1. Converts the request to device-specific command(s).
2. Submits them to the device’s hardware queue(s).
3. On completion, a hardware interrupt triggers the driver → notifies kernel, unblocks or frees resources.

---

## 5. Summarizing the I/O Stack

1. **User** space calls `read()` / `write()`.
2. **VFS** decides how to handle the file, possibly uses **Page Cache** for caching/dirty writes.
3. If the request is a block device request, it moves through the **Block Layer**.
4. The **I/O scheduler** merges or reorders requests in the device queue (SQ or MQ).
5. **Driver** hands them to the hardware.  
6. Completion returns from hardware → driver → block layer → process.

---

## 6. Code References & Examples

### 6.1 Reading Scheduler

```bash
cat /sys/block/sda/queue/scheduler
[mq-deadline] bfq kyber none
```

### 6.2 Setting SCSI Mod to Single Queue (older kernels)

```bash
# in /etc/default/grub
GRUB_CMDLINE_LINUX_DEFAULT="scsi_mod.use_blk_mq=N dm_mod.use_blk_mq=N"
sudo update-grub
sudo reboot
```

### 6.3 Tuning BFQ or Kyber

You might see attributes in `/sys/block/<dev>/queue/iosched/`. For BFQ:

```plaintext
/sys/block/<dev>/queue/iosched/bfq
```

where you can adjust parameters like `low_latency` or `slice_idle`.

---

## 7. Best Practices

- **NVMe** or high-performance SSD: Usually **`none`** or **`mq-deadline`**. Possibly **`kyber`** if you want advanced queue control and are willing to tune.  
- **HDD** or older SATA: Possibly **`mq-deadline`** in new kernels or the old SQ deadline if your kernel <5.3.  
- **Fairness** for desktop/multimedia: **`bfq`** if you need per-process I/O fairness at the cost of overhead.  
- For large HPC or DB servers: simpler schedulers often yield best throughput.  

---

## 8. Conclusion

In modern Linux (5.x+), the **multi-queue** design has replaced single-queue for better concurrency on SSD/NVMe. The kernel still offers multiple MQ schedulers:

1. **mq-deadline**: Balanced, merges some requests, good for many typical workloads.  
2. **bfq**: Complex fairness, best for interactive or real-time, can degrade throughput if not tuned.  
3. **kyber**: Focused on HPC servers or data center usage with moderate overhead.  
4. **none**: Minimal overhead, best if the hardware (NVMe) does internal scheduling well.

Switching is done by writing the desired scheduler name to `/sys/block/<dev>/queue/scheduler`. If required, load modules for bfq or kyber. Finally, note that if the kernel is older than 5.3, you can revert from multi-queue to single-queue by disabling `use_blk_mq`. After 5.3, that code is removed, so multi-queue is the only option for new kernels.

```