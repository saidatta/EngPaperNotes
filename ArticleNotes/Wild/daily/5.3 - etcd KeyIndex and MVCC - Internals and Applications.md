## Overview
This note expands on etcd’s **KeyIndex** structure, which underpins the Multi-Version Concurrency Control (MVCC) model in etcd v3. We will explore:

1. **KeyIndex** fields and how etcd stores different versions of the same key  
2. The read and write operations in etcd’s MVCC modules  
3. How etcd recovers its in-memory index at startup  
4. The distinction between `kvstore` and `watchableStore` and how the **Watch** mechanism is implemented  
5. Typical usage scenarios for etcd in distributed systems  

---

## 1. `KeyIndex` Internals

### 1.1 Multiple Generations for a Single Key

In etcd, a **Key** can have multiple *versions* across time. Each key has a `keyIndex` structure that tracks:

- **Generations** (`generations []generation`): each *generation* represents the life cycle of a key “from birth to deletion.”  
- When a key is **deleted**, etcd appends a special “tombstone” version to indicate the end of that generation, then starts a **new** generation for subsequent re-creations of the key.

```mermaid
flowchart LR
    A((keyIndex)) --> B[Generation #1<br>(key created)]
    B --> C[... versions ...]
    C --> D[Generation #2<br>(key re-created)]
    D --> E[... versions ...]
    E --> F[Tombstone (deleted)]
```

**Core** fields in `keyIndex`:

```go
type keyIndex struct {
    key       []byte
    modified  revision         // last modification revision
    generations []generation   // all generations of this key
}
```
- **generation** in turn holds a slice of `revs []revision` plus `created`, `ver` counters, etc.

#### Tombstone
When a key is deleted, `tombstone()` is called:

```go
func (ki *keyIndex) tombstone(lg *zap.Logger, main int64, sub int64) error {
    if ki.generations[len(ki.generations)-1].isEmpty() {
        return ErrRevisionNotFound
    }
    ki.put(lg, main, sub) // record the final revision
    ki.generations = append(ki.generations, generation{})
    return nil
}
```
This appends a tombstone revision and begins a new empty generation.

---

## 2. Read Operations in etcd

### 2.1 The `rangeKeys` Workflow

All etcd read queries eventually go through `rangeKeys`:

```go
func (tr *storeTxnRead) rangeKeys(key, end []byte, curRev int64, ro RangeOptions) (*RangeResult, error) {
    revpairs := tr.s.kvindex.Revisions(key, end, ro.Rev)
    if len(revpairs) == 0 {
        return &RangeResult{KVs: nil, Count: 0, Rev: curRev}, nil
    }

    kvs := make([]mvccpb.KeyValue, int(ro.Limit))
    revBytes := newRevBytes()
    for i, revpair := range revpairs[:len(kvs)] {
        revToBytes(revpair, revBytes)
        _, vs := tr.tx.UnsafeRange(keyBucketName, revBytes, nil, 0)
        kvs[i].Unmarshal(vs[0])
    }
    return &RangeResult{KVs: kvs, Count: len(revpairs), Rev: curRev}, nil
}
```
1. **Get** the relevant revisions from `ti.kvindex.Revisions(...)`.  
2. **Fetch** the actual serialized value from BoltDB with `UnsafeRange(...)`.  
3. **Unmarshal** into an `mvccpb.KeyValue`.

### 2.2 Finding the Correct Revision in the B-Tree

`ti.Revisions(key, end, atRev)` uses a **B-tree** to find all relevant versions:

```go
func (ti *treeIndex) Revisions(key, end []byte, atRev int64) (revs []revision) {
    if end == nil {
        rev, _, _, err := ti.Get(key, atRev)
        if err != nil { return nil }
        return []revision{rev}
    }
    ti.visit(key, end, func(ki *keyIndex) {
        if rev, _, _, err := ki.get(ti.lg, atRev); err == nil {
            revs = append(revs, rev)
        }
    })
    return revs
}
```
- **Single key**: calls `ti.Get(key, atRev)` → returns a single revision.
- **Range query**: uses `ti.visit(...)` to traverse the B-tree, calls `keyIndex.get(...)` on each key.

#### `keyIndex.get(...)`
```go
func (ki *keyIndex) get(lg *zap.Logger, atRev int64) (modified, created revision, ver int64, err error) {
    g := ki.findGeneration(atRev)
    if g.isEmpty() {
        return revision{}, revision{}, 0, ErrRevisionNotFound
    }

    n := g.walk(func(rev revision) bool { return rev.main > atRev })
    if n != -1 {
        return g.revs[n], g.created, g.ver - int64(len(g.revs)-n-1), nil
    }
    return revision{}, revision{}, 0, ErrRevisionNotFound
}
```
- Locates the appropriate generation for `atRev`.
- Finds the specific `revision` from `g.revs[...]`.

---

## 3. Write Operations in etcd

### 3.1 Adding a Key (`Put`)

```go
func (ti *treeIndex) Put(key []byte, rev revision) {
    keyi := &keyIndex{key: key}
    item := ti.tree.Get(keyi)
    if item == nil {
        keyi.put(ti.lg, rev.main, rev.sub)
        ti.tree.ReplaceOrInsert(keyi)
        return
    }
    // found existing key
    okeyi := item.(*keyIndex)
    okeyi.put(ti.lg, rev.main, rev.sub)
}
```

Then `keyIndex.put(...)` is called:

```go
func (ki *keyIndex) put(lg *zap.Logger, main int64, sub int64) {
    rev := revision{main: main, sub: sub}
    if len(ki.generations) == 0 {
        ki.generations = append(ki.generations, generation{})
    }
    g := &ki.generations[len(ki.generations)-1]
    if len(g.revs) == 0 {
        g.created = rev
    }
    g.revs = append(g.revs, rev)
    g.ver++
    ki.modified = rev
}
```
- If needed, creates a new generation.
- Appends the new revision, updates `created`/`ver` if first version in that generation.

### 3.2 Deleting a Key

```go
func (tw *storeTxnWrite) delete(key []byte) {
    idxRev := revision{main: tw.beginRev + 1, sub: int64(len(tw.changes))}
    ibytes := revToBytes(idxRev, ...)

    ibytes = appendMarkTombstone(ibytes)

    kv := mvccpb.KeyValue{Key: key}
    d, _ := kv.Marshal()
    tw.tx.UnsafeSeqPut(keyBucketName, ibytes, d)

    tw.s.kvindex.Tombstone(key, idxRev)
    tw.changes = append(tw.changes, kv)
}
```
- **Marks** the key as a tombstone in BoltDB (`appendMarkTombstone`).
- Calls `Tombstone(key, idxRev)` on the in-memory index, which appends a new tombstone revision in `keyIndex`.

---

## 4. Index Recovery at Startup

Because `keyIndex` is stored **in memory**, etcd must rebuild it from BoltDB each time it starts. This is done via `store.restore()`:

```go
func (s *store) restore() error {
    min, max := newRevBytes(), newRevBytes()
    revToBytes(revision{main: 1}, min)
    revToBytes(revision{main: math.MaxInt64, sub: math.MaxInt64}, max)

    tx := s.b.BatchTx()

    rkvc, revc := restoreIntoIndex(s.lg, s.kvindex)
    for {
        keys, vals := tx.UnsafeRange(keyBucketName, min, max, restoreChunkKeys)
        if len(keys) == 0 {
            break
        }
        restoreChunk(s.lg, rkvc, keys, vals, keyToLease)
        // move min to next revision
    }
    close(rkvc)
    s.currentRev = <-revc

    return nil
}
```

1. **Traverse** BoltDB between `[rev=1, rev=∞]`.  
2. For each chunk, calls `restoreChunk(...)` to parse keys into channel `rkvc`.  
3. A goroutine launched by `restoreIntoIndex(...)` consumes from `rkvc`, calls `keyIndex.put(...)` or `keyIndex.tombstone(...)`.

This “producer-consumer” approach decouples reading BoltDB keys from building the B-tree in memory:

```mermaid
flowchart LR
   A[UnsafeRange in BoltDB] --> B[restoreChunk <br> (send revKeyValue)]
   B --> C[(rkvc Channel)]
   C --> D[restoreIntoIndex goroutine <br> Rebuild treeIndex in memory]
```

---

## 5. `kvstore` vs `watchableStore`

### 5.1 `kvstore`

`kvstore` is a **basic** store providing CRUD operations:
- Coordinates with BoltDB (via `ReadTx` and `BatchTx`) to persist changes.
- Maintains in-memory B-tree index for fast lookups.
- Exposes typical methods: `Read()`, `Write()`, etc.

```go
func (s *store) Read() TxnRead {
    s.mu.RLock()
    tx := s.b.ReadTx()
    s.revMu.RLock()
    tx.Lock()
    ...
    return newMetricsTxnRead(&storeTxnRead{s, tx, firstRev, rev})
}
```
**Responsible** for data retrieval, range queries, single key queries, etc.

### 5.2 `watchableStore`

An extension adding **Watch** functionality:
- Embeds a `store` plus two **watcherGroup** fields: `unsynced` and `synced`.
- A background goroutine calls `syncWatchersLoop` every ~100ms to pull new revisions from BoltDB and notify watchers of changes.

```go
type watchableStore struct {
    *store

    mu sync.RWMutex
    unsynced watcherGroup
    synced   watcherGroup

    stopc chan struct{}
    wg    sync.WaitGroup
}
```
**Watch** flow:
1. A client issues a gRPC `Watch` request.
2. etcd server spawns two goroutines per watch:  
   - **recvLoop**: listens for watch commands (create, cancel, progress).  
   - **sendLoop**: emits watch events to the client.  
3. `watchableStore.watch()` registers a `watcher` in `unsynced` or `synced`.

#### `syncWatchers()`
```go
func (s *watchableStore) syncWatchers() int {
    curRev := s.store.currentRev
    wg, minRev := s.unsynced.choose(...)
    // get keys/vals from BoltDB [minRev, curRev+1)
    // convert to event lists
    // send events to watchers
    // move watchers from unsynced -> synced
    return s.unsynced.size()
}
```
- Gets recent changes from BoltDB, transforms them into events, and dispatches to watchers via channels.

**Diagram**:

```mermaid
flowchart LR
    A[Client Watch Request] --> B[gRPC watchServer]
    B --> C[watchStream <br> (sendLoop, recvLoop)]
    C --> D[watchableStore.watch()]
    D --> E[watcherGroup (unsynced)]
    E --> F[syncWatchersLoop <br> (pull changes from BoltDB)]
    F --> E
    F --> G[Notify watchers <br> move to synced if up-to-date]
```

---

## 6. Typical Usage Scenarios

1. **Service Discovery**:  
   - Microservices register their endpoints in etcd. Others watch for changes to discover new or removed instances.
2. **Distributed Configuration**:  
   - Central config keys in etcd. Services watch for changes and reload dynamically.
3. **Distributed Lock / Coordination**:  
   - Built on top of etcd’s strong consistency.  
   - Compare-and-swap or lease-based locks.
4. **Kubernetes**:  
   - etcd is the primary data store for cluster state: pods, deployments, configmaps, etc.

**etcd** is chosen for:
- Simpler deployment and modern codebase (written in Go).
- Good performance, active community, horizontally scalable with Raft-based replication.

---

## 7. Summary

- **KeyIndex**:  
  - Tracks multiple generations of a single key (birth → modifications → tombstone).  
  - Each generation stores an array of `revision`s.  
- **Reads** in etcd:  
  - B-tree (in-memory) → find the appropriate revision → fetch actual value from BoltDB.  
- **Writes** in etcd:  
  - Insert new revision in `keyIndex`, store data in BoltDB.  
  - Key deletion appends a tombstone revision.  
- **Index Recovery**:  
  - On restart, etcd scans BoltDB to rebuild the B-tree in memory.  
- **`kvstore` vs. `watchableStore`**:  
  - `kvstore`: Basic CRUD.  
  - `watchableStore`: Additional watchers, periodic syncs for changes.  
- **Use Cases**:  
  - Service discovery, config management, distributed locks, etc.

By examining etcd’s KeyIndex, watchers, and underlying MVCC store, we see how a **strongly consistent** distributed KV store can serve as a **building block** for higher-level distributed coordination systems.

---

## References

- [etcd GitHub Repository](https://github.com/etcd-io/etcd)  
- [MVCC internals in etcd](https://etcd.io/docs)  
- [Zookeeper vs etcd Coordinations](https://zookeeper.apache.org)  
- [Raft Protocol Paper](https://raft.github.io/)  

```