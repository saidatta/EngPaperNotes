Below is an **Obsidian**-formatted set of **very detailed** notes for a **PhD-level engineer** on **“OS: Async I/O”** (continued), focusing on **epoll**, **io_uring**, and cross-platform abstractions like **libuv**. These notes include code snippets, conceptual explanations, diagrams, tables, and thorough commentary. Feel free to copy/paste into your Obsidian vault.

## 1. epoll: The (Mostly) Standard for Asynchronous I/O on Linux

### 1.1 Basics
- **epoll** is a Linux-specific readiness-based mechanism for handling large numbers of file descriptors (especially sockets) **asynchronously**.
- It improves upon traditional `select()` / `poll()` by:
  1. Allowing **one-time registration** of FDs to watch.
  2. Only returning “ready” descriptors (no O(n) scanning).
  3. Handling events in either **edge-triggered** or **level-triggered** modes.

### 1.2 Flow and System Calls
1. **`epoll_create()`**: Creates an epoll instance → returns a special fd (the **epoll fd**).
2. **`epoll_ctl()`**: Add / Modify / Remove FDs in the epoll instance.
3. **`epoll_wait()`**: Blocks until at least one FD is ready, then returns an **array** of **events** describing which FDs are readable/writable/etc.

#### Example Pseudocode
```c
int epfd = epoll_create1(0); // create epoll instance
// add connection socket fd
struct epoll_event event;
event.events = EPOLLIN;         // or EPOLLOUT, EPOLLIN | EPOLLET, etc.
event.data.fd = conn_fd;
epoll_ctl(epfd, EPOLL_CTL_ADD, conn_fd, &event);

// In a loop:
struct epoll_event evlist[64];
int n = epoll_wait(epfd, evlist, 64, -1); // block until something ready
for (int i = 0; i < n; i++) {
    // evlist[i].data.fd is ready
    // read() or write() as appropriate
}
```

### 1.3 epoll Data Structures
1. **Interest List**: The set of file descriptors (sockets) you’ve registered.  
2. **Readiness Lists**: Internally, the kernel tracks which descriptors have pending events (e.g., data arrived on a socket).  
3. **epoll_wait** returns those ready descriptors to user space, so you only iterate over the *actually* ready set.

### 1.4 Edge-Triggered vs. Level-Triggered
- **Edge-Triggered (ET)**: Notifies you **only** on transitions from non-ready → ready. You must keep reading until `EAGAIN` or you risk missing data.
- **Level-Triggered (LT)**: Notifies you **while** the FD remains ready. The kernel can repeatedly tell you about the same FD if data is still unread.

**Trade-Off**:
- **ET**: Fewer syscalls from kernel to user space once a descriptor becomes ready, but you **must** handle partial reads/writes carefully.  
- **LT**: Simpler but can spam notifications if you don’t consume all data quickly.

### 1.5 Pros and Cons

| **Pros**                          | **Cons**                                                   |
|-----------------------------------|------------------------------------------------------------|
| \- High-performance readiness I/O | \- Linux-only (no direct Windows/macOS equivalent)        |
| \- Avoids O(n) scanning           | \- Still uses syscalls (`epoll_wait`) frequently          |
| \- Scales to millions of sockets  | \- Doesn’t handle disk I/O well (files are “always ready”) |

---

## 2. io_uring: Completion-Based Async I/O on Linux

### 2.1 Motivation
- **epoll** and similar readiness-based solutions focus on network FDs. For **files** (block devices), readiness is less helpful; a file is “always ready,” but the operation may still block internally.
- **io_uring** introduces **completion-based** I/O in Linux, letting you queue I/O operations to the kernel and get **completion events**.

### 2.2 Design
- **Shared Memory**: There is a ring buffer in shared memory accessible by both user space and kernel.
- Two ring buffers (conceptually):
  1. **Submission Queue** (SQ): where user space places I/O requests (reads, writes, accepts, etc.).
  2. **Completion Queue** (CQ): where the kernel posts results (status, bytes transferred, errors).

No repeated syscalls for each operation; user space can fill many SQ entries, then do a single `io_uring_submit()` system call to tell the kernel to process them.

#### Visual Diagram

```mermaid
flowchart LR
    subgraph Process (User Space)
    A[Submit Queue (SQ)] -- put requests --> B
    D[Completion Queue (CQ)] -- read results --> C
    end

    subgraph Kernel
    B[/Kernel reads requests/]
    B --> Execute I/O
    B --> E[Write completion]
    E --> D
    end
```

**User Flow**:
1. Prepare an **SQE** (Submission Queue Entry) describing the I/O (e.g. read from fd X into buffer Y).
2. `io_uring_submit()` to notify kernel.
3. Kernel performs the I/O asynchronously, writes a **CQE** (Completion Queue Entry) into the **CQ**.
4. User space reads the CQE → sees how many bytes read, any error codes, etc.

### 2.3 Performance & Security
- Very **fast**: fewer syscalls, minimal copying, ideal for large-scale asynchronous apps mixing network and file I/O.
- **Security Issues**: Shared memory interfaces are complex. Recent kernel vulnerabilities show io_uring can expose serious bugs.  
- Google and others have restricted or disabled io_uring in certain products until it matures further.

---

## 3. Cross-Platform Async: The Role of libuv

### 3.1 Problem Statement
- Windows, macOS, BSD, Linux each have different async or event-driven mechanisms (IOCP, kqueue, epoll, event ports, etc.).
- File I/O, DNS, user-level tasks might still require **thread pools**.

### 3.2 libuv
- A **library** behind Node.js that abstracts OS differences:
  - On Linux: uses epoll for network events, threads for file I/O.
  - On Windows: uses IOCP for sockets, possibly threads for file I/O.
  - On macOS: uses kqueue, plus a thread pool for CPU tasks or blocking ops.
- Provides a uniform async interface for Node’s event loop.

### 3.3 Thread Pools
- For tasks that cannot be easily made non-blocking (like file I/O on some OSes, DNS lookups, or CPU-bound crypto), **libuv** spawns a small thread pool.  
- The main event loop remains unblocked while these threads do the blocking tasks.

---

## 4. System Call Overheads & Context Switching

Even with **epoll** or **io_uring**, you do have kernel interactions:
- **epoll_wait()** still requires a syscall to enter kernel mode.
- **io_uring_submit()** / `io_uring_enter()` is also a syscall for telling kernel about new SQ entries.

**Context Switch** Overhead:
- Each syscall forces the CPU to flush certain registers, change privilege levels, etc.
- **io_uring** reduces syscalls by letting user space queue multiple I/O operations at once.  
- Shared memory ring buffers allow minimal user-kernel transitions.

---

## 5. Putting It All Together

### 5.1 Typical Patterns

1. **epoll-based Event Loop**  
   - `epoll_ctl()` to register sockets once.  
   - `epoll_wait()` in a loop, receiving ready FDs.  
   - For each FD: call non-blocking `read()` / `write()`.  
   - Suited for network servers (Nginx, Node.js, HAProxy, etc.).

2. **io_uring-based**  
   - Single ring structure for network and file I/O in Linux 5.x.  
   - Enqueue read/write requests, kernel completes them asynchronously.  
   - Potentially higher throughput if used correctly, but security concerns must be considered.

3. **libuv**  
   - Cross-platform abstraction used by Node.js.  
   - Under the hood, uses **epoll/kqueue/IOCP** for sockets, a thread pool for blocking tasks.  

### 5.2 Example Table: Async I/O Mechanisms Recap

| **Mechanism**  | **OS**       | **Readiness or Completion** | **Key Syscalls**        | **Handles Files?**         | **Pros**                        | **Cons**                                    |
|----------------|--------------|-----------------------------|-------------------------|-----------------------------|---------------------------------|---------------------------------------------|
| **select/poll**| POSIX        | Readiness                  | `select()`, `poll()`    | Very limited for files      | Simple but old design            | O(n) scanning, no scalability for large sets|
| **epoll**      | Linux        | Readiness                  | `epoll_*()`             | Not suitable for file I/O   | High perf. for net sockets       | Many syscalls, Linux-only                   |
| **kqueue**     | BSD/macOS    | Readiness                  | `kqueue()`, `kevent()`  | Partially supports files    | Similar to epoll for *BSD/macOS  | Still OS-specific, learning curve           |
| **IOCP**       | Windows      | Completion                 | `CreateIoCompletionPort`, etc. | Yes (files & sockets)    | True async for net & disk        | Windows-only                                |
| **io_uring**   | Linux >=5.x  | Completion                 | `io_uring_*()`          | Yes, net & files            | Very high perf, fewer syscalls   | Still maturing, security vulnerabilities    |

---

## 6. Conclusion & Key Takeaways

1. **epoll** is the de facto standard for high-performance **non-blocking** network I/O on Linux. It’s built on **readiness**.  
2. **io_uring** is an advanced, **completion-based** approach on modern Linux kernels, aiming to unify socket and file I/O with minimal overhead.  
3. **Security** vs. **Performance**: Shared memory and ring buffers can lead to vulnerabilities, so major organizations are cautious in production.  
4. **Cross-platform**: Tools like **libuv** hide OS differences, letting Node.js (and others) run efficiently on Windows, Linux, macOS.  
5. Understanding **context switches** and **syscall overhead** is crucial for building large-scale asynchronous systems.

---

## 7. Additional Reading

- **Linux `epoll` man page**: `man 7 epoll`  
- **io_uring**: [https://github.com/axboe/liburing](https://github.com/axboe/liburing) - Official GitHub repo  
- **Google’s Security Post** on io_uring: Summaries of vulnerabilities found, best practices for limiting exposure.  
- **Windows IOCP**: Microsoft Docs on [I/O Completion Ports](https://docs.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports).  
- **libuv**: [https://github.com/libuv/libuv](https://github.com/libuv/libuv) - used by Node.js.

**Links to Other Notes**:
- [[OS: Sockets, Connections, and Kernel Queues]]
- [[OS: Sending and Receiving Data]]
- [[OS: File Storage Management Demo]]
- [[Network Programming Basics]]

**Tags**:
- #OperatingSystems  
- #AsyncIO  
- #epoll  
- #ioUring  
- #libuv  

---

**End of Notes**.