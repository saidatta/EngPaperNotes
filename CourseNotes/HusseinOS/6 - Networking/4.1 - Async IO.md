Below is an **Obsidian**-formatted set of very detailed notes on **“OS: Async I/O.”** These notes consolidate the material from the lecture, providing thorough coverage of **blocking vs. non-blocking**, **readiness-based** (select/poll/epoll/kqueue) vs. **completion-based** (IOCP, io_uring) asynchronous I/O, including examples, code snippets, visualizations, tables, and extended commentary suitable for a PhD-level engineer.

## 1. Introduction

Modern operating systems offer **asynchronous I/O** methods to avoid blocking system calls. When an application performs a **blocking** I/O call (e.g., `read()` on a socket with no incoming data), the calling thread **blocks** until data is available, potentially causing **context switches** and hindering concurrency.

**Asynchronous I/O** (async I/O) techniques let processes or threads avoid blocking by:
1. Checking if an I/O operation is **ready** (read/write would succeed immediately), or
2. Submitting an I/O request and being **notified** (or collecting results) when it’s done.

This note explores:
- **Blocking vs. Non-Blocking** I/O
- **Readiness-based** multiplexing (select, poll, epoll, kqueue)
- **Completion-based** I/O (Windows IOCP, Linux io_uring)
- Implementation details, pros/cons, code snippets, and performance considerations

---

## 2. Blocking vs. Non-Blocking I/O

### 2.1 Blocking I/O Recap

A standard **POSIX read** (or `accept()`) in blocking mode:

- **Blocks** the calling thread if no data is in the **receive buffer** (for a socket), or if the **accept queue** is empty (for a listening socket).
- The thread can’t proceed until data arrives or a connection is established.
- Results in **context switching** if the kernel puts the thread to sleep.

#### Example of a Blocking Loop
```c
for (int i = 0; i < nconns; i++) {
    // read() on connection i
    // if connection i has no data, we block here indefinitely!
    read(conns[i], buf, sizeof(buf));
    // never reach here for other conns until data arrives
}
```
If the 3rd connection has no data (gray, empty buffer), we block and never process further connections.

### 2.2 Non-Blocking Sockets

- A **non-blocking** socket returns immediately from `read()`, `write()`, or `accept()`:
  - `read()` yields **-1** with `errno=EAGAIN` (or `EWOULDBLOCK`) if no data is ready.
  - `write()` yields **-1** with `errno=EAGAIN` if the send buffer is full.
  - The code can then try other operations or come back later.

**However**, you need a mechanism to know *when* a descriptor might be ready—otherwise you’d be spinning in a loop calling `read()` on every descriptor.

---

## 3. Readiness Approach: select, poll, epoll, kqueue

### 3.1 Concept

**Readiness-based** means the kernel tells you **which** file descriptors are *ready* for read or write. The application can then perform the actual `read()` or `write()` **without blocking**.

#### 3.1.1 Key Idea
1. Provide a list of descriptors (sockets, pipes, etc.) to the kernel.
2. Kernel checks readiness (data in buffer for read, free space for write).
3. Kernel returns a list or set of descriptors that are ready.
4. The application calls `read()` or `write()` on *only* those that are ready.

### 3.2 `select()`

**`select()`** is an older POSIX call:

```c
#include <sys/select.h>

int select(int maxfd, fd_set *readfds, fd_set *writefds, 
           fd_set *exceptfds, struct timeval *timeout);
```

1. You fill `readfds` with descriptors to watch for read.
2. The kernel blocks until at least one descriptor is ready or `timeout` expires.
3. You check each descriptor with `FD_ISSET(fd, &readfds)` to see which ones are actually ready.

**Drawbacks**:

- Maximum descriptor limit (often 1024).
- You must re-populate the sets **every time** you call `select()`.
- On return, you do a loop over all descriptors to find which are ready (O(n) scanning).

#### Visualization

```mermaid
flowchart LR
    A[App: fd_set with 10 conns] --> B[select(fd_set)]
    B -->|waits until any ready| C[returns to user space]
    C --> D[Loop over all 10 conns: FD_ISSET?]
    D --> E[Perform read() on ready ones]
```

### 3.3 `poll()`

**`poll()`** uses an array of `struct pollfd`:
```c
struct pollfd {
    int fd;         // file descriptor
    short events;   // events of interest (POLLIN, POLLOUT)
    short revents;  // events returned
};
```
- You pass an array + a timeout.
- Kernel modifies `revents` for each entry, telling you if it’s readable/writable.
- Still re-iterates the entire array.

**Pros**: No fixed max descriptor limit (unlike select).
**Cons**: You must re-build the pollfd array each time, still O(n) scanning on return.

### 3.4 `epoll` (Linux) or `kqueue` (BSD/macOS)

**`epoll`** on Linux (similar to **`kqueue`** on BSD/macOS) improves:

1. **Registration**: You create an epoll instance once (`epoll_create()`), then add or remove file descriptors with `epoll_ctl()`.  
2. **Wait**: Call `epoll_wait()` to get an array of *only the ready descriptors*—no scanning needed.

**epoll** can operate in **edge-triggered** or **level-triggered** modes.  
- Edge-triggered: Notified once when a descriptor transitions from not-ready to ready.  
- Level-triggered: Notified as long as a descriptor is in a ready state.

#### epoll Example

```c
int epfd = epoll_create1(0);

struct epoll_event event;
event.events = EPOLLIN;
event.data.fd = sock;
epoll_ctl(epfd, EPOLL_CTL_ADD, sock, &event);

// Now in your loop:
struct epoll_event evlist[64];
int n = epoll_wait(epfd, evlist, 64, -1);
for (int i = 0; i < n; i++) {
    if (evlist[i].events & EPOLLIN) {
        int fd = evlist[i].data.fd;
        // read from 'fd' without blocking
    }
}
```

**Pros**:
- No repeated scanning of all descriptors. Kernel directly returns which are ready.
- Scalable to thousands or millions of sockets.

**Cons**:
- Only notifies about readiness, not completion of an entire read/write operation.
- For disk files, often “always ready,” so it doesn’t solve the real problem with slow disks (synchronicity in block device I/O).

| Mechanism    | OS Compatibility     | Complexity | Performance | Typical Usage  |
|--------------|----------------------|-----------:|------------|----------------|
| **select()** | POSIX, very old      | Low        | Low        | Old code, small apps |
| **poll()**   | POSIX, universal     | Medium     | Medium     | Medium-scale servers |
| **epoll()**  | Linux-only          | Medium     | High       | Modern high-scale (Linux) |
| **kqueue()** | BSD/macOS           | Medium     | High       | Modern high-scale (BSD/macOS) |

---

## 4. Completion Approach: IOCP & io_uring

### 4.1 Concept

In **completion-based** async I/O, you **submit** an I/O request (like “read 8 KB from this file/socket into this buffer”) to the kernel. The kernel does the operation in the background. Once done, you get a **completion event** with the result (bytes read, error code, etc.).

### 4.2 Windows IOCP (I/O Completion Ports)

On **Windows**, `CreateIoCompletionPort()` allows you to bind a socket or file handle to a completion port. You do asynchronous calls (`ReadFileEx()`, etc.). The OS queues a completion packet to the IOCP when the operation finishes. Threads can block on `GetQueuedCompletionStatus()` to retrieve results.

**Pros**:
- No constant polling for readiness. Actual result data arrives asynchronously.
- Good for both **network** and **file** I/O.

**Cons**:
- Windows-only, more complex API.

### 4.3 Linux io_uring

**io_uring** (since Linux 5.x) is a modern interface that merges **submission queue** and **completion queue** in shared memory:

1. You create a ring buffer with `io_uring_setup()`.
2. Post operations (read, write, accept, etc.) into the **submission queue** (SQ).
3. Kernel picks them up, does the I/O, posts results in the **completion queue** (CQ).
4. User space reads the CQ for final status (bytes transferred, errors).

**Advantages**:

- **Zero-copy** approach to the ring buffers (no repeated system call overhead to register events).
- Good for **files** as well as **sockets**.  
- High performance for high concurrency servers.

**Example Snippet (Pseudo-C)**:
```c
#include <liburing.h>

struct io_uring ring;
io_uring_queue_init(256, &ring, 0); // 256 entries

// Prepare a read operation
struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);
io_uring_prep_read(sqe, fd, buf, 1024, 0);
// Some user data to identify
sqe->user_data = 12345;

// Submit
io_uring_submit(&ring);

// Later, retrieve completions
struct io_uring_cqe *cqe;
int ret = io_uring_wait_cqe(&ring, &cqe);
if (ret == 0) {
    unsigned res = cqe->res; // bytes read or error
    uint64_t userdata = cqe->user_data;
    io_uring_cqe_seen(&ring, cqe);
}
```

**Pros**:
- No scanning for readiness. The kernel completes the operation asynchronously.
- Works for **file** and **network** I/O.
- Very high performance if used correctly.

**Cons**:
- Newer, evolving interface on modern Linux kernels.

---

## 5. Example Comparison: Reading from Socket with Readiness vs. Completion

### 5.1 Readiness (epoll)
1. **epoll_ctl()** to add the socket.
2. **epoll_wait()** returns when the socket is *readable* (some data in the buffer).
3. User calls **`read()`** → blocks *only momentarily* to copy data from kernel to user space. (But if data is large, partial reads might require multiple epoll events.)

### 5.2 Completion (io_uring)
1. Prep a read with a user buffer of e.g. 8 KB.  
2. `io_uring_submit()` to the kernel.  
3. Kernel reads data directly into your buffer in background; once done, an event is posted.  
4. You retrieve from **completion queue** → see how many bytes arrived.

---

## 6. Putting It All Together

1. **Blocking** calls are simplest but can cause threads to stall, requiring large thread pools.  
2. **Non-blocking** + **readiness** approach is widely used in event-driven servers (Node.js, Nginx, etc.) with `select/poll/epoll/kqueue`.  
3. **Completion** approach is popular on Windows (IOCP) and is gaining traction on Linux with **io_uring**. Great for *both* network and **file** I/O.  
4. **Performance**:  
   - Minimizing context switches, system calls, and copying data are crucial for scale.  
   - **epoll** and **io_uring** are considered state-of-the-art on Linux.

---

## 7. Code & Visualizations

### 7.1 Simplified `epoll` Server Skeleton

```c
#include <sys/epoll.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <errno.h>

#define MAX_EVENTS 64

int main() {
    int listen_fd = create_listening_socket(8080); // Not shown

    int epfd = epoll_create1(0);
    struct epoll_event ev, events[MAX_EVENTS];
    ev.events = EPOLLIN;
    ev.data.fd = listen_fd;
    epoll_ctl(epfd, EPOLL_CTL_ADD, listen_fd, &ev);

    while(1) {
        int n = epoll_wait(epfd, events, MAX_EVENTS, -1);
        for(int i = 0; i < n; i++) {
            int fd = events[i].data.fd;
            if(fd == listen_fd) {
                // accept new connection
                int client_fd = accept(listen_fd, NULL, NULL);
                struct epoll_event client_ev;
                client_ev.events = EPOLLIN;
                client_ev.data.fd = client_fd;
                epoll_ctl(epfd, EPOLL_CTL_ADD, client_fd, &client_ev);
            } else {
                // read data from fd
                char buf[1024];
                ssize_t r = read(fd, buf, sizeof(buf));
                if(r > 0) {
                    // process data
                    // possibly write back
                } else if (r == 0) {
                    // client closed
                    close(fd);
                } else {
                    if(errno != EAGAIN) {
                        perror("read");
                        close(fd);
                    }
                }
            }
        }
    }
    return 0;
}
```

### 7.2 Comparison Table

| **Method**               | **OS**        | **Style**           | **Main Syscalls**           | **Suitable For**                          | **Notes**                                  |
|--------------------------|---------------|---------------------|-----------------------------|-------------------------------------------|--------------------------------------------|
| **Blocking**             | All           | Single-thread or multi-thread | `read()`, `write()`, etc. | Simpler code, smaller concurrency         | Each call can stall a thread              |
| **select()/poll()**      | POSIX         | Readiness (level)   | `select()`, `poll()`       | Medium concurrency servers                | O(n) scanning, repeated calls/updates     |
| **epoll() / kqueue()**   | Linux / BSD   | Readiness (level or edge) | `epoll_*()` / `kqueue()`   | High concurrency event loops             | Minimal overhead for large # of sockets   |
| **IOCP** (Windows)       | Windows       | Completion          | `GetQueuedCompletionStatus` | High concurrency servers, including files | Windows-only, widespread in .NET/Win Apps |
| **io_uring** (Linux)     | Linux >=5.x   | Completion          | `io_uring_*()`             | Both network & file high-performance async| New, advanced interface, minimal overhead |

---

## 8. Conclusion & Key Points

1. **Async I/O** solves the **blocking** problem by avoiding indefinite waits in `read()` or `accept()`.
2. **Readiness** approach (select/poll/epoll/kqueue) → widely used in event-driven servers, big improvement over naive blocking.  
3. **Completion** approach (Windows IOCP, Linux io_uring) → fully asynchronous model where you submit requests, kernel completes them in the background.  
4. **Performance**: Minimizing context switches and system calls is vital. Tools like **epoll** and **io_uring** can handle millions of connections efficiently on modern hardware.  
5. **Choice** depends on OS, scale requirements, code complexity, and whether you need asynchronous **file** I/O as well as sockets.

---

## 9. Further Reading

- **man pages**: `select(2)`, `poll(2)`, `epoll(7)`, `io_uring(7)`, etc.  
- **liburing**: [https://github.com/axboe/liburing](https://github.com/axboe/liburing) for practical usage examples.  
- **Windows IOCP**: Official MS Docs on “I/O Completion Ports.”  
- **Beej’s Guide**: [https://beej.us/guide/bgnet/](https://beej.us/guide/bgnet/) for simpler readiness-based examples.  
- **Network Programming**: “UNIX Network Programming” by W. Richard Stevens.  
- **High-concurrency** examples: Nginx source code (epoll), Node.js (libuv, epoll/kqueue on Unix, IOCP on Windows).

---

### Links to Other Notes

- [[OS: Sockets, Connections, and Kernel Queues]]  
- [[OS: Sending and Receiving Data]]  
- [[OS: File Storage Management Demo]]  
- [[Network Programming Basics]]  

**Tags**:  
- #OperatingSystems  
- #AsyncIO  
- #Concurrency  
- #epoll  
- #ioUring  

---

**End of Notes**.