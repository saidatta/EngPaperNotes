Below is an extensive set of Obsidian notes in Markdown format covering the lecture. These notes detail how to extend deep learning models for multi-class classification using the Iris dataset, explain the network architecture (including the role of bias, fully connected layers, and softmax vs. sigmoid), and include code snippets, numerical examples, and visualizations. You can paste the following into your Obsidian vault as a new note.

---

# Multi-class Classification with Deep Learning: The Iris Dataset & Fully Connected Networks

> *"This is going to be a fun lecture..."*  
> In this lecture, we learn how to extend our deep learning models to provide multiple outputs for classification problems (i.e., multi-class problems). We explore the famous Iris dataset, discuss the network architecture in detail, and see how these ideas are implemented in PyTorch.

---

## Table of Contents

1. [Lecture Overview](#lecture-overview)
2. [Background: The Iris Dataset](#background-the-iris-dataset)
3. [Model Architecture](#model-architecture)
    - [Layer Dimensions & Bias Terms](#layer-dimensions--bias-terms)
    - [Fully Connected Network Explanation](#fully-connected-network-explanation)
4. [Multi-class Classification: Softmax vs. Sigmoid](#multi-class-classification-softmax-vs-sigmoid)
5. [PyTorch Implementation](#pytorch-implementation)
    - [Data Preparation & Visualization](#data-preparation--visualization)
    - [Model Definition](#model-definition)
    - [Training Loop and Accuracy Calculation](#training-loop-and-accuracy-calculation)
    - [Inspecting Raw vs. Softmax Outputs](#inspecting-raw-vs-softmax-outputs)
6. [Visualizations & Training Dynamics](#visualizations--training-dynamics)
7. [Key Takeaways](#key-takeaways)
8. [References & Further Reading](#references--further-reading)

---

## Lecture Overview

- **Goal:**  
  Extend deep learning models to handle **multiple outputs** for a multi-class classification problem.
  
- **Focus Areas:**
  - Understanding the **Iris dataset**, a classic multivariate classification dataset.
  - Learning the terminology and architecture behind fully connected (feedforward) networks.
  - Developing and training models in **PyTorch**, including proper handling of the output layer (i.e., applying softmax).

---

## Background: The Iris Dataset

- **History & Importance:**
  - The Iris dataset is nearly 100 years old and has been used extensively for teaching multivariate classification.
  - It has a long-standing reputation in the machine learning community.

- **Dataset Details:**
  - **Features (Measurements):**
    - **Sepal Length**
    - **Sepal Width**
    - **Petal Length**
    - **Petal Width**
  - **Outcome (Label):**
    - Three species of iris: *Setosa*, *Versicolor*, and *Virginica*.
  
- **Visualization Insight:**
  - When visualizing pair plots of the features, no single pair can completely separate all three classes.  
  - This emphasizes that a multivariate (multi-feature) approach is required to achieve high accuracy.

---

## Model Architecture

We design a **fully connected** (feedforward) network with three layers to perform multi-class classification.

### Layer Dimensions & Bias Terms

1. **Input Layer:**  
   - **Input:** 4 features (optionally, think of bias as an extra constant input).
   - **Output:** 64 neurons.
   - **Weight Matrix:** \(4 \times 64\).

2. **Hidden Layer:**  
   - **Input:** 64 neurons.
   - **Output:** 64 neurons.
   - **Weight Matrix:** \(64 \times 64\).

3. **Output Layer:**  
   - **Input:** 64 neurons.
   - **Output:** 3 neurons (one per iris species).
   - **Weight Matrix:** \(64 \times 3\).

> **Note on Bias:**  
> Although not always depicted in diagrams, every layer includes a bias term (a constant value, typically 1) that is multiplied by its own set of learnable weights. In PyTorch’s `nn.Linear`, the bias is handled automatically.

### Fully Connected Network Explanation

- **Fully Connected (Feedforward):**
  - Every node in one layer connects to every node in the next layer.
  - The *arrows* in a network diagram represent the trainable parameters (i.e., the weight matrices).
  - The nodes (circles) represent the summation of weighted inputs plus the non-linear activation function.

### Architecture Diagram

```mermaid
flowchart TD
    A[Input Features (4)]
    B[Input Layer: 4 → 64]
    C[ReLU Activation]
    D[Hidden Layer: 64 → 64]
    E[ReLU Activation]
    F[Output Layer: 64 → 3]
    G[Softmax Transformation]
    
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
```

---

## Multi-class Classification: Softmax vs. Sigmoid

- **Why Not Sigmoid?**
  - In binary classification, a sigmoid is used to generate a probability for one class (with the other being its complement).
  - In multi-class classification, applying a sigmoid to each output independently would not guarantee that the sum of probabilities equals 1.

- **Softmax Function:**
  - **Purpose:** Convert raw outputs (logits) into a probability distribution over the classes.
  - **Property:** The output probabilities sum to 1.
  - **Implementation:** In PyTorch, `CrossEntropyLoss` automatically applies a LogSoftmax (and combines it with Negative Log Likelihood Loss).

---

## PyTorch Implementation

### Data Preparation & Visualization

```python
import seaborn as sns
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Load the Iris dataset from seaborn
iris_df = sns.load_dataset("iris")
print(iris_df.head())

# Generate pair plots to visualize feature relationships
sns.pairplot(iris_df, hue='species')
plt.show()

# Prepare data: select features and map labels to numerical values
features = iris_df.iloc[:, :4].values  # sepal_length, sepal_width, petal_length, petal_width
labels = iris_df['species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2}).values

# Convert the features and labels to PyTorch tensors
X = torch.tensor(features, dtype=torch.float32)
y = torch.tensor(labels, dtype=torch.long)
print("X shape:", X.shape)  # Expected shape: (150, 4)
print("y shape:", y.shape)  # Expected shape: (150,)
```

### Model Definition

```python
class IrisNet(nn.Module):
    def __init__(self):
        super(IrisNet, self).__init__()
        # Define the three fully connected layers
        self.input_layer = nn.Linear(4, 64)    # Maps 4 input features to 64 nodes
        self.hidden_layer = nn.Linear(64, 64)   # Hidden layer: 64 -> 64
        self.output_layer = nn.Linear(64, 3)    # Output layer: 64 -> 3 (one for each class)
    
    def forward(self, x):
        x = self.input_layer(x)
        x = torch.relu(x)  # Apply non-linearity after first layer
        x = self.hidden_layer(x)
        x = torch.relu(x)  # Apply non-linearity after hidden layer
        x = self.output_layer(x)  # Get raw outputs (logits)
        return x

# Instantiate the model, loss function, and optimizer
model = IrisNet()
criterion = nn.CrossEntropyLoss()  # Internally computes LogSoftmax + NLLLoss
optimizer = optim.SGD(model.parameters(), lr=0.01)
```

### Training Loop and Accuracy Calculation

```python
num_epochs = 100
loss_history = []
accuracy_history = []

for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    
    # Forward pass: compute logits
    outputs = model(X)  # outputs shape: (150, 3)
    
    # Compute the loss
    loss = criterion(outputs, y)
    loss_history.append(loss.item())
    
    # Backpropagation and parameter update
    loss.backward()
    optimizer.step()
    
    # Calculate accuracy by taking the argmax over logits
    _, predicted = torch.max(outputs, 1)
    correct = (predicted == y).sum().item()
    accuracy = correct / y.size(0)
    accuracy_history.append(accuracy)
    
    if (epoch+1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy*100:.2f}%")

print("Final Accuracy: {:.2f}%".format(accuracy_history[-1]*100))
```

### Inspecting Raw vs. Softmax Outputs

```python
# Evaluate the model without gradient tracking
model.eval()
with torch.no_grad():
    raw_outputs = model(X)
    print("Raw outputs (first 5 samples):\n", raw_outputs[:5])
    
    # Apply softmax to obtain probabilities
    softmax = nn.Softmax(dim=1)
    probabilities = softmax(raw_outputs)
    print("Softmax probabilities (first 5 samples):\n", probabilities[:5])
    
    # Verify that the probabilities for each sample sum to 1
    print("Sum of probabilities (first sample):", probabilities[0].sum().item())
```

---

## Visualizations & Training Dynamics

### Training Progress: Loss and Accuracy

```python
plt.figure(figsize=(12, 5))

# Plot Training Loss
plt.subplot(1, 2, 1)
plt.plot(loss_history, label='Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss over Epochs")
plt.legend()

# Plot Training Accuracy
plt.subplot(1, 2, 2)
plt.plot(accuracy_history, label='Accuracy', color='orange')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Training Accuracy over Epochs")
plt.legend()

plt.tight_layout()
plt.show()
```

- **Observations:**
  - The loss decreases smoothly over time.
  - Accuracy may start near chance levels (around 33% for 3 classes) and then jump to high accuracy (e.g., ~98%) once the network “figures out” the optimal weights.
  - Monitoring accuracy at each epoch helps in understanding the training dynamics and weight adjustments.

### Visualizing the Model’s Output Distribution

- **Raw vs. Softmax Outputs:**
  - **Raw Outputs (Logits):** Can be any real numbers and do not represent probabilities.
  - **Softmax Outputs:** Scale the raw outputs into a probability distribution.  
  - Plotting these side-by-side (or comparing their statistics) helps in understanding how the softmax function “corrects” the raw logits into interpretable probabilities.

---

## Key Takeaways

- **Data Understanding:**
  - The Iris dataset is a classic multi-feature, multi-class problem requiring all features for accurate classification.
  
- **Network Architecture:**
  - A fully connected network consists of layers where each neuron is connected to every neuron in the next layer.
  - Bias terms, though not always visualized, are essential and are automatically managed by PyTorch’s `nn.Linear`.

- **Activation Functions:**
  - **ReLU** introduces non-linearity between layers.
  - **Softmax** is crucial in multi-class classification to ensure that the outputs form a valid probability distribution (i.e., they sum to 1).

- **Loss Function:**
  - `CrossEntropyLoss` in PyTorch automatically applies LogSoftmax and computes the negative log-likelihood, streamlining the process for multi-class classification.

- **Training Dynamics:**
  - The network’s accuracy can exhibit “jagged” behavior before settling into high accuracy.
  - Using `argmax` on the output logits provides the predicted class, which is key to calculating accuracy.

---

## References & Further Reading

- **Iris Dataset:**  
  Fisher’s Iris dataset — a benchmark in multivariate classification problems.

- **PyTorch Documentation:**
  - [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)
  - [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

- **Deep Learning Texts:**
  - *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

- **Seaborn Documentation:**
  - [Pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)

---

*End of Note*

Feel free to expand or modify these notes as you continue your deep learning research and experiments. Happy coding and learning!