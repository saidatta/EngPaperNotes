
aliases: [Additional Meta-Parameters, Advanced Hyperparameter Tuning, Fine-Tuning Strategies]
tags: [Deep Learning, Lecture Notes, Meta Parameters, Advanced Topics]

This note **extends** the previously discussed **meta-parameter** topics (optimizers, regularization, learning-rate scheduling) by highlighting **advanced or less common** strategies. While the major parameters (e.g., learning rate, momentum, weight decay, hidden layers) are central to typical workflows, deeper research or sophisticated real-world projects might benefit from **further** meta-parameter tuning and exploration.

---
## 1. Beyond Basic Optimizers & LR Schedules

### 1.1 Other Learning Rate Schedulers

1. **ExponentialLR**:  
   - \(\text{LR}_\text{new} = \text{LR}_\text{old} \times \gamma^{\text{(epoch)}}\).  
   - Continuous decay each epoch rather than steps. 
2. **MultiStepLR**:
   - Define **milestones** (epochs) and reduce LR by \(\gamma\) at each milestone.
3. **ReduceLROnPlateau**:
   - Monitors a metric (e.g., validation loss). If it **plateaus** or worsens, reduce LR.  
   - Useful for tasks where the best epoch is hard to predict.

#### 1.1.1 Cyclical Learning Rates

- Instead of monotonically decreasing, **CLR** (Cyclical Learning Rate) makes LR **oscillate** between lower and upper bounds:
  - E.g., *OneCycleLR* approach used in fast.ai or certain PyTorch workflows.  
  - Can help models escape local minima or sharp minima with minimal tuning.

### 1.2 Advanced Optimizers

1. **AdaGrad** / **AdaDelta**:
   - Early attempts at parameter-wise adaptive LR.  
2. **LAMB** (Layer-wise Adaptive Moments):
   - Proposed for large-batch training on HPC or large-scale distributed environments (e.g. BERT training).  
3. **AMSGrad** (Variant of Adam):
   - Maintains a “long-term memory” of squared gradients to ensure monotonic or stable updates.

---

## 2. Advanced Regularization & Architecture Hyperparameters

### 2.1 Additional Regularization Approaches

1. **L1 (Absolute value) Regularization**:  
   - Encourages *sparsity* of weights.  
   - Typically specified as `weight_decay` if the framework supports it, or implemented manually in the loss.  
2. **Group Lasso** or **Structured Sparsity**:
   - Encourages entire filters or sets of parameters to become zero.  
3. **Label Smoothing** (for classification):
   - Instead of one-hot targets, reduce overconfident predictions.  

### 2.2 Architectural Hyperparameters

1. **Batch Normalization** parameters:  
   - Momentum for running mean/variance.  
   - Epsilon value to avoid division by zero.  
   - Placement in the network (before/after activation).  
2. **Residual Connections** (in deeper networks):
   - *Depth* vs. *width* trade-offs.  
   - Hyperparameters around how skip connections are organized.  
3. **Attention Mechanisms** (NLP / seq tasks):
   - Number of heads, dimensionality, dropout inside attention blocks.  

---

## 3. Searching High-Dimensional Spaces

### 3.1 Bayesian Optimization

- **Bayesian** approaches (e.g. **Spearmint**, **SMAC**, **Hyperopt**, **Optuna**) aim to **learn** from previous trials to propose better hyperparameter sets in subsequent trials.
- Good for **expensive** model evaluations (large-scale or HPC training).

### 3.2 Hyperband / ASHA

- Methods that combine **random** search with early **stopping** of poor configurations.  
- Efficient for extremely large hyperparameter spaces.

---

## 4. Handling Resource & Experiment Constraints

### 4.1 Distributed / HPC Environments

- For large-scale **distributed** training:
  - The **batch size** itself becomes a meta-parameter that interacts with LRs and optimizers.  
  - Large-batch training might require special LR warm-up or advanced optimizers like **LAMB**.

### 4.2 Multi-Objective Optimization

- Sometimes you want to optimize not just **accuracy** but also:
  - **Inference latency** or memory footprint.  
  - **Energy consumption** or GPU usage.  
- Balancing multiple objectives adds complexity to meta-parameter tuning.

---

## 5. Practical “PhD-Level” Tips

1. **Use Strong Baselines**:  
   - Always compare new parameter sets to a known solid baseline (e.g., Adam with typical defaults).  
2. **Record & Automate**:
   - Keep systematic logs (e.g., *TensorBoard*, *Weights & Biases*, or a spreadsheet) of each run’s hyperparameters, seeds, final results.  
   - Scripts for batch jobs on HPC, structured naming of run outputs.
3. **Iterate in Stages**:
   - Coarse (broad) search on a smaller model or subset of data to narrow plausible ranges.  
   - Fine search on the final or scaled-up model.
4. **Domain Knowledge**:
   - If your domain suggests certain capacity or certain weight constraints, incorporate that.  
   - E.g., speech tasks might benefit from smaller LR in final layers or certain architecture constraints.
5. **Don’t Over-Index on Dev**:
   - Maintain a final “**untouched**” test set or double-blind fold for your **closing** evaluation.  
   - Resist the urge to incorporate test data back into the dev cycle.

---

## 6. Example: Incorporating Advanced Techniques

A more complete script might:

```python
def create_complex_model(lr=1e-3, weight_decay=0.0, momentum=0.9, schedule=None):
    model = MyDeepResNet(...)
    # Maybe add dropout, BN
    # E.g. use Adam
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(momentum, 0.999))

    # Possibly a scheduler
    if schedule == "OneCycle":
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=EPOCHS
        )
    elif schedule == "Step":
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    else:
        scheduler = None

    return model, optimizer, scheduler
```

**Use** advanced or custom modules for these layers. Then run your **informed** or **Bayesian** search process to find good combos of `(lr, momentum, weight_decay, schedule, etc.)`.

---

## 7. Concluding Remarks

- **No universal** best set of hyperparameters.  
- Advanced models might add dozens of new meta-parameters.  
- **Methodology**:  
  1. Start from known references + defaults.  
  2. (Optionally) try small random or coarse grid searches.  
  3. Confirm on dev set.  
  4. Evaluate final on test set.  
- Over time, your “phd-level engineering” **intuition** grows, making you **faster** at picking promising parameter sets.

**Final Note**: “**All models are wrong, but some are useful**.” The same applies to hyperparameters: we seldom find an absolute optimum, but we can find a **good enough** set that yields strong results in practice.

---

## 8. References

1. **Deep Learning Book** (Goodfellow et al.) – Chapter on **Hyperparameter Optimization**  
2. **AutoML**: [**Auto-Keras**](https://autokeras.com/), [**AutoGluon**](https://auto.gluon.ai/), [**Optuna**](https://optuna.org/)  
3. **Stanford CS231n** – [hyperparameter tips](http://cs231n.github.io/neural-networks-3/#hyper)

```
Created by: [Your Name / Lab / Date]
Lecture Reference: “Meta parameters: How to pick the right metaparameters (Extended)”
```
```