
aliases: [Hyperparameter Tuning, Meta Parameter Selection, Model Parameters, Search Methods]
tags: [Deep Learning, Lecture Notes, Meta Parameters]

Choosing **hyperparameters** (or **meta parameters**)—like the number of layers, learning rate, optimizer, or regularization strength—can be one of the most **challenging** tasks in deep learning. There is **no single formula** that works for every problem. This note discusses:

1. The **scope** of possible meta parameters  
2. The risk of **overfitting** and “researcher overfitting”  
3. Strategies like **grid search**, **random search**, or **informed** search  
4. Practical tips: **Start** with known defaults from references, **fine-tune** with domain knowledge

---

## 1. Motivation

- **Deep Learning Models** have many meta parameters:  
  - **Architecture** (layers, units)  
  - **Optimizers** & their sub-parameters (e.g., momentum, betas, weight decay)  
  - **Learning Rate** & **schedules**  
  - **Regularization** (dropout rates, L2, data augmentation, etc.)  
- **Combinatorial Explosion**: Testing *every* combination is **impossible** for non-trivial setups. We must be strategic.

### 1.1 Why Not Blindly Copy from Others?

- Indeed, **existing solutions** (papers, GitHub repos) are a good **starting point**.  
- However, each dataset differs, and even “state-of-the-art” solutions might not be **optimal** for your domain.  
- Need a **balance**: borrow from references + rely on **your own experience** to adapt.

---

## 2. The Overfitting Cycle

**Key Concept**: We typically have **train**, **dev**, and **test** sets.  

1. **Train** the model on **train** set (fitting weights).  
2. Evaluate on **dev** (or validation) set to gauge performance.  
3. **Adjust** meta parameters or architecture if performance is unsatisfactory.  
4. Re-train & repeat.  
5. **Final** performance is measured on a truly **held-out test** set (untouched in the dev cycle).

> **Pitfall**: If you keep re-checking the **test** set to tweak hyperparameters, you effectively incorporate it into your training cycle → you lose a **true** measure of generalization.

### 2.1 Why This Matters

- **Researcher Overfitting**: Manually or systematically adjusting the architecture based on dev set results can lead to **overfitting** the dev set.  
- Always keep a final **unseen** test set to confirm generalization.

---

## 3. Search Space & Methods

### 3.1 Searching the Parameter Space

Imagine a **2D** parameter space (e.g., **Learning Rate** vs. **L2 Regularization**):

```
 LR
  ^
  |
  |        .  .  .  (grid or random sample)
  |     .     .     .
  |  .     .     .  
  +----------------->  L2 lambda
```

- Each point = training a model with that combination of LR & L2.  
- In reality, we can have **dozens** of hyperparameters → **High-dimensional** space → Exhaustive search is **impractical**.

### 3.2 Grid Search vs. Random / Informed Search

1. **Grid Search**:  
   - Define discrete values for each hyperparameter dimension.  
   - Evaluate **all** combinations on dev set → pick best.  
   - Works okay for low-dimensional or small ranges.

2. **Random / Informed Search**:  
   - Sample **random** points from “likely” ranges.  
   - Possibly refine distribution based on partial results (a form of **Bayesian** or **iterative** approach).  
   - More **efficient** in high dimensions; can skip “clearly bad” regions.

### 3.3 Start Simple, Focus on Key Parameters

- **Example**: If using **Adam**, we know from experience \(\beta_1=0.9,\ \beta_2=0.999\) often works well. We might only tune **lr** and maybe `weight_decay`.  
- Don’t complicate by searching across *all* possible dropout rates, L2, number of layers, etc., simultaneously.

---

## 4. Combining Prior Work, Intuition, and Searching

1. **Look at Similar Problems**:  
   - E.g., for image classification tasks in PyTorch, typical “default” meta parameters might be a known baseline.  
2. **Leverage** knowledge from:
   - This course’s examples  
   - Kaggle or other competitions  
   - Official model implementations  
3. **Refine**: Once you have a workable baseline, do small **sweeps** of parameters.

---

## 5. Example Snippet: Simple Parameter Sweeps

Below is **pseudo-code** for a minimal multi-parameter search:

```python
import itertools

# Example: a few chosen values
learning_rates = [1e-3, 1e-4]
l2_values      = [0.0, 0.01, 0.1]
num_units      = [16, 64]

best_acc = 0
best_params = None

for lr, lam, units in itertools.product(learning_rates, l2_values, num_units):
    # Configure model
    model = MyModel(num_units=units)
    
    # e.g. Adam with lr, weight_decay=lam
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lam)
    
    # Train & evaluate
    acc = train_and_eval(model, optimizer, train_loader, dev_loader)
    
    # Track best
    if acc > best_acc:
        best_acc = acc
        best_params = (lr, lam, units)

print("Best dev accuracy:", best_acc)
print("Best params:", best_params)
```

**Caveat**: For large parameter spaces, we might do partial or random sampling to reduce training overhead.

---

## 6. Remarks & Practical Tips

1. **Impossible** to guarantee the **true** optimal solution.  
2. **Experience** speeds up picking plausible ranges.  
3. **Smaller** tasks/datasets → faster iteration cycles to test more combos.  
4. For big tasks, consider advanced or iterative hyperparameter tuning tools (e.g. **Optuna**, **Hyperopt**, **Ray Tune**, or **Bayesian Optimization**).

### 6.1 Balancing Laziness & Diligence

- **Laziness**: Simply copy a known architecture or meta param set from GitHub.  
- **Diligence**: Exhaustive or large-scale search.  
- Realistic approach: Start with known defaults + a small local search → refine if needed.

---

## 7. Conclusion

- **Choosing meta parameters** is an **art**. No universal recipe.  
- **Grid search** is feasible for a **small** subset of parameters, but random or **informed** search is often more practical in high dimensions.  
- **Never** finalize meta parameters based on **test** set – keep a **dev** set for model tuning to avoid hidden overfitting.  
- Over time, your **intuition** improves from repeated practice, so you can quickly guess which hyperparameters deserve the most attention.

**Key Lesson**: Real-world deep learning typically finds a **good** solution rather than the **perfect** one. Embrace iterative tuning, small experiments, and knowledge from both references and your own experience.

---

## 8. References

- [**Deep Learning Book** – Goodfellow, Bengio, Courville: Chapter on **Hyperparameter Optimization**.](https://www.deeplearningbook.org/)  
- **Stanford CS231n** – [Notes on hyperparameter optimization](http://cs231n.github.io/neural-networks-3/#hyper)  
- **Optuna** / **Ray Tune** / **Hyperopt** – advanced frameworks for searching large parameter spaces

```
Created by: [Your Name / Lab / Date]
Lecture Reference: “Meta parameters: How to pick the right metaparameters”
```
```