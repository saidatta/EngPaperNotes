## 1. Overview and Motivation

1. **Autoencoder Architecture (Recap)**  
   - Input layer: \(784\) units (flattened \(28 \times 28\) MNIST image).  
   - Encoder layers: Compress from \(784\) to a smaller dimension.  
   - **Latent layer (bottleneck)**: Smallest dimension, e.g., \(15\) units.  
   - Decoder layers: Expand back to \(784\) units for output.

2. **Goal**  
   - Examine the **representations** that the autoencoder learns in its **innermost latent layer**.  
   - Compare how these representations differ from the original data (784D) and how they cluster or spread out among digits 0–9.

3. **Interpretability Challenge**  
   - Although the network is made of simple building blocks (linear layers, ReLUs, etc.), the resulting internal representation can be difficult to interpret.  
   - Different training runs can yield different latent codes for the same digit due to **random initialization**.

---

## 2. PyTorch Implementation

### 2.1 Libraries & Data

```yaml
```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Hypothetical loading of MNIST (or a subset):
# X_mnist of shape (N, 28, 28), y_mnist of shape (N,)
N = 20000  # number of samples
X = X_mnist[:N].reshape(N, 784).astype(np.float32) / 255.0
y = y_mnist[:N]  # digit labels 0-9
X_tensor = torch.tensor(X)
y_np = np.array(y)  # for indexing
```
```

**Key Points**:
- We flatten each \(28 \times 28\) image into \(784\) features.  
- Data normalized to \([0, 1]\).  
- \(\mathbf{X}_{\text{tensor}}\) is our input to PyTorch models, \(\mathbf{y}_{\text{np}}\) are the labels (for analysis, not used in autoencoder training).

### 2.2 Defining an Autoencoder That Returns Latent Codes

```yaml
```python
class AENet(nn.Module):
    def __init__(self):
        super(AENet, self).__init__()
        # Encoder
        self.enc1 = nn.Linear(784, 128)
        self.enc2 = nn.Linear(128, 15)  # 15D latent space
        
        # Decoder
        self.dec1 = nn.Linear(15, 128)
        self.dec2 = nn.Linear(128, 784)
        
        # Activation
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # x shape: (batch_size, 784)
        # Encoder
        x_enc = self.relu(self.enc1(x))
        code_x = self.relu(self.enc2(x_enc))  # Bx15
        
        # Decoder
        x_dec = self.relu(self.dec1(code_x))
        x_out = self.sigmoid(self.dec2(x_dec))  # Bx784
        
        # Return both final output AND the latent code
        return x_out, code_x

def create_model():
    return AENet()
```
```

**Explanation**:
- `forward` returns a **tuple**: `(reconstructed, latent_code)`.  
- This is a **key modification** from typical autoencoders that only return the reconstruction.

---

## 3. Training the Autoencoder

### 3.1 Simple Training Loop (Random Mini-Batches)

```yaml
```python
def train_autoencoder(X, epochs=10, batch_size=32, lr=0.001):
    model = create_model()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    
    N = X.shape[0]
    
    for ep in range(epochs):
        epoch_loss = 0.0
        for i in range(100):  # for demonstration, 100 random batches per epoch
            idx = np.random.choice(N, batch_size, replace=False)
            x_batch = X[idx]
            
            # forward
            x_out, _ = model(x_batch)
            loss = criterion(x_out, x_batch)
            
            # backprop
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        print(f"Epoch {ep+1}/{epochs}, Avg Loss={epoch_loss/100:.4f}")
    
    return model

# Example training
model = train_autoencoder(X_tensor, epochs=10, batch_size=32, lr=0.001)
```
```

**Note**:  
- This training code picks **random mini-batches** for each iteration (not covering all data systematically).  
- We do it for demonstration speed. In practice, use a full DataLoader approach for better coverage.

---

## 4. Extracting the Latent Representations

### 4.1 Forward Pass on Entire Dataset

```yaml
```python
# Pass all images through the trained model
with torch.no_grad():
    x_out_full, x_code_full = model(X_tensor)  # shapes: (N, 784), (N, 15)

print("Reconstructed shape:", x_out_full.shape)  # (20000, 784)
print("Latent code shape:", x_code_full.shape)   # (20000, 15)
```
```

- `x_out_full`: the final autoencoder outputs for each image (shape \((20000, 784)\)).  
- `x_code_full`: the latent vectors in 15D for each image (shape \((20000, 15)\)).

### 4.2 Inspect Distribution of Latent Units

```yaml
```python
# Convert to NumPy for plotting
x_code_np = x_code_full.numpy()

plt.figure(figsize=(7,4))
plt.hist(x_code_np.flatten(), bins=50, color='blue', alpha=0.7)
plt.title("Histogram of All Latent Unit Activations (Flattened)")
plt.xlabel("Latent activation value")
plt.ylabel("Frequency")
plt.show()
```
```

- Expect many near-zero values if the autoencoder learns a **sparse** representation or if certain latent dimensions are underutilized.

---

## 5. Analyzing Latent Codes per Digit

### 5.1 Mean Latent Activation per Digit

```yaml
```python
digits = np.arange(10)
mean_codes = []

for d in digits:
    idx_d = np.where(y_np == d)[0]
    mean_latent = x_code_np[idx_d].mean(axis=0)
    mean_codes.append(mean_latent)

mean_codes = np.array(mean_codes)  # shape (10, 15)

# Plot
plt.figure(figsize=(10,4))
for d in digits:
    plt.plot(mean_codes[d], marker='o', label=f'Digit {d}')
plt.legend()
plt.title("Average Latent Activation per Digit")
plt.xlabel("Latent Dimension Index")
plt.ylabel("Mean Activation")
plt.show()
```
```

- Observe if certain latent dimensions **light up** for specific digits.  
- Potentially see if some dimensions are consistently near zero for all digits (unused “dead” units).

---

## 6. PCA Analysis for Visualization

### 6.1 PCA on the 15D Latent Space

```yaml
```python
pca_code = PCA(n_components=15)
pca_code.fit(x_code_np)
x_code_pca = pca_code.transform(x_code_np)  # shape: (N, 15)

# Plot scree (variance explained)
var_explained_code = pca_code.explained_variance_ratio_
plt.figure(figsize=(6,4))
plt.plot(var_explained_code*100, 'o-')
plt.xlabel("Principal Component")
plt.ylabel("Variance Explained (%)")
plt.title("PCA Scree Plot of the Latent Code")
plt.grid(True)
plt.show()
```
```

**Interpretation**:
- A steep drop in the first few components suggests the autoencoder learned a **compact** representation.  
- If some latent units are effectively zero, we might see zero variance in the final components.

### 6.2 PCA on the Original Data (Optional Comparison)

```yaml
```python
pca_data = PCA(n_components=15)
pca_data.fit(X)
var_explained_data = pca_data.explained_variance_ratio_

plt.figure(figsize=(6,4))
plt.plot(var_explained_data*100, 'o-r')
plt.xlabel("Principal Component")
plt.ylabel("Variance Explained (%)")
plt.title("PCA Scree Plot of Original Data")
plt.grid(True)
plt.show()
```
```

- Comparing the scree plots, **latent code** may show higher variance explained in the first few components, indicating a more “efficient” or “tightly organized” representation of digit variability.

### 6.3 2D Projection of Codes with Color-coded Labels

```yaml
```python
# Take just the first 2 PCA components
x_code_pca_2D = x_code_pca[:, :2]

plt.figure(figsize=(7,6))
scatter = plt.scatter(x_code_pca_2D[:,0], x_code_pca_2D[:,1], 
                      c=y_np, cmap='tab10', alpha=0.5, s=10)
plt.colorbar(scatter, ticks=range(10), label='Digit')
plt.title("Latent Codes Projected to PCA(2D)")
plt.xlabel("PC 1")
plt.ylabel("PC 2")
plt.show()
```
```

- Each point = one MNIST image. Color = digit label.  
- Some digits may cluster distinctly; others might overlap, reflecting the autoencoder’s partial or complete separation in 15D space.

---

## 7. Observations & Interpretability

1. **Sparse or Clustered Representation**:  
   - Some latent dimensions may remain near zero for most digits. Others might be specifically active for certain digit subsets.
2. **Efficiency**:  
   - The autoencoder’s 15D bottleneck compresses the original 784D space.  
   - Large eigenvalues in the first few PCA components of the latent code indicate it captures digit variability in fewer dimensions.
3. **Overlapping Clusters**:  
   - In 2D PCA plots, we might still see overlap because we’re only viewing 2 out of 15 dimensions.  
   - In higher-dimensional latent space, digits can be more separable.
4. **Randomness**:  
   - Each run can yield a different latent “basis” or different usage of each dimension.  
   - There is **no guarantee** that dimension #3 always corresponds to the same concept across runs.

---

## 8. Additional Explorations

1. **Per-Digit Variance**:  
   - Instead of plotting average latent activation, also plot **standard deviation** across each latent unit for each digit.
2. **Class-Conditioned Reconstructions**:  
   - Compare reconstructions of each digit. Are certain digits systematically better or worse?
3. **Alternative Loss Functions**:  
   - Try `BCEWithLogitsLoss` for images in \([0,1]\) vs. MSE. Does it change the distribution of latent codes?
4. **Convolutional Autoencoders**:  
   - Replace linear layers with conv/deconv to handle the 2D structure natively.
5. **Beyond 2D PCA**:  
   - Use t-SNE or UMAP to visualize potential non-linear embeddings of the latent codes in 2D.

---

## 9. Philosophical Note on Interpretability

1. **Internal Representations**: 
   - Despite the **simple** mathematical structure (linear + ReLU, etc.), the emergent patterns in the latent space can appear **opaque** or unintuitive.
2. **Dependence on Random Initialization**:  
   - Different seeds → Different local minima → Different latent dimension usage.
3. **Practical Stance**:  
   - We often treat autoencoders as **useful black boxes** for tasks like denoising, dimension reduction, etc., without deeply understanding every neuron’s “meaning.”
4. **Takeaway**: 
   - “Interpretation” might be partial: we can measure how well the code separates data or how dimension usage correlates with classes, but a full conceptual breakdown is elusive.

---

## 10. Summary

- **Core Concept**: By modifying the **forward** method to return both final output and latent code, we can **inspect** how the network encodes MNIST digits in a reduced-dimensional space.  
- **Findings**:  
  1. Autoencoders often learn a **sparse, efficient** representation (some latent dimensions remain near zero).  
  2. PCA on the latent code can reveal how the autoencoder organizes digit variability more compactly than raw data.  
  3. Full interpretability remains challenging—though we can measure correlations, visualize clusters, and note usage patterns, the underlying internal “concepts” are not straightforward to decode.

**End of Notes**.