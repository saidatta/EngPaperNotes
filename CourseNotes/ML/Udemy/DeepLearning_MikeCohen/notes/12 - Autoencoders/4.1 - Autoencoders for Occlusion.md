## 1. Motivation and Context

### 1.1 Visual Occlusion

In visual scenes, **occlusion** occurs when one object partially blocks the view of another. For example, a finger in front of a pen hides part of the pen; yet humans can effortlessly infer the pen is a single continuous object. The question is whether a **deep autoencoder** can similarly reconstruct or "remove" occlusions in images.

### 1.2 Simplified Problem Setup

- **Dataset**: MNIST digits (or EMNIST) with shape \(28 \times 28\).
- **Occlusion Simulation**:
  - Artificially add horizontal or vertical white bars into the digit images. 
  - The bar acts like an "occluder."
- **Autoencoder Goal**: Train on normal (unoccluded) images, then input occluded images and see if the autoencoder "removes" or reconstructs the occluded content.

---

## 2. Approach

### 2.1 Model Architecture

We use a **feedforward autoencoder**:
1. **Encoder**:
   - Input layer: 784 units (flattened 28×28 image).
   - Hidden layer: 128 units (ReLU).
   - Bottleneck: 50 units (ReLU).
2. **Decoder**:
   - Hidden layer: 128 units (ReLU).
   - Output layer: 784 units (Sigmoid).

**Loss**: Mean Squared Error (MSE) comparing reconstructed image \(\hat{\mathbf{x}}\) to original \(\mathbf{x}\).

### 2.2 Training

- **Training data**: 20,000 MNIST images.  
- **Goal**: Learn to reconstruct **original, clean** images; no occluded images are seen during training.
- **Epochs**: 5 full passes over 20k images.
- **Mini-batch**: 32 or 64 (depending on preference).
- **Optimizer**: Adam with a typical learning rate (e.g., 0.001).

### 2.3 Testing with Occlusion

After training, we:
1. Take **test images** (or a subset from the same dataset).
2. Randomly place a **white bar** horizontally or vertically:
   - Horizontal: pick random row indices, set pixel values to 1 in those rows.
   - Vertical: pick random column indices, set pixel values to 1 in those columns.
3. Feed the occluded images to the trained autoencoder.
4. Observe the **output**: see if the bar disappears and if the digit remains recognizable.

---

## 3. Code Implementation

Below is a representative PyTorch notebook-style code. Adapt paths and variable names as needed.

```yaml
```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# 1) Load MNIST data subset (e.g., 20,000 samples)
# Suppose X_mnist is loaded as a NumPy array of shape [N, 28, 28].
# For demonstration, we assume you already have it in memory.

N = 20000  # number of samples
X = X_mnist[:N].reshape(N, 784).astype(np.float32) / 255.0
X_tensor = torch.tensor(X)

# 2) Define Autoencoder architecture
class AENet(nn.Module):
    def __init__(self):
        super(AENet, self).__init__()
        self.enc1 = nn.Linear(784, 128)
        self.enc2 = nn.Linear(128, 50)  # bottleneck
        
        self.dec1 = nn.Linear(50, 128)
        self.dec2 = nn.Linear(128, 784)
        
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # encoder
        x = self.relu(self.enc1(x))
        x = self.relu(self.enc2(x))
        
        # decoder
        x = self.relu(self.dec1(x))
        x = self.sigmoid(self.dec2(x))
        return x

def create_model():
    return AENet()

# 3) Training function
def train_autoencoder(X, epochs=5, batch_size=32, lr=0.001):
    model = create_model()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    
    N = X.shape[0]
    n_batches = N // batch_size
    
    for ep in range(epochs):
        idx_perm = np.random.permutation(N)  # shuffle indices
        for b_i in range(n_batches):
            start = b_i * batch_size
            end   = start + batch_size
            batch_idx = idx_perm[start:end]
            
            x_batch = X[batch_idx]
            
            # forward
            x_hat = model(x_batch)
            loss = criterion(x_hat, x_batch)
            
            # backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # Print epoch status
        print(f"Epoch {ep+1}/{epochs}, Loss={loss.item():.4f}")
    
    return model

# 4) Train the model
model = train_autoencoder(X_tensor, epochs=5, batch_size=32, lr=0.001)
```
```

**Training Notes**:
- We only train on **unoccluded** MNIST. The autoencoder learns a compressed representation that effectively captures digit patterns but never sees occluders during training.

---

### 3.1 Testing with Occlusion

```yaml
```python
# Let's pick a small set of test images to occlude
n_test_images = 10
test_imgs = X_tensor[:n_test_images].clone()

# Apply random occlusion
occluded_imgs = []
for i in range(n_test_images):
    img = test_imgs[i].view(28, 28).clone()
    # Randomly decide horizontal or vertical
    if np.random.rand() < 0.5:
        # horizontal bar
        row_start = np.random.randint(8, 20)  # random row
        # set several rows to 1
        img[row_start:row_start+3, :] = 1.0
    else:
        # vertical bar
        col_start = np.random.randint(8, 20)  # random column
        # set several columns to 1
        img[:, col_start:col_start+3] = 1.0
    
    occluded_imgs.append(img.view(-1))

occluded_imgs = torch.stack(occluded_imgs)  # shape: (10, 784)

# Pass occluded images through the trained autoencoder
with torch.no_grad():
    reconstructed = model(occluded_imgs)
```
```

---

## 4. Visualizing Results

### 4.1 Comparing Original, Occluded, and Reconstructed

```yaml
```python
fig, axs = plt.subplots(3, n_test_images, figsize=(2*n_test_images, 6))

for i in range(n_test_images):
    # Original (top row)
    axs[0, i].imshow(test_imgs[i].view(28,28), cmap='gray')
    axs[0, i].set_title("Original")
    axs[0, i].axis('off')
    
    # Occluded (middle row)
    axs[1, i].imshow(occluded_imgs[i].view(28,28), cmap='gray')
    axs[1, i].set_title("Occluded")
    axs[1, i].axis('off')
    
    # Reconstructed (bottom row)
    axs[2, i].imshow(reconstructed[i].view(28,28), cmap='gray')
    axs[2, i].set_title("Reconstructed")
    axs[2, i].axis('off')

plt.tight_layout()
plt.show()
```
```

**Expected Outcome**:
- The white bars **disappear** in many cases.
- The final digit is often intact but might be “smudged” or slightly distorted.

---

### 4.2 Quantitative Analysis: Correlation

We can measure how closely the autoencoder output \(\hat{\mathbf{x}}\) matches the original \(\mathbf{x}\) via **correlation**:

1. **All Pixels** (including zeros).
2. **Non-zero Pixels Only** to avoid trivial correlations from background zeros.

```yaml
```python
import scipy.stats as st

img_idx = 0  # example index
original_img = test_imgs[img_idx].numpy()
occluded_img = occluded_imgs[img_idx].numpy()
recon_img    = reconstructed[img_idx].numpy()

# 1) Full correlation
corr_full = np.corrcoef(original_img, recon_img)[0, 1]

# 2) Non-zero correlation
tolerance = 1e-5
non_zero_mask = (original_img > tolerance) & (recon_img > tolerance)
corr_non_zero = np.corrcoef(original_img[non_zero_mask], 
                            recon_img[non_zero_mask])[0, 1] if non_zero_mask.sum() > 1 else 0

print(f"Full correlation: {corr_full:.3f}")
print(f"Non-zero correlation: {corr_non_zero:.3f}")
```
```

**Interpretation**:
- If the autoencoder reconstructs digits (and ignores occluders), the correlation should be relatively high.
- However, even **perfect** ignoring of the occluder does not guarantee a perfect reconstruction, since autoencoders typically produce slightly blurred outputs.

---

## 5. Observations & Discussion

1. **Occluder Removal**:  
   - The network “filters out” the bar because it did not learn any representation for horizontal or vertical bars floating in digits. It only learned digit features in the bottleneck.
2. **Smeared Artifacts**:  
   - Some reconstructions show faint lines or weird distortions (e.g., a 7 might look like a 9).
   - This is partially due to limited model capacity and MSE smoothing.
3. **Not a Perfect Fix**:  
   - While the bar is gone, the digits might be slightly distorted, indicating that the autoencoder’s latent representation is not exactly capturing all fine details.
4. **Comparison**:
   - The correlation with the original image is typically **lower** for occluded inputs than for clean inputs, but still surprisingly high given the bar was fully removed in many places.
5. **Biological Inspiration**:
   - Humans solve occlusion effortlessly; deep autoencoders can do a rudimentary version with straightforward training, showing promise for more advanced computer vision tasks.

---

## 6. Extensions & Additional Explorations

1. **Change the Loss Function**:  
   - Try `nn.BCELoss()` or cross-entropy if you scale images \([0,1]\).
   - Compare reconstruction quality vs. MSE.
2. **Systematic Bias**:  
   - Check if certain digits are more easily “de-occluded” than others. Do vertical occlusions cause more distortion than horizontal ones?
3. **Larger or Convolutional Autoencoders**:  
   - Replace linear layers with convolutional and deconvolutional layers.
   - Typically yields better spatial reconstruction for occlusion tasks.

---

## 7. Summary

- **Key Takeaway**: A simple feedforward autoencoder, trained only on **clean** MNIST images, can **remove artificial occluding bars** when tested on occluded images.  
- **Mechanism**: The bottleneck representation captures digit structure but not isolated lines, effectively filtering out non-digit occluders.  
- **Limitations**: 
  - Some distortion in the reconstructed digit remains, showing the network doesn’t perfectly reconstruct original images.
- **Future Directions**: Larger networks, **convolutional** architectures, or **attention-based** models can better address complex occlusion in real-world images.

**End of Notes**.