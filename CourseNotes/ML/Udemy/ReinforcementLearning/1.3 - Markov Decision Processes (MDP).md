This note provides a comprehensive and detailed exploration of **Markov Decision Processes (MDPs)**, a foundational concept in reinforcement learning and decision theory. MDPs are used to model environments where outcomes are partly random and partly under the control of an agent. This note builds on your existing knowledge of the **Bellman Equation** and **Reinforcement Learning** to cover MDPs in-depth, including code examples, equations, and real-world applications.

---
## Overview of Markov Decision Processes (MDPs)

### What is a Markov Decision Process (MDP)?
A **Markov Decision Process (MDP)** provides a mathematical framework for modeling decision-making problems where outcomes are partly random and partly under the agent's control. The process has the **Markov property**, meaning that the future state of the system depends only on the current state and the action taken, not on the sequence of events that preceded it.
### Components of an MDP
An MDP is defined by the tuple \( (S, A, P, R, \gamma) \):
1. **State space (S)**: A set of states \( S \) in which the agent can be.
2. **Action space (A)**: A set of actions \( A \) the agent can take in any state.
3. **Transition probabilities (P)**: The probability \( P(s' | s, a) \) of transitioning from state \( s \) to state \( s' \) after taking action \( a \).
4. **Reward function (R)**: The reward \( R(s, a) \) the agent receives for taking action \( a \) in state \( s \).
5. **Discount factor (γ)**: A factor \( \gamma \in [0, 1] \) that determines the importance of future rewards.
### The Markov Property

The **Markov property** implies that the probability distribution of the next state depends only on the current state and the action taken, not on the past sequence of events. Mathematically, this can be expressed as:

\$[$
P(s' | s, a, s_{t-1}, a_{t-1}, \dots) = P(s' | s, a)
\$]

---

## Deterministic vs. Non-Deterministic Environments
![[Screenshot 2024-10-12 at 10.44.02 AM.png]]
### Deterministic Environments
In a **deterministic environment**, the outcome of an action is always the same. If the agent takes an action \( a \) in state \( s \), the result will always be the same state transition and reward.
For example, in a maze:
- Action: Move up
- Result: The agent moves up with 100% probability.
### Non-Deterministic (Stochastic) Environments
In a **non-deterministic (stochastic)** environment, the outcome of an action is probabilistic. For example:
- Action: Move up
- Result: There is an 80% chance the agent moves up, a 10% chance it moves left, and a 10% chance it moves right.

This introduces randomness into the environment, making the decision process more complex.

---
## Markov Processes and Markov Decision Processes

### Markov Process

A **Markov Process** is a type of stochastic process where the outcome only depends on the current state and not the history of previous states. It is defined as a tuple \( (S, P) \), where:
- \( S \): A set of states.
- \( P \): A state transition probability matrix, where each element \( P(s' | s) \) represents the probability of transitioning from state \( s \) to state \( s' \).
### Markov Decision Process (MDP)
An **MDP** is an extension of a Markov Process with actions. The agent can choose actions that influence state transitions. It is defined as \( (S, A, P, R, \gamma) \).

- **Actions (A)**: The set of actions the agent can take.
- **Rewards (R)**: The agent receives rewards based on its state and the action it takes.
- **Policy (π)**: The strategy that the agent uses to choose actions, defined as \( \pi(a | s) \), the probability of taking action \( a \) in state \( s \).

---

## The Bellman Equation in MDPs

The **Bellman Equation** lies at the heart of MDPs. It provides a recursive decomposition of the value function, which represents the expected cumulative reward from any given state.

### Bellman Expectation Equation

For a given **policy** \( \pi \), the value function \( V^\pi(s) \) can be expressed using the **Bellman Expectation Equation**:

\[
V^\pi(s) = \sum_{a \in A} \pi(a | s) \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V^\pi(s') \right]
\]

Where:
- \( \pi(a | s) \): The policy’s probability of taking action \( a \) in state \( s \).
- \( R(s, a) \): The reward received after taking action \( a \) in state \( s \).
- \( P(s' | s, a) \): The probability of transitioning to state \( s' \) after taking action \( a \) in state \( s \).

### Bellman Optimality Equation

The **Bellman Optimality Equation** defines the optimal value function \( V^*(s) \) by taking the maximum over all possible actions:

\[
V^*(s) = \max_{a \in A} \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V^*(s') \right]
\]

The optimal policy \( \pi^*(s) \) is the one that maximizes the expected cumulative reward.

---

## Value Iteration for Solving MDPs

One popular method to solve MDPs is **Value Iteration**, which iteratively applies the Bellman Optimality Equation to estimate the value function until it converges.

### Value Iteration Algorithm

1. **Initialize**: Set \( V(s) = 0 \) for all states \( s \in S \).
2. **Update**: For each state \( s \in S \), update \( V(s) \) using the Bellman equation:
   
   \[
   V(s) = \max_{a \in A} \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V(s') \right]
   \]

3. **Repeat**: Continue until the value function converges (i.e., the difference between successive iterations is below a threshold).

### Python Code Example: Value Iteration
```python
import numpy as np

# Define states, actions, rewards, transition probabilities
states = [0, 1, 2, 3]
actions = [0, 1]  # 0: left, 1: right
rewards = np.array([
    [-1, 0],   # Rewards for state 0
    [0, 1],    # Rewards for state 1
    [1, -1],   # Rewards for state 2
    [-1, 0]    # Rewards for state 3
])

# Transition probabilities for state-action pairs
transition_probabilities = np.array([
    [[0.8, 0.2], [0.2, 0.8]],  # Transitions from state 0
    [[0.7, 0.3], [0.3, 0.7]],  # Transitions from state 1
    [[0.6, 0.4], [0.4, 0.6]],  # Transitions from state 2
    [[0.5, 0.5], [0.5, 0.5]]   # Transitions from state 3
])

gamma = 0.9  # Discount factor
threshold = 1e-4  # Convergence threshold

def value_iteration(states, actions, rewards, transitions, gamma, threshold):
    V = np.zeros(len(states))
    
    while True:
        delta = 0
        for s in states:
            v = V[s]
            # Compute the value of each action
            V[s] = max([
                sum([transitions[s, a, s_next] * (rewards[s, a] + gamma * V[s_next])
                     for s_next in states])
                for a in actions
            ])
            delta = max(delta, abs(v - V[s]))
        
        if delta < threshold:
            break
    
    return V

optimal_values = value_iteration(states, actions, rewards, transition_probabilities, gamma, threshold)
print("Optimal Value Function:", optimal_values)
```

### Explanation:
- **State Space (S)**: The agent can be in one of four states.
- **Action Space (A)**: The agent can move left (0) or right (1).
- **Transition Probabilities (P)**: The environment is stochastic, meaning that the agent may end up in unintended states with certain probabilities.
- **Value Iteration**: The value of each state is updated by taking the maximum expected reward over all possible actions until convergence.

---

## Policy Iteration

An alternative to **Value Iteration** is **Policy Iteration**, which iteratively evaluates and improves the policy until convergence.

### Policy Iteration Steps:
1. **Policy Evaluation**: Compute the value function

 for a given policy \( \pi \).
   
   \[
   V^\pi(s) = \sum_{a \in A} \pi(a | s) \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V^\pi(s') \right]
   \]
   
2. **Policy Improvement**: Update the policy by choosing the action that maximizes the expected value.
   
   \[
   \pi'(s) = \arg\max_a \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V(s') \right]
   \]
   
3. **Repeat**: Continue alternating between policy evaluation and policy improvement until the policy converges to the optimal policy.

---

## ASCII Visualization: Agent in a Stochastic Maze

Let’s visualize the environment with an ASCII maze where the agent faces uncertainty in the outcome of its actions:

```
+---+---+---+
| S |   | E |
+---+---+---+
| F |   |   |
+---+---+---+
```

- \( S \): Start state.
- \( E \): Goal state with reward +1.
- \( F \): Fire pit with reward -1.

The agent may attempt to move up, but due to stochasticity, it could end up moving left or right with some probability. The optimal policy must take this uncertainty into account.

---

## Real-World Applications of MDPs

1. **Robotics**: MDPs are used to model decision-making for autonomous robots operating in uncertain environments.
   
2. **Finance**: MDPs are applied to portfolio management, where investment decisions are influenced by stochastic market behavior.
   
3. **Healthcare**: MDPs can help optimize treatment strategies in clinical decision-making where the effects of treatments are probabilistic.

4. **Gaming**: MDPs are used in game AI to model decision-making under uncertainty, such as in strategy games or real-time simulations.

---

## Conclusion

**Markov Decision Processes (MDPs)** provide a powerful framework for modeling decision-making in environments with stochasticity and uncertainty. By leveraging the **Bellman Equation** and value-based methods like **Value Iteration** and **Policy Iteration**, MDPs enable agents to make optimal decisions even in complex, uncertain environments.

The understanding of MDPs is critical for solving real-world reinforcement learning problems, as they form the mathematical foundation for many advanced algorithms such as **Q-Learning** and **Deep Reinforcement Learning**.

---

By mastering MDPs, you will be equipped to apply reinforcement learning techniques to a wide range of practical problems, from robotics to finance and beyond.