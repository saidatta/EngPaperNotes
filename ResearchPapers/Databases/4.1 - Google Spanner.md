https://hemantkgupta.medium.com/insights-from-paper-spanner-googles-globally-distributed-database-2da23c245722

Tags: #google #spanner #paper #distributed-systems #database

---
**Table of Contents**  
1. [[#Introduction|Introduction]]  
2. [[#High-Level Features|High-Level Features]]  
3. [[#Architecture-and-Implementation|Architecture and Implementation]]  
   1. [[#Spanner-Server-Organization|Spanner Server Organization]]  
   2. [[#Rationale-of-Implementation|Rationale of Implementation]]  
   3. [[#Directory-Abstraction|Directory Abstraction]]  
   4. [[#Data-Model|Data Model]]  
4. [[#TrueTime-API|TrueTime API]]  
5. [[#Concurrency-Control|Concurrency Control]]  
   1. [[#Timestamp-Management|Timestamp Management]]  
   2. [[#Serving-Reads-at-a-Timestamp|Serving Reads at a Timestamp]]  
   3. [[#Assigning-Timestamps-to-RO-Transactions|Assigning Timestamps to Read-Only Transactions]]  
   4. [[#Details-of-RW-vs-RO-Transactions|Details of RW vs. RO Transactions]]  
6. [[#Schema-Change-Transactions|Schema-Change Transactions]]  
7. [[#Related-Work|Related Work]]  
8. [[#Future-Work|Future Work]]  
9. [[#Conclusions|Conclusions]]  
---
## Introduction
- **Spanner** is Google’s scalable, multi-version, globally distributed, and synchronously-replicated database.
- It is the first system to **distribute data globally** and support **externally-consistent** (a.k.a. linearizable) **distributed transactions**.
- Spanner introduces the **TrueTime API**, which exposes **bounded clock uncertainty** to the database layer to achieve strict external consistency.
- At a high level:
  - Data is **sharded** and **replicated** across data centers worldwide.
  - Clients can **failover** between replicas automatically.
  - The system automatically **migrates data** across machines or data centers to balance load or in response to failures.
  - Designed to scale to **millions of machines** in **hundreds of data centers** and handle **trillions of rows**.
### Key Milestones
- **Initial customer**: F1, Google’s advertising backend (rewritten from MySQL).
- **Primary focus**: Managing cross-datacenter replicated data.
- Evolved from a **Bigtable-like** versioned key-value store into a **temporal multi-version database**.

---
## High-Level Features
1. **Dynamic Replication Configurations**  
   - Fine-grained control over replication policies (number of replicas, geographic placement, etc.).
   - Applications can specify constraints on which data centers store which data.
2. **Externally Consistent Reads and Writes**  
   - Strong consistency and linearizability across the global database.
3. **Globally Consistent Reads at a Timestamp**  
   - A snapshot of the entire database can be read at a specific timestamp.
4. **Atomic Schema Updates**  
   - Schema changes are globally atomic and can occur even in ongoing transactions.
5. **Globally Meaningful Commit Timestamps**  
   - Spanner uses a novel TrueTime API to assign monotonic timestamps that reflect serialization order, ensuring external consistency.
---
## Architecture and Implementation
### Spanner Deployment Terminology
- A **Spanner deployment** is called a **universe**.
- It is organized as a set of **zones**.
  - **Zone**: The unit of administrative deployment and physical isolation.
  - Data replication can spread across multiple zones.
#### ASCII Diagram: Spanner Universe
```
 ┌─────────────────────────────────────────┐
 │              Spanner Universe          │
 │ (consists of multiple zones worldwide) │
 └─────────────────────────────────────────┘
                  /   |   \
                 /    |    \
        ┌─────────────┴───────┬────────────┐
        │   Zone #1            │   Zone #2  │  ...
        │ (Data Center A)      │ (DC B)     │
        └───────────────────────┴────────────┘
```

---
### Spanner Server Organization
- Each zone has:
  1. **Zonemaster**  
     - Assigns data (tablets) to spanservers.
  2. **Spanservers** (hundreds to thousands)  
     - Each spanserver manages 100–1000 tablets.
  3. **Location proxies**  
     - Help clients find the correct spanserver for data lookups.
- **Universe master**: A console to display status information for the entire deployment.
- **Placement driver**: Orchestrates automatic data movement across zones.

#### ASCII Diagram: Zone Components

```
 ┌──────────────────────── Zone ──────────────────────────┐
 │                                                        │
 │  ┌───────────────┐        ┌───────────────────────┐   │
 │  │ Zonemaster    │        │  Location Proxies     │   │
 │  │ (Assigns      │        │ (Client request       │   │
 │  │  tablets)     │        │  routing)             │   │
 │  └───────────────┘        └───────────────────────┘   │
 │                ┌───────────────────────────────┐       │
 │                │  Spanservers (100s - 1000s)   │       │
 │                │  Each manages ~100-1000       │       │
 │                │  tablets.                     │       │
 │                └───────────────────────────────┘       │
 └─────────────────────────────────────────────────────────┘
```

---
### Rationale of Implementation
Spanner’s implementation is **layered on top of a Bigtable-like** architecture, but with:
1. **Tablets**  
   - Each spanserver manages multiple **tablets**.  
   - A tablet in Spanner is akin to Bigtable’s tablet, but here the system, not the user, assigns timestamps to data.  
   - The data store for each tablet is in a **key-timestamp → value** format:
     ```
     (key:string, timestamp:int64) -> string
     ```
   - Data is persisted in B-Tree-like files + a write-ahead log stored on **Colossus** (Google’s successor to GFS).
2. **Paxos State Machine**  
   - Each tablet is replicated via a Paxos group.
   - The leader replica handles writes (Paxos) and concurrency control (lock table).
   - Reads can be served by any replica that is “sufficiently up-to-date.”
3. **Transaction Manager**  
   - Each leader replica implements a transaction manager for distributed transactions across multiple Paxos groups (via 2PC).
4. **Long-Lived Transactions**  
   - Concurrency control uses lock tables at the leader.
   - Operations that need synchronization acquire locks, while others can bypass.
---
### Directory Abstraction
- **Directory**: A bucketing abstraction on top of the key-value space.
- **Unit of data movement**: Entire directories (rather than individual rows) are moved from one Paxos group to another to balance load or handle failures.
- **Unit of replication**: Each directory has a replication configuration specifying how many replicas and where they reside geographically.
#### ASCII Diagram: Directory Movement

```
                      Paxos Group A
                     ┌─────────────┐
                     │   Tablet    │
                     │ (Directory) │
                     └─────────────┘
                            |
       [ MoveDir operation ]|        (Background process)
                            v
                      Paxos Group B
                     ┌─────────────┐
                     │   Tablet    │
                     │ (Directory) │
                     └─────────────┘
```
- **Movedir**: A background task used to:
  1. Move directories between Paxos groups.
  2. Add or remove replicas from a Paxos group.
- **Placement**:  
  - The directory is also the smallest unit with its own replication properties.
  - Administrators can specify the **number/type of replicas** and **geographic location** of those replicas.
---
### Data Model
- **Semi-Relational**: Spanner’s data model is built upon schematized tables with rows, columns, and versioned values.
- **SQL-like Query Language**: Supports an SQL-like syntax for queries.
- **General-Purpose Transactions**:  
  > *“We believe it is better to have application programmers deal with performance problems ... rather than always coding around the lack of transactions.”*
- **Hierarchical Table Layout** via the `INTERLEAVE IN` directive:
  - Interleaved tables share the primary key prefix and are considered part of the same directory.
#### Example Schema
```sql
CREATE TABLE Users {
  uid INT64 NOT NULL,
  email STRING
} PRIMARY KEY (uid), DIRECTORY;

CREATE TABLE Albums {
  uid INT64 NOT NULL,
  aid INT64 NOT NULL,
  name STRING
} PRIMARY KEY (uid, aid),
INTERLEAVE IN PARENT Users ON DELETE CASCADE;
```
- **`DIRECTORY`** on the `Users` table indicates it’s a **top-level** (directory) table.
- Rows from child tables that share the same parent primary key value belong to the **same directory**.
- **ON DELETE CASCADE** ensures that when a parent row is removed, all child rows are removed as well.
---
## TrueTime API
- **Core Innovation**: Exposes clock uncertainty intervals rather than a single time value.
- **API**:
  - `TT.now() -> TTinterval`
    - Returns an interval `[earliest, latest]` guaranteed to contain the absolute current time.
  - `TT.after(t)`, `TT.before(t)`
    - Convenience methods to check if the current real time is definitely after/before some timestamp `t`.
- **Implementation**:
  - Uses a combination of **GPS** and **atomic clocks** across data centers.
  - Time is distributed via **time masters** and **time slaves**.
    - Most time masters have GPS receivers; a few have atomic clocks (Armageddon masters).
    - Time slaves poll masters using a variant of **Marzullo’s algorithm** to eliminate faulty masters.
  - Each machine’s clock is **periodically synchronized**; between syncs, the time daemon **increases** the uncertainty bound `ε`.
  - **Typical Uncertainty (`ε`)**: 1–7 ms in production.
#### ASCII Diagram: TrueTime Infrastructure

```
 +-------------------+        +-------------------+
 |   GPS Master #1   |        |   GPS Master #2   |
 |  (Atomic Clock)   |        |  (Atomic Clock)   |
 +-------------------+        +-------------------+
           \                       /
            \                     /
             \     Network      /
              \                 /
            +-------------------------+
            |  Time Slave Daemon     |
            | (Machine local clock)  |
            +-------------------------+
```
---
## Concurrency Control
### Timestamp Management
- Spanner supports:
  1. **Read-Write (RW) Transactions**
  2. **Read-Only (RO) Transactions**
  3. **Snapshot Reads (reads in the past)**
- **Leader Lease** (10s default)  
  - Each Paxos leader holds a lease for a time interval; only that leader can assign timestamps in its lease interval.
- **Monotonically Increasing Timestamps**  
  - Within a Paxos group, straightforward to ensure increasing timestamps.
  - Across multiple leaders, each leader is restricted to assign timestamps only within its lease interval, ensuring disjoint intervals.
- **External Consistency Requirement**  
  - If `T2` starts after `T1` commits, then `commit(T2).timestamp > commit(T1).timestamp`.
---
### Serving Reads at a Timestamp
- Each replica maintains a **safe time** (`Tsafe`) — the maximum timestamp at which the replica is consistent.
- A replica can serve a read at timestamp `t` if `t <= Tsafe`.
---
### Assigning Timestamps to RO Transactions
- Read-Only transactions (no writes) can be served at a timestamp chosen by the system without requiring locks.
- Multiple Paxos groups:  
  - The client picks a timestamp close to `TT.now().latest`.
  - Waits for each relevant replica’s `Tsafe` to catch up if needed.
- Single Paxos group:  
  - The client can talk directly to the group’s leader to assign a timestamp for the read.
---
### Details of RW vs. RO Transactions
#### Read-Write Transactions (2PL + 2PC)
1. **Locking**:  
   - Two-phase locking at each leader replica.  
2. **Commit**:  
   - When the client is done reading/updating, it begins two-phase commit across the involved Paxos groups.  
3. **Buffered Writes**:  
   - The client buffers writes locally and sends them to leaders during commit.  
4. **Coordinator Group**:  
   - One group is chosen as the coordinator.
#### Read-Only Transactions
1. **Scope Expression**:  
   - Summarizes the keys to be read across the transaction.
2. **Timestamp Negotiation**:  
   - If multiple groups are involved, the client uses a negotiated timestamp or a “trick” to pick a timestamp from `TT.now().latest`.
3. **No Locking**:  
   - Because it’s purely read-only, snapshot reads at a consistent timestamp require no distributed locks.
---
## Schema-Change Transactions
- Spanner allows **atomic schema changes**:
  - A schema-change transaction is assigned a **future timestamp**.
  - Reads/writes that occur at timestamps **beyond** that future timestamp must block until the schema change is effective.
  - Reads/writes with timestamps **before** that timestamp proceed normally.
---
## Related Work
- **Megastore**: Pioneered cross-datacenter consistent replication and semi-relational data model within Google.
- **DynamoDB**: A key-value store with flexible replication but eventually consistent by default.
- **Calvin**: Eliminates concurrency control by pre-assigning timestamps in a deterministic ordering.
- **VoltDB**: Sharded in-memory database with master-slave replication.
- **MarkLogic, Oracle Total Recall**: Support historical reads in some form.
- **Scatter**: DHT-based store with layered transactions.
---
## Future Work
- Collaboration with the **F1** team (Google’s Ads backend) to ensure performance and correctness at large scale.
- Improving **monitoring**, **support tools**, and **backup/restore** performance.
- Implementing:
  - **Spanner schema language** enhancements.
  - Automatic maintenance of **secondary indexes**.
  - **Automatic load-based resharding**.
---
## Conclusions
- Spanner blends the best from **database research** (transactions, SQL, relational schemas) and **distributed systems** (scalability, fault tolerance, wide-area distribution, consistent replication).
- The **TrueTime** API is the key to providing **external consistency** by reifying clock uncertainty in the system.
- This design took **5+ years** to refine at Google, culminating in a system that can run massive, globally consistent workloads at scale.