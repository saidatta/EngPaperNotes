Below is a **PhD-level** explanation of the **Write-Ahead Log (WAL)** for **InfluxDB 3.0**, focusing on its **architecture**, **data structures**, **interfaces**, and **concurrency model**. We include **code snippets**, **examples**, and **visual diagrams** to illustrate how the WAL operates, from buffering writes in memory to persisting them into object storage.

---
# Table of Contents

1. [High-Level Overview](#high-level-overview)  
2. [Architecture & Design](#architecture--design)  
3. [Core Data Structures](#core-data-structures)  
   1. [WalOp: Write or Catalog Operations](#walop-write-or-catalog-operations)  
   2. [WriteBatch & TableChunks](#writebatch--tablechunks)  
   3. [WAL File Contents & SnapshotDetails](#wal-file-contents--snapshotdetails)  
4. [WAL Trait & Lifecycle](#wal-trait--lifecycle)  
   1. [Buffering & Flushing Writes](#buffering--flushing-writes)  
   2. [Concurrency & Background Flush](#concurrency--background-flush)  
   3. [Snapshotting & Cleanup](#snapshotting--cleanup)  
5. [Configuration & Durability Guarantees](#configuration--durability-guarantees)  
6. [Example Flow](#example-flow)  
7. [Diagram](#diagram)  
8. [Summary & References](#summary--references)

---

## 1. High-Level Overview

**InfluxDB 3.0** uses a **Write-Ahead Log (WAL)** to ensure **durability** for incoming writes before they are persisted as larger **Parquet** files. The WAL operates as follows:

1. **In-Memory Buffer**: Incoming writes are appended to an internal buffer.  
2. **WAL File**: Once the buffer fills or a flush interval passes, the data is serialized into a **WAL file** in an object store.  
3. **Snapshot**: Periodically, older WAL data is “snapshotted” into Parquet or other metadata forms.  
4. **Cleanup**: Once snapshotted, those older WAL segments can be safely removed.

By offloading these steps to a **background task** and using an **async trait** design, InfluxDB can remain responsive to new writes while guaranteeing durability.

---

## 2. Architecture & Design

### 2.1 Key Goals

- **Durability**: Ensure data is not lost if the server restarts.  
- **Scalability**: Break writes into smaller WAL files to avoid massive single WAL append.  
- **Snapshot**: Convert older WAL data into stable formats (Parquet).  
- **Concurrent**: Writes can continue while the WAL flushes or snapshots in the background.

### 2.2 WAL in the Larger InfluxDB 3.0

```
             +---------------+  writes  +-------------------+
  (Clients)   |  InfluxDB 3  |----------|  WAL in Object    |
             +---------------+          |  Store            |
                     |                 +-------------------+
                     | WAL flush
                     v
               +---------------------------+
               | Parquet & Snapshots      |
               +---------------------------+
```

When writes come in, they go through an in-memory buffer, and the WAL ensures they are durably recorded. Eventually, a snapshot merges these WAL entries into Parquet.

---

## 3. Core Data Structures

### 3.1 WalOp: Write or Catalog Operations

```rust
#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]
pub enum WalOp {
    Write(WriteBatch),
    Catalog(CatalogBatch),
}
```

- **`WalOp::Write(WriteBatch)`**: A batch of timeseries data to store.  
- **`WalOp::Catalog(CatalogBatch)`**: Metadata changes (create table, drop table, etc.).  

**Catalog operations** ensure DDL changes are also durably logged.

### 3.2 WriteBatch & TableChunks

```rust
#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]
pub struct WriteBatch {
    pub database_id: DbId,
    pub database_name: Arc<str>,
    pub table_chunks: SerdeVecMap<TableId, TableChunks>,
    pub min_time_ns: i64,
    pub max_time_ns: i64,
}
```

A **`WriteBatch`** aggregates multiple “table chunks,” each chunk containing rows:

```rust
#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]
pub struct TableChunks {
    pub min_time: i64,
    pub max_time: i64,
    pub chunk_time_to_chunk: HashMap<i64, TableChunk>,
}
```

- A **`TableChunk`** has a vector of `Row` objects, each having a `time` and a list of `Field`s (data points).  
- **`min_time`** and **`max_time`** help to quickly identify the time range of this chunk.

### 3.3 WAL File Contents & SnapshotDetails

```rust
#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]
pub struct WalContents {
    pub persist_timestamp_ms: i64,
    pub min_timestamp_ns: i64,
    pub max_timestamp_ns: i64,
    pub wal_file_number: WalFileSequenceNumber,
    pub ops: Vec<WalOp>,
    pub snapshot: Option<SnapshotDetails>,
}
```

- **`WalFileSequenceNumber`**: A monotonically increasing ID for each WAL file.  
- **`snapshot`**: If present, signals that after loading these ops, the system should take a snapshot (e.g., flush older data to Parquet).

**`SnapshotDetails`** includes the last WAL sequence number that can be safely removed after the snapshot completes.

---

## 4. WAL Trait & Lifecycle

```rust
#[async_trait]
pub trait Wal: Debug + Send + Sync + 'static {
    async fn buffer_op_unconfirmed(&self, op: WalOp) -> Result<(), Error>;
    async fn write_ops(&self, ops: Vec<WalOp>) -> Result<(), Error>;
    async fn flush_buffer(
        &self,
    ) -> Option<(
        oneshot::Receiver<SnapshotDetails>,
        SnapshotInfo,
        OwnedSemaphorePermit,
    )>;
    ...
    async fn cleanup_snapshot(
        &self,
        snapshot_details: SnapshotInfo,
        snapshot_permit: OwnedSemaphorePermit,
    );
    ...
}
```

### 4.1 Buffering & Flushing Writes

1. **`buffer_op_unconfirmed(...)`**: Quickly enqueue an operation (doesn’t wait for persistence).  
2. **`write_ops(...)`**: Writes the operations and **waits** until they’re persisted. When this method returns, the WAL file is safely in object storage.

### 4.2 Concurrency & Background Flush

A typical pattern is to **periodically flush** the in-memory buffer to a WAL file. The function:

```rust
pub fn background_wal_flush<W: Wal>(
    wal: Arc<W>,
    flush_interval: Duration,
) -> tokio::task::JoinHandle<()>
```

spawns a **background task** that calls `wal.flush_buffer()` every `flush_interval`. After flushing, it might also trigger a **snapshot** in another async task.

### 4.3 Snapshotting & Cleanup

If the WAL determines it’s “time” for a snapshot (based on `snapshot_size` or other logic), it returns:

```rust
Some((snapshot_complete, snapshot_info, snapshot_permit))
```

The caller waits on `snapshot_complete`, and once the snapshot is done, calls:

```rust
wal.cleanup_snapshot(snapshot_info, snapshot_permit).await
```

At that point, older WAL files can be safely deleted.

---

## 5. Configuration & Durability Guarantees

```rust
#[derive(Debug, Clone, Copy)]
pub struct WalConfig {
    pub gen1_duration: Gen1Duration,
    pub max_write_buffer_size: usize,
    pub flush_interval: Duration,
    pub snapshot_size: usize,
}
```

- **`gen1_duration`**: Groups data by time window for chunking.  
- **`max_write_buffer_size`**: If in-memory ops exceed this, a flush is forced.  
- **`flush_interval`**: Periodic time-based flush.  
- **`snapshot_size`**: After writing a certain number of WAL files, trigger a snapshot.

**Durability**: Once `write_ops(...)` returns, the data is guaranteed to be in the object store. This is crucial for systems requiring **ACID** semantics at the log level.

---

## 6. Example Flow

1. **Client** sends lines (LP) to InfluxDB.  
2. The server calls `wal.buffer_op_unconfirmed(WalOp::Write(batch))`.  
3. The in-memory buffer grows. Eventually, time or size triggers a flush:  
   ```rust
   wal.write_ops(vec![WalOp::Write(batch)]).await?;
   ```  
   Internally, the WAL serializes to an object store file:  
   - **WalFile #10** is created.  
4. The WAL notifies a **WalFileNotifier**. This loads the newly persisted data into memory for query-ability.  
5. Every few writes or files, the WAL includes a `snapshot_details`. The system spawns snapshot tasks, ultimately calling `wal.cleanup_snapshot(...)`.

---

## 7. Diagram

Here is a simplified ASCII diagram of the WAL flow:

```
           +------------------------------+
           |  InfluxDB 3: Accept Writes  |
           +--------------+---------------+
                          |
                          v
                1) buffer_op_unconfirmed()
                          |
                          v
                 In-Memory WAL Buffer
                          |
                 (size/time flush)
                          v
                2) write_ops([WalOp::Write(...)] )
                          |
         +--------------------------------------+
         |  Serialize WAL file => object store |
         +-----------------+--------------------+
                           |
               3) WalFileNotifier.notify()
                           |
                +---------------------+
                |   Query Engine     |
                | (load into memory) |
                +---------------------+
                           |
            (optionally trigger snapshot)
                           |
                           v
        4) snapshot =>   create Parquet, cleanup older WAL files
```

---

## 8. Summary & References

The **InfluxDB 3.0 WAL** is a **robust** mechanism ensuring:
- **Durable** acceptance of writes in the **object store**.  
- **Periodic snapshot** to transform write batches into more compact Parquet.  
- **Concurrent** flushing via an async architecture.  
- **Catalog** changes (DDL) handled consistently with `WalOp::Catalog(...)`.  

It exemplifies a **modern Rust** system design, leveraging:
- `async_trait` for concurrency,  
- `oneshot` channels for snapshot notifications,  
- `Arc` referencing,  
- custom data structures (`WriteBatch`, `TableChunks`, `Row`, etc.) for time-series data.

### References
- [Tokio’s concurrency model](https://docs.rs/tokio/latest/tokio/)  
- [Object Store crate design](https://docs.rs/object_store/latest/object_store/)  
- [InfluxDB 3.0 Architecture](https://github.com/influxdata/influxdb_iox)  

Such a design allows InfluxDB 3.0 to ingest massive time-series volumes while guaranteeing **no data loss** and enabling background tasks to efficiently maintain and snapshot the WAL.