Below is **additional detail** extracted from the **same transcript** (the talk by Yi about Trino at Pinterest). These sections expand on various topics—from the **infrastructure** to **Spark–Trino bridging**, **Kubernetes** details, **user behavior patterns**, and **Q&A** themes. This supplements the previous notes, providing an even **fuller picture**.

---
## 1. Kubernetes Usage

### 1.1 Kubernetes Management
- Pinterest **does not** use AWS-managed EKS or kops; instead, Pinterest manages its own Kubernetes clusters internally.
- A dedicated internal team maintains:
  - **Cluster lifecycle** (spin-up, upgrades, node pools).
  - **Networking** (VPC, load balancers).
  - **Security** (cert distribution, secrets management).

### 1.2 Kubernetes & Trino Workers
- **Ad-hoc clusters** frequently run on top of K8s:
  - Workers scale up/down within a range, though full auto-scaling is not yet implemented.
  - The Trino “Controller” service can drain workers or kill hung queries to keep pods healthy.
- **Scheduled clusters** sometimes run on **EC2 VMs** instead (for predictable, stable batch workloads).

### 1.3 Worker Health & Draining
- Custom logic periodically identifies **slow or stuck** workers:
  1. If a worker’s heartbeat or throughput is low for too long, the system sends a “**drain**” signal.
  2. The worker finishes its current tasks if possible, is removed from the pool, then gets **recycled**.

> **Why not rely only on K8s auto-scaling or health checks?**  
> Trino requires deeper, more **query-specific** checks (e.g., hung operators, memory pressure, large spilled data). Pinterest’s custom “**Presto/Trino Controller**” can catch these conditions better than generic K8s readiness/liveness probes.

---

## 2. Query Gateway & Routing

### 2.1 Forked Gateway from Lyft
- The Gateway includes UI enhancements:
  - **Cluster health** summary: which clusters have high CPU usage or running worker slots.
  - **Query logs**: up to 2 weeks for replay/forensics.
  - **Per-organization resource usage**: team leads can see who is consuming cluster time.

### 2.2 Dynamic Routing Policies
- **Time-based**: e.g., certain user groups allowed only on a specific cluster during business hours.
- **Health-based**: if Ad-hoc cluster is busy, route queries to Scheduled cluster (if it’s under capacity).
- **User-based**: restricted data or PII queries automatically routed to specialized clusters.

```plaintext
Gateway logic pseudo-flow:
1. Evaluate user, data sensitivity, time-of-day
2. Check cluster load (CPU, memory, queued queries)
3. Route query to the best available cluster
```

---

## 3. Advanced User Guidance via Warnings

Pinterest invests heavily in **Trino warnings** to immediately surface potential issues to the user. Examples:

1. **Large Partition Reads**  
   - If the query is scanning many partitions (e.g., thousands) with no filtering.  
   - Warns: “Consider adding a WHERE clause or partition pruning.”

2. **Join Reordering**  
   - If the plan suggests a big table joining a small table incorrectly (or missing a possible broadcast join).  
   - Warns: “You might broadcast the smaller table or reorder your join for efficiency.”

3. **Potential Cartesians**  
   - Triggers if no join condition is found for multiple tables.  
   - Warns: “Cross join detected. This can lead to extremely large data expansions.”

4. **Excessive Aggregations**  
   - If the user does multiple nested aggregations or heavy DISTINCT usage.  
   - Warns: “Expression can be optimized to reduce repeated scans.”

These warnings are shown **during the query** (e.g., partial message) or in the final UI logs. Users can:
- **Cancel immediately** upon seeing a big partition read warning to save resources.
- **Rewrite and re-run** with improved logic.

---

## 4. Fine-Grained Access Control (FGAC) – More Detail

### 4.1 Old vs. New Approach

| **Old**                         | **New (FGAC)**                                         |
|--------------------------------|--------------------------------------------------------|
| Separated data physically (PII vs. non-PII clusters). | Unified data lake, one (or fewer) Trino clusters. |
| Access controlled by cluster membership & machine-level IAM roles. | Access controlled by **user identity** & **LDAP group** → short-lived STS tokens. |

### 4.2 Operational Flow

```plaintext
User (has Oauth or mTLS cert) 
      | 
      v
Trino Gateway -> passes username in HTTP headers
      |
      v
Trino Coordinator - obtains Hive Metastore auth token
      |
      v
Token Service (AWS STS) -> short-lived credentials based on the user’s LDAP groups
      |
      v
Trino Workers -> Only read/write S3 objects the user is allowed to see
```

### 4.3 Benefits
- Reduces operational overhead of maintaining **cluster-level** separation.
- More flexible: easy to add new groups or new data restrictions in the **IAM policy**.
- Improved user experience: “One cluster or one endpoint to rule them all.”

### 4.4 Caveats & Challenges
- Requires continuous sync between **LDAP** → **policy engine** → **token service**.
- Might need fallback logic if the user’s group membership changes mid-session.
- Some overhead: short-lived STS tokens need to be reissued for long queries.

---

## 5. Managing Diverse Workloads

### 5.1 Observed Patterns
- **Peak usage**: Business hours, large ad-hoc queries can spike concurrency.
- **Off-peak**: Lower interactive load; scheduled batch jobs can run more aggressively.

### 5.2 Cross-Cluster Overflow
- If the scheduled cluster is idle off-hours, the system can accept ad-hoc queries to use the available capacity.
- If the ad-hoc cluster is overloaded during peak, some scheduled queries move to a different cluster.

### 5.3 Organization-Based Quotas
- Queries are tagged by **org/team** metadata at submission.
- Each org has **max running** queries or CPU concurrency. If they exceed, further queries queue.
- Minimizes “noisy neighbor” issues.

```ini
# Example snippet for potential queueing config
root.organizationA.max-running=30
root.organizationA.scheduling-policy=fifo

root.organizationB.max-running=15
...
```

---

## 6. Bridging Trino & Spark SQL

### 6.1 Common Use Case at Pinterest
1. **Prototyping**: Trino is fast for interactive iteration.  
2. **Production**: If queries are extremely large or need batch fault-tolerance, teams often rewrite them in **Spark SQL** or use a Spark pipeline.

### 6.2 Automatic Query Translation
- Pinterest explored converting a Trino logical plan into **Apache Calcite** IR, then using **Coral** (an internal LinkedIn tool in some cases) or other frameworks to generate Spark SQL.
- **Main goal**: Let users author in Trino, then systematically produce an equivalent Spark job for big production workflows.  
- **Ongoing Challenge**: 
  - Preserving correctness (esp. with UDFs and advanced Trino syntax).
  - Ensuring the plan is fully representable in Spark’s engine (some queries might not map 1:1).

### 6.3 Future: Fault-Tolerant Trino (Project Tardigrade)
- Pinterest is watching the **fault tolerance** developments in Trino (code-named “Tardigrade”).
- Potentially could reduce the need for Spark if Trino can handle large-scale ETL with partial fault tolerance and better batch scheduling.

---

## 7. Q&A Highlights

During the presentation, a few interesting questions arose:

1. **Q**: _Which managed Kubernetes product do you use—EKS, kops, etc.?_  
   **A**: Pinterest has an **internal** Kubernetes engineering team. Not using EKS. They integrate with AWS but keep custom control.

2. **Q**: _When upgrading Trino, how do you handle code forks and custom patches?_  
   **A**: Pinterest has many **custom patches** (warning system logic, token handling, etc.). Porting them to a new major Trino release is **non-trivial**. This slows down upgrades.

3. **Q**: _How far along is adoption of Iceberg vs. the Hive connector?_  
   **A**: Still in the **early** stages for Iceberg. A few non-critical datasets are moved. Expect ramp-up in upcoming quarters.

4. **Q**: _How do you handle node auto-scaling in production for Trino?_  
   **A**: True auto-scaling is **not fully** implemented. The Trino Controller can kill or drain nodes, but spinning up new nodes depends on capacity reservations, cost, and cluster policy.

5. **Q**: _Any plan to unify ad-hoc & scheduled clusters entirely?_  
   **A**: Possibly in the future, once fine-grained Access Control is fully deployed and resource management can handle both workloads gracefully.

---

## 8. Detailed Observations & Lessons

1. **Maintenance & Self-Healing**  
   - Automated daily restarts catch memory leaks or out-of-memory states on long-running workers.  
   - Rolling approach ensures minimal user disruption.

2. **Historical Metadata**  
   - **Query logs** and usage stats are crucial for teams to self-service debug. This fosters a culture where each org is responsible for optimizing their queries.

3. **Security Overhaul**  
   - Migrating from an environment-based security model (separate PII clusters) to user-based is a **significant** engineering & governance effort.  
   - But it provides flexibility for future expansions and reduces hardware duplication.

4. **Complex SQL → Spark**  
   - The bridging solution is a partial workaround for Trino’s lack of **long-running** or fault-tolerant queries in older versions.  
   - The possibility of a **future** Trino with robust fault tolerance may reduce the need for double maintenance (Trino + Spark).

---

## 9. Example Potential Configurations & Code Snippets

### 9.1 Worker Tuning Snippet

```properties
# trino-config.properties (on each worker)
query.max-memory=50GB
query.max-memory-per-node=4GB
spill-enabled=true
spill-max-used-space-per-node=300GB
```

- These are **example** values. Pinterest’s real values vary based on instance size and typical usage.

### 9.2 Dockerfile Excerpt (Hypothetical)

```dockerfile
FROM openjdk:11-jre-slim

ARG TRINO_VERSION=xxx
ADD trino-server-${TRINO_VERSION}.tar.gz /opt/

# Add Pinterest custom patches / libs
COPY custom-connector.jar /opt/trino/plugin/custom/
COPY custom-warnings.jar /opt/trino/plugin/warnings/

...
```

- Pinterest includes custom warning plugins in the build or via side-loading in Docker images.  

### 9.3 Resource Group Mapping by LDAP

```yaml
resource-groups:
  - name: "Engineering"
    matchers:
      - user_groups: ["eng_ldap_group"]
    max-concurrency: 20

  - name: "DataScience"
    matchers:
      - user_groups: ["ds_ldap_group"]
    max-concurrency: 10
```

- Hypothetical YAML for user-group-based resource groups. In production, Pinterest might store these configurations in an internal service or DB.

---

## 10. Final Takeaways

1. **Scale & Adoption**: Trino is the de-facto interactive SQL engine at Pinterest, seeing widespread daily usage across the company.  
2. **Kubernetes & EC2**: Clusters run on both to address ad-hoc vs. scheduled usage.  
3. **FGAC**: Moving away from environment-based security to user-level, short-lived STS tokens.  
4. **Bridging Spark**: Some large or fault-tolerant jobs still rely on Spark, but there's a trend to unify or automatically translate.  
5. **Warnings**: Rich custom warnings have significantly improved user behavior and query efficiency.  
6. **Future**: Evaluating **Graviton** for cost efficiency, continuing advanced **warning system** development, finalizing FGAC, and possibly adopting **fault-tolerant** Trino features.

---

## 11. Related Reading & Links

1. **Trino on Kubernetes**  
   - [Trino Official Docs - Kubernetes](https://trino.io/docs/current/installation/kubernetes.html)
2. **Iceberg Table Format**  
   - [Apache Iceberg](https://iceberg.apache.org/)
3. **Calcite & Coral**  
   - [Apache Calcite](https://calcite.apache.org/)
   - [LinkedIn Coral](https://github.com/linkedin/coral)  
4. **AWS STS**  
   - [AWS STS Docs](https://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html)

---

**These extended notes** capture more granular details from the transcript about Pinterest’s Trino usage, including Kubernetes discussions, user Q&A, and deeper security aspects. You may combine these with the [previous notes](obsidian://notes/Scaling-Trino-at-Pinterest) for a **comprehensive** perspective on how Pinterest operates Trino at massive scale.